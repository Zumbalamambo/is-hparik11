{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import os,sys, shutil\n",
    "import time\n",
    "from datetime import date\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import pprint\n",
    "from collections import deque\n",
    "from shutil import copyfile\n",
    "import random\n",
    "import glob\n",
    "# Import the required modules\n",
    "import cv2, os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Logistic Regression\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import math\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_fl = open(\"linkedin_profiles.pickle\",\"rb\")\n",
    "my_original_list=pickle.load(pkl_fl) # errors out here\n",
    "pkl_fl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = \"Male\"\n",
    "    \n",
    "if os.path.exists(directory):\n",
    "    shutil.rmtree(directory)\n",
    "    os.makedirs(directory)     \n",
    "else:\n",
    "    os.makedirs(directory) \n",
    "\n",
    "directory1 = \"Female\"\n",
    "\n",
    "if os.path.exists(directory1):\n",
    "    shutil.rmtree(directory1)\n",
    "    os.makedirs(directory1)     \n",
    "else:\n",
    "    os.makedirs(directory1)     \n",
    "\n",
    "directory2 = \"Label_Images_Gender\"\n",
    "\n",
    "if os.path.exists(directory2):\n",
    "    shutil.rmtree(directory2)\n",
    "    os.makedirs(directory2)     \n",
    "else:\n",
    "    os.makedirs(directory2)     \n",
    "    \n",
    "fileList = glob.glob(\"./Images/*.*\")\n",
    "\n",
    "for id,fp in enumerate(fileList):\n",
    "    filename, file_extension = os.path.splitext(fp)\n",
    "    uid = filename.split('/')[-1]\n",
    "    #print fp\n",
    "    for prof in my_original_list:\n",
    "        if prof['User_ID'] == uid:\n",
    "            new_file_extension = prof['Gender']\n",
    "            new_file_extension = new_file_extension.title()\n",
    "            if new_file_extension != 'Unknown':\n",
    "                copyfile(filename+\".jpg\", './Label_Images_Gender/'+ uid + '.' + str(id) + \".\"+new_file_extension +'.jpg')\n",
    "                copyfile(filename+\".jpg\", new_file_extension +'/' + uid + \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For face detection we will use the Haar Cascade provided by OpenCV.\n",
    "cascadePath = \"haarcascade_frontalface_default.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cascadePath)\n",
    "\n",
    "# For face recognition we will the the LBPH Face Recognizer \n",
    "recognizer = cv2.createLBPHFaceRecognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_images_and_labels(path):\n",
    "    # Append all the absolute image paths in a list image_paths\n",
    "    \n",
    "    image_paths = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "    # images will contains face images\n",
    "    images = []\n",
    "    # labels will contains the label that is assigned to the image\n",
    "    labels = []\n",
    "    #gender will contains 1 or 0 indecating male or female\n",
    "    gender =[]\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        # Read the image and convert to grayscale\n",
    "        try:\n",
    "            image_pil = Image.open(image_path).convert('L')\n",
    "            # Convert the image format into numpy array\n",
    "            image = np.array(image_pil, 'uint8')\n",
    "            # Get the label of the image\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        nbr = int(os.path.split(image_path)[1].split(\".\")[1])\n",
    "        gender_current = os.path.split(image_path)[1].split(\".\")[2]\n",
    "        print nbr\n",
    "        \n",
    "        # Detect the face in the image\n",
    "        faces = faceCascade.detectMultiScale(image)\n",
    "        # If face is detected, append the face to images and the label to labels\n",
    "        try:\n",
    "            for (x, y, w, h) in faces:\n",
    "\n",
    "                ref_image = image[y: y + h, x: x + w]\n",
    "                resized = cv2.resize(ref_image, (100, 100), interpolation = cv2.INTER_AREA)\n",
    "                #edge_images = cv2.Canny(resized,100,200)\n",
    "                \n",
    "                images.append(np.array(resized))   #resized.reshape(1,10000)\n",
    "                labels.append(nbr)\n",
    "\n",
    "                if gender_current == 'Male':\n",
    "                    gender.append(1)\n",
    "                else:\n",
    "                    gender.append(0)\n",
    "                \n",
    "                #face_file_name = \"faces/face_\" + str(y) + \".jpg\"\n",
    "                #cv2.imwrite(face_file_name, sub_face)\n",
    "                \n",
    "                cv2.imshow(\"Adding faces to traning set...\", resized)\n",
    "                cv2.waitKey(1)\n",
    "        except:\n",
    "            pass\n",
    "    # return the images list and labels list\n",
    "    print \"lables\"\n",
    "    print labels\n",
    "    print \"gender_current\"\n",
    "    print gender\n",
    "    \n",
    "    return images, labels, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1870\n",
      "491\n",
      "2508\n",
      "3052\n",
      "1700\n",
      "2382\n",
      "132\n",
      "783\n",
      "1643\n",
      "209\n",
      "2392\n",
      "442\n",
      "1609\n",
      "2895\n",
      "406\n",
      "2689\n",
      "2754\n",
      "89\n",
      "299\n",
      "1382\n",
      "1096\n",
      "1824\n",
      "1208\n",
      "1689\n",
      "980\n",
      "2924\n",
      "2015\n",
      "321\n",
      "2799\n",
      "1213\n",
      "2802\n",
      "2109\n",
      "2553\n",
      "2635\n",
      "2175\n",
      "2275\n",
      "42\n",
      "3192\n",
      "641\n",
      "260\n",
      "1937\n",
      "1978\n",
      "2587\n",
      "754\n",
      "1843\n",
      "1592\n",
      "516\n",
      "2416\n",
      "3232\n",
      "558\n",
      "1591\n",
      "2524\n",
      "970\n",
      "1306\n",
      "135\n",
      "2184\n",
      "859\n",
      "676\n",
      "737\n",
      "3118\n",
      "1976\n",
      "229\n",
      "1904\n",
      "134\n",
      "2817\n",
      "2317\n",
      "1810\n",
      "3298\n",
      "2927\n",
      "3036\n",
      "1282\n",
      "2462\n",
      "1350\n",
      "196\n",
      "659\n",
      "1108\n",
      "1561\n",
      "371\n",
      "2664\n",
      "2094\n",
      "704\n",
      "2352\n",
      "73\n",
      "1140\n",
      "2783\n",
      "580\n",
      "786\n",
      "2593\n",
      "855\n",
      "1154\n",
      "1585\n",
      "2051\n",
      "986\n",
      "1247\n",
      "2311\n",
      "3047\n",
      "3223\n",
      "2050\n",
      "2271\n",
      "370\n",
      "2452\n",
      "2439\n",
      "43\n",
      "337\n",
      "2764\n",
      "189\n",
      "1648\n",
      "3264\n",
      "1199\n",
      "3316\n",
      "1386\n",
      "295\n",
      "1979\n",
      "226\n",
      "2973\n",
      "1287\n",
      "1807\n",
      "2201\n",
      "840\n",
      "3018\n",
      "161\n",
      "3031\n",
      "1428\n",
      "2979\n",
      "724\n",
      "3238\n",
      "1159\n",
      "2968\n",
      "2280\n",
      "455\n",
      "1403\n",
      "2611\n",
      "3318\n",
      "2562\n",
      "2429\n",
      "2911\n",
      "914\n",
      "916\n",
      "91\n",
      "818\n",
      "2223\n",
      "642\n",
      "3236\n",
      "757\n",
      "3322\n",
      "3335\n",
      "1496\n",
      "3141\n",
      "868\n",
      "3034\n",
      "1196\n",
      "3261\n",
      "1885\n",
      "928\n",
      "2019\n",
      "188\n",
      "3030\n",
      "1543\n",
      "1733\n",
      "2714\n",
      "3199\n",
      "3163\n",
      "400\n",
      "2733\n",
      "278\n",
      "2353\n",
      "2338\n",
      "2900\n",
      "1048\n",
      "109\n",
      "144\n",
      "930\n",
      "1802\n",
      "14\n",
      "2018\n",
      "2766\n",
      "1955\n",
      "1435\n",
      "2087\n",
      "1338\n",
      "2608\n",
      "190\n",
      "2180\n",
      "2235\n",
      "56\n",
      "2103\n",
      "1803\n",
      "79\n",
      "3095\n",
      "1515\n",
      "1696\n",
      "2486\n",
      "2117\n",
      "2710\n",
      "732\n",
      "1808\n",
      "154\n",
      "405\n",
      "2425\n",
      "1931\n",
      "245\n",
      "1983\n",
      "2684\n",
      "2248\n",
      "239\n",
      "2368\n",
      "3211\n",
      "25\n",
      "1574\n",
      "3325\n",
      "3338\n",
      "1092\n",
      "468\n",
      "1188\n",
      "2921\n",
      "1427\n",
      "426\n",
      "1078\n",
      "1879\n",
      "2548\n",
      "557\n",
      "1900\n",
      "2992\n",
      "2890\n",
      "1333\n",
      "1954\n",
      "1736\n",
      "2475\n",
      "302\n",
      "2898\n",
      "199\n",
      "201\n",
      "1250\n",
      "355\n",
      "2634\n",
      "334\n",
      "428\n",
      "1903\n",
      "1618\n",
      "1358\n",
      "2858\n",
      "272\n",
      "252\n",
      "2028\n",
      "2102\n",
      "3273\n",
      "2751\n",
      "711\n",
      "2866\n",
      "1433\n",
      "163\n",
      "1203\n",
      "3299\n",
      "128\n",
      "195\n",
      "2052\n",
      "2971\n",
      "540\n",
      "1919\n",
      "3045\n",
      "1761\n",
      "2055\n",
      "2424\n",
      "1095\n",
      "1023\n",
      "1662\n",
      "3368\n",
      "927\n",
      "699\n",
      "1182\n",
      "1625\n",
      "235\n",
      "2627\n",
      "3014\n",
      "2449\n",
      "3362\n",
      "1570\n",
      "1209\n",
      "2162\n",
      "1895\n",
      "1537\n",
      "77\n",
      "2579\n",
      "2940\n",
      "1019\n",
      "2857\n",
      "2203\n",
      "2584\n",
      "2278\n",
      "3139\n",
      "2243\n",
      "2081\n",
      "3214\n",
      "1206\n",
      "1951\n",
      "429\n",
      "1880\n",
      "1155\n",
      "3152\n",
      "2143\n",
      "2458\n",
      "1599\n",
      "1742\n",
      "150\n",
      "1102\n",
      "1404\n",
      "1731\n",
      "567\n",
      "2530\n",
      "1842\n",
      "2411\n",
      "2270\n",
      "286\n",
      "2167\n",
      "2894\n",
      "1215\n",
      "523\n",
      "1638\n",
      "2056\n",
      "519\n",
      "356\n",
      "1856\n",
      "678\n",
      "322\n",
      "2008\n",
      "1456\n",
      "2916\n",
      "1593\n",
      "3124\n",
      "1356\n",
      "3204\n",
      "965\n",
      "1611\n",
      "1065\n",
      "607\n",
      "3162\n",
      "2323\n",
      "1639\n",
      "860\n",
      "3218\n",
      "1926\n",
      "3288\n",
      "267\n",
      "1719\n",
      "3302\n",
      "2561\n",
      "1074\n",
      "1191\n",
      "703\n",
      "437\n",
      "3209\n",
      "3320\n",
      "956\n",
      "1907\n",
      "2489\n",
      "419\n",
      "459\n",
      "945\n",
      "2869\n",
      "2959\n",
      "738\n",
      "1587\n",
      "1266\n",
      "2362\n",
      "259\n",
      "3077\n",
      "2063\n",
      "3277\n",
      "2298\n",
      "3361\n",
      "2204\n",
      "139\n",
      "2174\n",
      "26\n",
      "939\n",
      "2149\n",
      "145\n",
      "943\n",
      "2172\n",
      "562\n",
      "614\n",
      "2930\n",
      "982\n",
      "588\n",
      "2717\n",
      "2865\n",
      "3178\n",
      "1424\n",
      "2809\n",
      "1828\n",
      "2598\n",
      "3231\n",
      "3158\n",
      "1375\n",
      "1103\n",
      "1264\n",
      "829\n",
      "1027\n",
      "2742\n",
      "3169\n",
      "1473\n",
      "2880\n",
      "306\n",
      "2633\n",
      "3366\n",
      "294\n",
      "1845\n",
      "2445\n",
      "1024\n",
      "3289\n",
      "2711\n",
      "865\n",
      "2722\n",
      "3171\n",
      "959\n",
      "1920\n",
      "1562\n",
      "323\n",
      "2398\n",
      "985\n",
      "1999\n",
      "1395\n",
      "3336\n",
      "2622\n",
      "785\n",
      "2062\n",
      "640\n",
      "1081\n",
      "2659\n",
      "994\n",
      "1603\n",
      "1418\n",
      "319\n",
      "2502\n",
      "3284\n",
      "2419\n",
      "1984\n",
      "3123\n",
      "1899\n",
      "2021\n",
      "3065\n",
      "1068\n",
      "3365\n",
      "3037\n",
      "2781\n",
      "338\n",
      "1559\n",
      "2274\n",
      "591\n",
      "398\n",
      "679\n",
      "1612\n",
      "2148\n",
      "717\n",
      "2247\n",
      "175\n",
      "2887\n",
      "2378\n",
      "1582\n",
      "35\n",
      "1482\n",
      "1730\n",
      "2208\n",
      "1564\n",
      "2299\n",
      "2708\n",
      "1323\n",
      "637\n",
      "1290\n",
      "688\n",
      "3195\n",
      "2588\n",
      "1097\n",
      "2320\n",
      "2997\n",
      "3107\n",
      "2465\n",
      "1148\n",
      "2784\n",
      "1279\n",
      "261\n",
      "2660\n",
      "2721\n",
      "3349\n",
      "1918\n",
      "1750\n",
      "1185\n",
      "2178\n",
      "1277\n",
      "339\n",
      "1341\n",
      "483\n",
      "1627\n",
      "799\n",
      "2238\n",
      "825\n",
      "900\n",
      "1503\n",
      "2432\n",
      "514\n",
      "1876\n",
      "104\n",
      "2099\n",
      "2987\n",
      "788\n",
      "1315\n",
      "1522\n",
      "568\n",
      "3346\n",
      "999\n",
      "2666\n",
      "1321\n",
      "1460\n",
      "2361\n",
      "1073\n",
      "2690\n",
      "1809\n",
      "3262\n",
      "3000\n",
      "1693\n",
      "850\n",
      "34\n",
      "124\n",
      "2011\n",
      "1569\n",
      "2688\n",
      "3063\n",
      "19\n",
      "2934\n",
      "1162\n",
      "381\n",
      "2797\n",
      "1464\n",
      "3354\n",
      "344\n",
      "807\n",
      "2871\n",
      "3089\n",
      "3104\n",
      "1325\n",
      "3237\n",
      "2020\n",
      "3357\n",
      "1676\n",
      "1241\n",
      "572\n",
      "172\n",
      "672\n",
      "2605\n",
      "733\n",
      "1131\n",
      "2808\n",
      "814\n",
      "1507\n",
      "1362\n",
      "1383\n",
      "2903\n",
      "2534\n",
      "3114\n",
      "474\n",
      "2291\n",
      "1010\n",
      "2671\n",
      "3102\n",
      "2438\n",
      "2960\n",
      "1772\n",
      "955\n",
      "1510\n",
      "744\n",
      "551\n",
      "1993\n",
      "1606\n",
      "694\n",
      "3360\n",
      "715\n",
      "1336\n",
      "2471\n",
      "146\n",
      "2395\n",
      "536\n",
      "1952\n",
      "374\n",
      "587\n",
      "148\n",
      "1715\n",
      "2437\n",
      "790\n",
      "1125\n",
      "1722\n",
      "3177\n",
      "595\n",
      "2914\n",
      "1214\n",
      "771\n",
      "413\n",
      "317\n",
      "3153\n",
      "740\n",
      "657\n",
      "2132\n",
      "1738\n",
      "1067\n",
      "652\n",
      "2358\n",
      "2367\n",
      "1588\n",
      "1177\n",
      "372\n",
      "1370\n",
      "2179\n",
      "1376\n",
      "2144\n",
      "2072\n",
      "1632\n",
      "911\n",
      "1881\n",
      "2303\n",
      "2665\n",
      "2816\n",
      "282\n",
      "2863\n",
      "3088\n",
      "2950\n",
      "1038\n",
      "3081\n",
      "858\n",
      "2746\n",
      "497\n",
      "993\n",
      "1576\n",
      "1235\n",
      "1511\n",
      "1770\n",
      "1091\n",
      "1285\n",
      "3101\n",
      "3194\n",
      "2517\n",
      "3254\n",
      "6\n",
      "2281\n",
      "1368\n",
      "2762\n",
      "780\n",
      "1822\n",
      "414\n",
      "877\n",
      "1673\n",
      "3105\n",
      "3085\n",
      "3310\n",
      "3260\n",
      "1036\n",
      "951\n",
      "1176\n",
      "2793\n",
      "3189\n",
      "542\n",
      "1698\n",
      "1054\n",
      "3060\n",
      "2313\n",
      "3186\n",
      "2743\n",
      "1921\n",
      "1831\n",
      "2905\n",
      "1500\n",
      "912\n",
      "2767\n",
      "887\n",
      "3164\n",
      "2073\n",
      "1962\n",
      "2720\n",
      "1047\n",
      "2258\n",
      "2129\n",
      "2447\n",
      "1332\n",
      "1057\n",
      "2327\n",
      "93\n",
      "692\n",
      "1883\n",
      "1669\n",
      "2446\n",
      "555\n",
      "957\n",
      "268\n",
      "886\n",
      "2222\n",
      "1539\n",
      "1779\n",
      "675\n",
      "961\n",
      "938\n",
      "1441\n",
      "61\n",
      "1982\n",
      "1516\n",
      "2257\n",
      "1767\n",
      "3347\n",
      "1110\n",
      "2371\n",
      "2881\n",
      "2775\n",
      "486\n",
      "1860\n",
      "2342\n",
      "3332\n",
      "82\n",
      "326\n",
      "1536\n",
      "876\n",
      "1369\n",
      "2985\n",
      "821\n",
      "173\n",
      "2381\n",
      "734\n",
      "2867\n",
      "443\n",
      "1757\n",
      "2878\n",
      "2215\n",
      "3146\n",
      "2319\n",
      "1806\n",
      "2800\n",
      "1820\n",
      "460\n",
      "2549\n",
      "204\n",
      "593\n",
      "1813\n",
      "1457\n",
      "1953\n",
      "2913\n",
      "862\n",
      "2833\n",
      "222\n",
      "2893\n",
      "3200\n",
      "425\n",
      "2610\n",
      "972\n",
      "2761\n",
      "1488\n",
      "1234\n",
      "2182\n",
      "2135\n",
      "2262\n",
      "1432\n",
      "2607\n",
      "3067\n",
      "103\n",
      "3239\n",
      "538\n",
      "922\n",
      "3355\n",
      "2644\n",
      "1547\n",
      "219\n",
      "548\n",
      "1045\n",
      "3278\n",
      "3012\n",
      "1769\n",
      "2302\n",
      "576\n",
      "1132\n",
      "1481\n",
      "1866\n",
      "3312\n",
      "2606\n",
      "1101\n",
      "1545\n",
      "1864\n",
      "2339\n",
      "2776\n",
      "756\n",
      "273\n",
      "2060\n",
      "3324\n",
      "2170\n",
      "2861\n",
      "1688\n",
      "2630\n",
      "3092\n",
      "2470\n",
      "342\n",
      "629\n",
      "2133\n",
      "492\n",
      "1324\n",
      "1935\n",
      "2106\n",
      "3202\n",
      "1535\n",
      "2337\n",
      "2499\n",
      "122\n",
      "1701\n",
      "3142\n",
      "2272\n",
      "590\n",
      "2941\n",
      "151\n",
      "2041\n",
      "1617\n",
      "853\n",
      "3340\n",
      "1122\n",
      "1026\n",
      "456\n",
      "2750\n",
      "2511\n",
      "2328\n",
      "2585\n",
      "31\n",
      "668\n",
      "3055\n",
      "3027\n",
      "2654\n",
      "1184\n",
      "349\n",
      "3075\n",
      "177\n",
      "2571\n",
      "1985\n",
      "1112\n",
      "3175\n",
      "2160\n",
      "662\n",
      "1069\n",
      "2065\n",
      "1490\n",
      "2426\n",
      "3198\n",
      "1275\n",
      "2121\n",
      "15\n",
      "2241\n",
      "1737\n",
      "509\n",
      "1355\n",
      "2962\n",
      "2568\n",
      "823\n",
      "525\n",
      "832\n",
      "100\n",
      "1991\n",
      "1934\n",
      "1469\n",
      "490\n",
      "2285\n",
      "1994\n",
      "1518\n",
      "2269\n",
      "2942\n",
      "2195\n",
      "2498\n",
      "2496\n",
      "673\n",
      "966\n",
      "1502\n",
      "1529\n",
      "1862\n",
      "1960\n",
      "2343\n",
      "2673\n",
      "1194\n",
      "1961\n",
      "394\n",
      "2346\n",
      "2013\n",
      "774\n",
      "2412\n",
      "1113\n",
      "794\n",
      "968\n",
      "1071\n",
      "706\n",
      "2333\n",
      "2650\n",
      "1372\n",
      "3093\n",
      "214\n",
      "236\n",
      "2806\n",
      "765\n",
      "2286\n",
      "843\n",
      "464\n",
      "3235\n",
      "1975\n",
      "2870\n",
      "1816\n",
      "2612\n",
      "3147\n",
      "152\n",
      "2390\n",
      "1607\n",
      "2724\n",
      "2745\n",
      "1164\n",
      "185\n",
      "2535\n",
      "1932\n",
      "2138\n",
      "1015\n",
      "2529\n",
      "1957\n",
      "3323\n",
      "2220\n",
      "796\n",
      "1440\n",
      "180\n",
      "3187\n",
      "2756\n",
      "3327\n",
      "1270\n",
      "3121\n",
      "3\n",
      "2126\n",
      "1674\n",
      "560\n",
      "1437\n",
      "1257\n",
      "2945\n",
      "2747\n",
      "2791\n",
      "98\n",
      "223\n",
      "2086\n",
      "2626\n",
      "2329\n",
      "198\n",
      "3082\n",
      "475\n",
      "1558\n",
      "1878\n",
      "2477\n",
      "1052\n",
      "929\n",
      "2824\n",
      "2563\n",
      "2949\n",
      "649\n",
      "1334\n",
      "368\n",
      "60\n",
      "1354\n",
      "2955\n",
      "1298\n",
      "813\n",
      "2538\n",
      "1255\n",
      "2792\n",
      "2224\n",
      "1692\n",
      "1835\n",
      "1211\n",
      "3004\n",
      "2616\n",
      "2306\n",
      "2902\n",
      "2884\n",
      "1966\n",
      "1398\n",
      "39\n",
      "125\n",
      "1521\n",
      "2636\n",
      "546\n",
      "918\n",
      "2859\n",
      "2234\n",
      "1320\n",
      "1012\n",
      "1170\n",
      "3111\n",
      "2110\n",
      "240\n",
      "430\n",
      "158\n",
      "441\n",
      "1314\n",
      "2734\n",
      "2814\n",
      "2877\n",
      "2289\n",
      "1759\n",
      "3374\n",
      "646\n",
      "2680\n",
      "2580\n",
      "1129\n",
      "1877\n",
      "2774\n",
      "220\n",
      "303\n",
      "631\n",
      "2629\n",
      "2876\n",
      "3287\n",
      "2628\n",
      "1117\n",
      "2044\n",
      "168\n",
      "1183\n",
      "143\n",
      "2993\n",
      "1498\n",
      "2641\n",
      "2820\n",
      "1686\n",
      "3350\n",
      "2152\n",
      "1373\n",
      "1419\n",
      "971\n",
      "873\n",
      "1474\n",
      "1450\n",
      "30\n",
      "1675\n",
      "1556\n",
      "2026\n",
      "2681\n",
      "563\n",
      "3367\n",
      "2400\n",
      "1198\n",
      "1928\n",
      "763\n",
      "2115\n",
      "3041\n",
      "72\n",
      "52\n",
      "1058\n",
      "2397\n",
      "2522\n",
      "893\n",
      "1743\n",
      "1179\n",
      "2114\n",
      "1173\n",
      "1381\n",
      "2124\n",
      "1871\n",
      "432\n",
      "1753\n",
      "1409\n",
      "1390\n",
      "647\n",
      "753\n",
      "976\n",
      "2108\n",
      "2787\n",
      "2888\n",
      "1681\n",
      "1589\n",
      "892\n",
      "2936\n",
      "2798\n",
      "1734\n",
      "726\n",
      "1852\n",
      "401\n",
      "193\n",
      "1659\n",
      "3241\n",
      "3116\n",
      "162\n",
      "1347\n",
      "2492\n",
      "2297\n",
      "389\n",
      "3193\n",
      "2811\n",
      "1242\n",
      "1455\n",
      "582\n",
      "2402\n",
      "480\n",
      "2855\n",
      "2570\n",
      "2082\n",
      "2651\n",
      "2421\n",
      "2031\n",
      "795\n",
      "2130\n",
      "1489\n",
      "427\n",
      "81\n",
      "995\n",
      "701\n",
      "1863\n",
      "2068\n",
      "2476\n",
      "3233\n",
      "417\n",
      "2318\n",
      "845\n",
      "1348\n",
      "1805\n",
      "819\n",
      "772\n",
      "2560\n",
      "2193\n",
      "232\n",
      "3359\n",
      "2455\n",
      "418\n",
      "1152\n",
      "1939\n",
      "3016\n",
      "1005\n",
      "1172\n",
      "2078\n",
      "2119\n",
      "1246\n",
      "1295\n",
      "1956\n",
      "1453\n",
      "2540\n",
      "2830\n",
      "1690\n",
      "2920\n",
      "1925\n",
      "269\n",
      "902\n",
      "650\n",
      "3056\n",
      "1513\n",
      "1762\n",
      "3070\n",
      "3128\n",
      "1326\n",
      "3208\n",
      "720\n",
      "328\n",
      "1201\n",
      "2263\n",
      "2422\n",
      "654\n",
      "2854\n",
      "864\n",
      "602\n",
      "1353\n",
      "718\n",
      "543\n",
      "1190\n",
      "831\n",
      "581\n",
      "990\n",
      "2088\n",
      "1549\n",
      "987\n",
      "395\n",
      "410\n",
      "2265\n",
      "382\n",
      "1600\n",
      "1267\n",
      "687\n",
      "3015\n",
      "1990\n",
      "841\n",
      "2514\n",
      "2853\n",
      "1619\n",
      "3132\n",
      "2718\n",
      "1145\n",
      "1462\n",
      "1683\n",
      "2349\n",
      "1747\n",
      "1538\n",
      "2840\n",
      "3292\n",
      "1872\n",
      "2000\n",
      "1181\n",
      "330\n",
      "1566\n",
      "281\n",
      "1732\n",
      "666\n",
      "2104\n",
      "2122\n",
      "2559\n",
      "1044\n",
      "2290\n",
      "2012\n",
      "2532\n",
      "3205\n",
      "2675\n",
      "2531\n",
      "1754\n",
      "59\n",
      "2770\n",
      "270\n",
      "883\n",
      "2048\n",
      "2079\n",
      "882\n",
      "3039\n",
      "170\n",
      "3086\n",
      "1228\n",
      "2287\n",
      "1512\n",
      "238\n",
      "2505\n",
      "2027\n",
      "2652\n",
      "1089\n",
      "2005\n",
      "906\n",
      "926\n",
      "800\n",
      "1127\n",
      "2308\n",
      "1892\n",
      "1940\n",
      "1682\n",
      "2023\n",
      "3057\n",
      "2780\n",
      "618\n",
      "584\n",
      "2925\n",
      "1346\n",
      "1303\n",
      "2599\n",
      "1400\n",
      "3130\n",
      "863\n",
      "1927\n",
      "2885\n",
      "2030\n",
      "2372\n",
      "2991\n",
      "1461\n",
      "1118\n",
      "3344\n",
      "352\n",
      "1020\n",
      "527\n",
      "1256\n",
      "1429\n",
      "3197\n",
      "2577\n",
      "1859\n",
      "983\n",
      "2952\n",
      "608\n",
      "354\n",
      "1138\n",
      "2575\n",
      "2977\n",
      "2159\n",
      "231\n",
      "2544\n",
      "1568\n",
      "1393\n",
      "2233\n",
      "838\n",
      "2679\n",
      "1501\n",
      "899\n",
      "1451\n",
      "2661\n",
      "2420\n",
      "76\n",
      "2370\n",
      "2003\n",
      "889\n",
      "2653\n",
      "2694\n",
      "2091\n",
      "2391\n",
      "1446\n",
      "1274\n",
      "2221\n",
      "760\n",
      "2053\n",
      "3315\n",
      "2173\n",
      "2032\n",
      "1322\n",
      "1447\n",
      "2713\n",
      "1137\n",
      "1575\n",
      "2738\n",
      "1463\n",
      "2331\n",
      "1752\n",
      "1896\n",
      "2096\n",
      "1829\n",
      "511\n",
      "1629\n",
      "2097\n",
      "2493\n",
      "953\n",
      "2668\n",
      "2827\n",
      "2139\n",
      "225\n",
      "3154\n",
      "2860\n",
      "131\n",
      "1218\n",
      "669\n",
      "2715\n",
      "140\n",
      "1086\n",
      "2543\n",
      "3180\n",
      "2330\n",
      "377\n",
      "2250\n",
      "1905\n",
      "1826\n",
      "2879\n",
      "2128\n",
      "933\n",
      "3143\n",
      "1060\n",
      "2046\n",
      "1812\n",
      "1352\n",
      "3185\n",
      "422\n",
      "1416\n",
      "2284\n",
      "2029\n",
      "416\n",
      "857\n",
      "3048\n",
      "3145\n",
      "1998\n",
      "3369\n",
      "1897\n",
      "101\n",
      "3042\n",
      "318\n",
      "2047\n",
      "2623\n",
      "2245\n",
      "291\n",
      "1581\n",
      "2430\n",
      "2703\n",
      "1666\n",
      "2555\n",
      "2461\n",
      "2972\n",
      "2380\n",
      "2638\n",
      "2539\n",
      "1967\n",
      "3054\n",
      "3274\n",
      "4\n",
      "1633\n",
      "1800\n",
      "1000\n",
      "2100\n",
      "3181\n",
      "2077\n",
      "3010\n",
      "1195\n",
      "920\n",
      "1509\n",
      "1007\n",
      "1571\n",
      "2687\n",
      "1942\n",
      "656\n",
      "2268\n",
      "809\n",
      "2414\n",
      "495\n",
      "141\n",
      "2163\n",
      "2192\n",
      "613\n",
      "1973\n",
      "29\n",
      "357\n",
      "2875\n",
      "271\n",
      "1260\n",
      "3020\n",
      "215\n",
      "1784\n",
      "2481\n",
      "1672\n",
      "2526\n",
      "1011\n",
      "2501\n",
      "2873\n",
      "1439\n",
      "45\n",
      "1174\n",
      "633\n",
      "2443\n",
      "3226\n",
      "2214\n",
      "402\n",
      "2882\n",
      "1679\n",
      "2989\n",
      "616\n",
      "471\n",
      "192\n",
      "3370\n",
      "1944\n",
      "526\n",
      "949\n",
      "2788\n",
      "1296\n",
      "3339\n",
      "450\n",
      "3295\n",
      "1823\n",
      "907\n",
      "2483\n",
      "3255\n",
      "2943\n",
      "2386\n",
      "836\n",
      "545\n",
      "387\n",
      "947\n",
      "304\n",
      "110\n",
      "728\n",
      "2795\n",
      "1037\n",
      "755\n",
      "2418\n",
      "2821\n",
      "628\n",
      "1304\n",
      "1493\n",
      "1751\n",
      "665\n",
      "391\n",
      "2937\n",
      "2678\n",
      "292\n",
      "1466\n",
      "2141\n",
      "1309\n",
      "2198\n",
      "2210\n",
      "2504\n",
      "784\n",
      "932\n",
      "2487\n",
      "179\n",
      "996\n",
      "385\n",
      "1452\n",
      "1694\n",
      "2807\n",
      "325\n",
      "2309\n",
      "421\n",
      "1062\n",
      "3373\n",
      "2545\n",
      "2938\n",
      "409\n",
      "2399\n",
      "1917\n",
      "909\n",
      "1217\n",
      "1225\n",
      "2528\n",
      "681\n",
      "510\n",
      "3356\n",
      "3216\n",
      "1415\n",
      "436\n",
      "1811\n",
      "622\n",
      "1707\n",
      "2849\n",
      "713\n",
      "1263\n",
      "2695\n",
      "2582\n",
      "99\n",
      "300\n",
      "390\n",
      "167\n",
      "1454\n",
      "2757\n",
      "1526\n",
      "1531\n",
      "1741\n",
      "697\n",
      "2389\n",
      "1664\n",
      "1402\n",
      "86\n",
      "3337\n",
      "3286\n",
      "1775\n",
      "1223\n",
      "2725\n",
      "2702\n",
      "3363\n",
      "901\n",
      "2384\n",
      "2907\n",
      "88\n",
      "138\n",
      "2207\n",
      "83\n",
      "1289\n",
      "285\n",
      "2042\n",
      "518\n",
      "2324\n",
      "2910\n",
      "3113\n",
      "2825\n",
      "2385\n",
      "3301\n",
      "1499\n",
      "2472\n",
      "18\n",
      "1660\n",
      "2842\n",
      "2454\n",
      "1114\n",
      "2347\n",
      "1134\n",
      "2189\n",
      "1335\n",
      "3212\n",
      "350\n",
      "2495\n",
      "992\n",
      "1384\n",
      "2600\n",
      "921\n",
      "3090\n",
      "1216\n",
      "2040\n",
      "3215\n",
      "2804\n",
      "3096\n",
      "2786\n",
      "1706\n",
      "87\n",
      "2279\n",
      "501\n",
      "885\n",
      "3282\n",
      "1765\n",
      "2236\n",
      "17\n",
      "447\n",
      "1830\n",
      "287\n",
      "481\n",
      "1817\n",
      "2036\n",
      "606\n",
      "3329\n",
      "837\n",
      "2749\n",
      "1210\n",
      "485\n",
      "1756\n",
      "2981\n",
      "1846\n",
      "2014\n",
      "1661\n",
      "1739\n",
      "187\n",
      "47\n",
      "1359\n",
      "3224\n",
      "1261\n",
      "1889\n",
      "779\n",
      "3083\n",
      "3291\n",
      "1815\n",
      "1725\n",
      "1684\n",
      "1594\n",
      "1875\n",
      "1865\n",
      "1797\n",
      "531\n",
      "716\n",
      "940\n",
      "1649\n",
      "1924\n",
      "776\n",
      "2433\n",
      "1785\n",
      "1709\n",
      "1628\n",
      "160\n",
      "476\n",
      "2908\n",
      "1283\n",
      "2589\n",
      "2801\n",
      "1567\n",
      "102\n",
      "1691\n",
      "1746\n",
      "2194\n",
      "683\n",
      "2944\n",
      "991\n",
      "2823\n",
      "444\n",
      "341\n",
      "2948\n",
      "2516\n",
      "1768\n",
      "1517\n",
      "116\n",
      "2667\n",
      "2431\n",
      "1230\n",
      "1442\n",
      "2469\n",
      "822\n",
      "2533\n",
      "1597\n",
      "1695\n",
      "803\n",
      "1972\n",
      "917\n",
      "13\n",
      "2407\n",
      "3311\n",
      "403\n",
      "3008\n",
      "820\n",
      "3074\n",
      "224\n",
      "3005\n",
      "3330\n",
      "3240\n",
      "2218\n",
      "1401\n",
      "1825\n",
      "1150\n",
      "3341\n",
      "764\n",
      "347\n",
      "1483\n",
      "913\n",
      "423\n",
      "213\n",
      "496\n",
      "1008\n",
      "433\n",
      "1646\n",
      "1563\n",
      "1197\n",
      "2253\n",
      "636\n",
      "3167\n",
      "1379\n",
      "2187\n",
      "28\n",
      "3213\n",
      "478\n",
      "361\n",
      "186\n",
      "438\n",
      "58\n",
      "2892\n",
      "1714\n",
      "1726\n",
      "1874\n",
      "2387\n",
      "1144\n",
      "1430\n",
      "2294\n",
      "2326\n",
      "3144\n",
      "1613\n",
      "1888\n",
      "493\n",
      "1867\n",
      "1126\n",
      "3138\n",
      "343\n",
      "534\n",
      "670\n",
      "597\n",
      "2552\n",
      "766\n",
      "2512\n",
      "1207\n",
      "2889\n",
      "1480\n",
      "619\n",
      "2369\n",
      "1911\n",
      "612\n",
      "2466\n",
      "293\n",
      "2191\n",
      "2069\n",
      "107\n",
      "2994\n",
      "1258\n",
      "2864\n",
      "1387\n",
      "68\n",
      "1491\n",
      "2089\n",
      "2672\n",
      "3206\n",
      "2899\n",
      "2441\n",
      "1788\n",
      "1121\n",
      "2574\n",
      "1557\n",
      "3191\n",
      "1677\n",
      "396\n",
      "651\n",
      "2136\n",
      "1780\n",
      "2592\n",
      "375\n",
      "1665\n",
      "1449\n",
      "1085\n",
      "431\n",
      "2154\n",
      "210\n",
      "2904\n",
      "2503\n",
      "2037\n",
      "2995\n",
      "1406\n",
      "2276\n",
      "2642\n",
      "2851\n",
      "815\n",
      "2590\n",
      "3110\n",
      "1249\n",
      "1467\n",
      "1339\n",
      "2473\n",
      "32\n",
      "2137\n",
      "782\n",
      "3173\n",
      "1781\n",
      "1478\n",
      "1964\n",
      "1064\n",
      "1059\n",
      "3176\n",
      "2557\n",
      "57\n",
      "1887\n",
      "2896\n",
      "1141\n",
      "878\n",
      "1847\n",
      "1614\n",
      "1721\n",
      "1\n",
      "2670\n",
      "686\n",
      "1417\n",
      "1801\n",
      "1506\n",
      "1821\n",
      "693\n",
      "2996\n",
      "2727\n",
      "3035\n",
      "2547\n",
      "1901\n",
      "1116\n",
      "80\n",
      "1284\n",
      "1930\n",
      "735\n",
      "1099\n",
      "274\n",
      "2491\n",
      "2485\n",
      "248\n",
      "1524\n",
      "3293\n",
      "1485\n",
      "2931\n",
      "1166\n",
      "1590\n",
      "512\n",
      "1604\n",
      "682\n",
      "327\n",
      "2307\n",
      "1407\n",
      "23\n",
      "2886\n",
      "3155\n",
      "960\n",
      "3304\n",
      "1115\n",
      "729\n",
      "1445\n",
      "2035\n",
      "1161\n",
      "1818\n",
      "1744\n",
      "2460\n",
      "2551\n",
      "1980\n",
      "2705\n",
      "695\n",
      "65\n",
      "2161\n",
      "830\n",
      "3150\n",
      "1302\n",
      "266\n",
      "2075\n",
      "288\n",
      "2828\n",
      "1550\n",
      "1525\n",
      "1189\n",
      "2071\n",
      "69\n",
      "1229\n",
      "1327\n",
      "2153\n",
      "658\n",
      "2617\n",
      "3263\n",
      "1119\n",
      "2982\n",
      "111\n",
      "479\n",
      "2010\n",
      "3129\n",
      "3119\n",
      "1421\n",
      "2442\n",
      "2692\n",
      "1124\n",
      "1014\n",
      "2810\n",
      "1946\n",
      "648\n",
      "3244\n",
      "1142\n",
      "722\n",
      "1056\n",
      "2336\n",
      "2839\n",
      "329\n",
      "2965\n",
      "1854\n",
      "739\n",
      "3351\n",
      "2760\n",
      "1479\n",
      "2024\n",
      "2228\n",
      "118\n",
      "1365\n",
      "2150\n",
      "3371\n",
      "919\n",
      "2519\n",
      "1477\n",
      "1094\n",
      "404\n",
      "2405\n",
      "775\n",
      "661\n",
      "97\n",
      "1006\n",
      "639\n",
      "147\n",
      "2017\n",
      "314\n",
      "2728\n",
      "2057\n",
      "359\n",
      "2701\n",
      "449\n",
      "3046\n",
      "2168\n",
      "465\n",
      "2752\n",
      "3174\n",
      "174\n",
      "1886\n",
      "1749\n",
      "3188\n",
      "2748\n",
      "2844\n",
      "1459\n",
      "773\n",
      "721\n",
      "1766\n",
      "2740\n",
      "603\n",
      "2581\n",
      "2596\n",
      "507\n",
      "458\n",
      "1364\n",
      "3166\n",
      "1465\n",
      "1243\n",
      "1030\n",
      "2700\n",
      "924\n",
      "228\n",
      "731\n",
      "1349\n",
      "1193\n",
      "559\n",
      "690\n",
      "1988\n",
      "3044\n",
      "989\n",
      "2394\n",
      "3127\n",
      "2691\n",
      "2768\n",
      "407\n",
      "1391\n",
      "601\n",
      "935\n",
      "2025\n",
      "1311\n",
      "1035\n",
      "1476\n",
      "644\n",
      "3266\n",
      "2980\n",
      "40\n",
      "2202\n",
      "1893\n",
      "888\n",
      "1641\n",
      "2123\n",
      "477\n",
      "3247\n",
      "1299\n",
      "2360\n",
      "3201\n",
      "1396\n",
      "1204\n",
      "1533\n",
      "1013\n",
      "1363\n",
      "1061\n",
      "805\n",
      "2790\n",
      "958\n",
      "946\n",
      "1650\n",
      "3050\n",
      "2230\n",
      "411\n",
      "1573\n",
      "615\n",
      "505\n",
      "2586\n",
      "539\n",
      "2765\n",
      "1906\n",
      "3280\n",
      "2595\n",
      "1313\n",
      "263\n",
      "3326\n",
      "2643\n",
      "1644\n",
      "290\n",
      "2427\n",
      "3151\n",
      "78\n",
      "574\n",
      "3112\n",
      "2613\n",
      "1423\n",
      "2374\n",
      "1487\n",
      "133\n",
      "289\n",
      "307\n",
      "2249\n",
      "1297\n",
      "446\n",
      "1025\n",
      "1002\n",
      "1787\n",
      "1171\n",
      "1276\n",
      "627\n",
      "767\n",
      "632\n",
      "1425\n",
      "1470\n",
      "1232\n",
      "2043\n",
      "770\n",
      "611\n",
      "2107\n",
      "643\n",
      "257\n",
      "1221\n",
      "3058\n",
      "1898\n",
      "3059\n",
      "1288\n",
      "1286\n",
      "973\n",
      "752\n",
      "1504\n",
      "592\n",
      "2459\n",
      "1031\n",
      "1307\n",
      "1869\n",
      "1411\n",
      "789\n",
      "2758\n",
      "218\n",
      "254\n",
      "1837\n",
      "2500\n",
      "3290\n",
      "3087\n",
      "564\n",
      "3017\n",
      "1343\n",
      "2007\n",
      "197\n",
      "2730\n",
      "2939\n",
      "2209\n",
      "2736\n",
      "358\n",
      "75\n",
      "3125\n",
      "2578\n",
      "2935\n",
      "2403\n",
      "494\n",
      "521\n",
      "1849\n",
      "2366\n",
      "566\n",
      "467\n",
      "3098\n",
      "376\n",
      "1834\n",
      "2836\n",
      "570\n",
      "620\n",
      "979\n",
      "2497\n",
      "808\n",
      "833\n",
      "3258\n",
      "2707\n",
      "2363\n",
      "741\n",
      "308\n",
      "3348\n",
      "2618\n",
      "1399\n",
      "1631\n",
      "1259\n",
      "2576\n",
      "2457\n",
      "3079\n",
      "2614\n",
      "2546\n",
      "1716\n",
      "1051\n",
      "684\n",
      "332\n",
      "609\n",
      "380\n",
      "2277\n",
      "1745\n",
      "1186\n",
      "2507\n",
      "384\n",
      "2034\n",
      "1076\n",
      "324\n",
      "265\n",
      "96\n",
      "2293\n",
      "2295\n",
      "1950\n",
      "3333\n",
      "2200\n",
      "1093\n",
      "105\n",
      "1340\n",
      "817\n",
      "2049\n",
      "3358\n",
      "1222\n",
      "727\n",
      "331\n",
      "2479\n",
      "2625\n",
      "2909\n",
      "2273\n",
      "1151\n",
      "1717\n",
      "3051\n",
      "2196\n",
      "577\n",
      "412\n",
      "1530\n",
      "365\n",
      "284\n",
      "2417\n",
      "1070\n",
      "1018\n",
      "2778\n",
      "1640\n",
      "884\n",
      "596\n",
      "569\n",
      "714\n",
      "3076\n",
      "846\n",
      "3256\n",
      "1123\n",
      "2406\n",
      "2061\n",
      "762\n",
      "1908\n",
      "183\n",
      "1028\n",
      "3372\n",
      "1910\n",
      "1149\n",
      "3006\n",
      "1663\n",
      "2354\n",
      "1729\n",
      "3268\n",
      "1938\n",
      "3103\n",
      "578\n",
      "250\n",
      "554\n",
      "2976\n",
      "3183\n",
      "1342\n",
      "2686\n",
      "2789\n",
      "2663\n",
      "967\n",
      "3117\n",
      "2444\n",
      "1839\n",
      "3071\n",
      "1251\n",
      "1431\n",
      "1922\n",
      "1468\n",
      "165\n",
      "217\n",
      "537\n",
      "1579\n",
      "1705\n",
      "415\n",
      "1615\n",
      "2785\n",
      "2375\n",
      "2662\n",
      "3328\n",
      "1763\n",
      "1505\n",
      "2131\n",
      "64\n",
      "3053\n",
      "1053\n",
      "3219\n",
      "2467\n",
      "1248\n",
      "2322\n",
      "974\n",
      "948\n",
      "787\n",
      "2897\n",
      "383\n",
      "2357\n",
      "3023\n",
      "1032\n",
      "2954\n",
      "653\n",
      "801\n",
      "2964\n",
      "379\n",
      "1909\n",
      "880\n",
      "2732\n",
      "3168\n",
      "3078\n",
      "1160\n",
      "811\n",
      "1494\n",
      "1055\n",
      "1239\n",
      "1388\n",
      "2975\n",
      "826\n",
      "1042\n",
      "685\n",
      "1948\n",
      "3279\n",
      "212\n",
      "529\n",
      "1783\n",
      "191\n",
      "2434\n",
      "941\n",
      "264\n",
      "2647\n",
      "90\n",
      "2712\n",
      "1836\n",
      "2520\n",
      "3300\n",
      "2961\n",
      "3069\n",
      "2188\n",
      "1890\n",
      "998\n",
      "487\n",
      "311\n",
      "963\n",
      "565\n",
      "1687\n",
      "2829\n",
      "1158\n",
      "70\n",
      "364\n",
      "1133\n",
      "3084\n",
      "3134\n",
      "2933\n",
      "1471\n",
      "3343\n",
      "3115\n",
      "2822\n",
      "645\n",
      "2169\n",
      "2847\n",
      "2325\n",
      "498\n",
      "2083\n",
      "2177\n",
      "3296\n",
      "3148\n",
      "1240\n",
      "798\n",
      "2619\n",
      "1548\n",
      "1678\n",
      "660\n",
      "2856\n",
      "448\n",
      "1004\n",
      "1272\n",
      "2658\n",
      "1624\n",
      "2966\n",
      "157\n",
      "2646\n",
      "1596\n",
      "473\n",
      "1508\n",
      "849\n",
      "71\n",
      "1995\n",
      "262\n",
      "1577\n",
      "707\n",
      "866\n",
      "253\n",
      "440\n",
      "3272\n",
      "1252\n",
      "3149\n",
      "1605\n",
      "2176\n",
      "816\n",
      "3136\n",
      "689\n",
      "434\n",
      "1996\n",
      "1840\n",
      "500\n",
      "234\n",
      "3024\n",
      "1544\n",
      "2305\n",
      "33\n",
      "221\n",
      "85\n",
      "1791\n",
      "2601\n",
      "305\n",
      "1329\n",
      "1540\n",
      "824\n",
      "2947\n",
      "1657\n",
      "1233\n",
      "27\n",
      "1602\n",
      "2837\n",
      "2261\n",
      "3294\n",
      "2834\n",
      "472\n",
      "1063\n",
      "1947\n",
      "1475\n",
      "2112\n",
      "2259\n",
      "600\n",
      "1647\n",
      "1941\n",
      "2569\n",
      "2957\n",
      "861\n",
      "16\n",
      "3317\n",
      "2657\n",
      "1016\n",
      "936\n",
      "2967\n",
      "2843\n",
      "3234\n",
      "3013\n",
      "630\n",
      "1163\n",
      "2190\n",
      "277\n",
      "925\n",
      "680\n",
      "2983\n",
      "2365\n",
      "1075\n",
      "159\n",
      "2609\n",
      "2456\n",
      "1153\n",
      "439\n",
      "3007\n",
      "194\n",
      "2312\n",
      "1704\n",
      "544\n",
      "1989\n",
      "2364\n",
      "1168\n",
      "3203\n",
      "301\n",
      "1827\n",
      "1273\n",
      "1527\n",
      "137\n",
      "241\n",
      "184\n",
      "2999\n",
      "2815\n",
      "1620\n",
      "53\n",
      "3064\n",
      "915\n",
      "1374\n",
      "2919\n",
      "1965\n",
      "3126\n",
      "2219\n",
      "1080\n",
      "1774\n",
      "2763\n",
      "2556\n",
      "844\n",
      "1552\n",
      "74\n",
      "702\n",
      "3242\n",
      "3245\n",
      "1238\n",
      "2874\n",
      "2536\n",
      "1553\n",
      "2090\n",
      "1792\n",
      "1316\n",
      "2922\n",
      "835\n",
      "777\n",
      "155\n",
      "2564\n",
      "2906\n",
      "1278\n",
      "3281\n",
      "2401\n",
      "905\n",
      "3108\n",
      "247\n",
      "556\n",
      "667\n",
      "393\n",
      "598\n",
      "2315\n",
      "3061\n",
      "2300\n",
      "828\n",
      "1740\n",
      "2301\n",
      "2693\n",
      "997\n",
      "1551\n",
      "1997\n",
      "2970\n",
      "1735\n",
      "1776\n",
      "1634\n",
      "962\n",
      "55\n",
      "1220\n",
      "2348\n",
      "854\n",
      "2321\n",
      "879\n",
      "3305\n",
      "2076\n",
      "164\n",
      "2649\n",
      "3032\n",
      "903\n",
      "2604\n",
      "2998\n",
      "3001\n",
      "2232\n",
      "2314\n",
      "3353\n",
      "3252\n",
      "2344\n",
      "1718\n",
      "2832\n",
      "22\n",
      "599\n",
      "1981\n",
      "216\n",
      "508\n",
      "5\n",
      "834\n",
      "2826\n",
      "1377\n",
      "208\n",
      "1237\n",
      "2206\n",
      "3094\n",
      "2\n",
      "561\n",
      "1484\n",
      "2111\n",
      "182\n",
      "0\n",
      "1434\n",
      "1670\n",
      "931\n",
      "3297\n",
      "3159\n",
      "1318\n",
      "2841\n",
      "3073\n",
      "2436\n",
      "1448\n",
      "504\n",
      "2058\n",
      "95\n",
      "2216\n",
      "1916\n",
      "2116\n",
      "2782\n",
      "671\n",
      "1833\n",
      "48\n",
      "1855\n",
      "1300\n",
      "243\n",
      "1621\n",
      "3259\n",
      "2537\n",
      "362\n",
      "2113\n",
      "lables\n",
      "[1870, 491, 3052, 1700, 2382, 132, 783, 1643, 209, 2392, 442, 1609, 1609, 2895, 406, 2689, 2754, 89, 299, 1382, 1096, 1096, 1824, 1824, 1208, 1208, 1689, 980, 2924, 2015, 321, 2799, 1213, 2802, 2802, 2802, 2109, 2553, 2553, 2635, 2175, 2275, 42, 3192, 641, 1937, 1978, 2587, 2587, 754, 1843, 1843, 1592, 1592, 516, 2416, 3232, 3232, 3232, 558, 1591, 2524, 970, 1306, 1306, 2184, 859, 676, 737, 737, 3118, 229, 1904, 134, 2817, 2817, 1810, 3298, 2927, 3036, 3036, 2462, 1350, 196, 659, 1108, 1561, 371, 2664, 2094, 704, 704, 2352, 2352, 73, 1140, 1140, 580, 786, 2593, 855, 1154, 1585, 2051, 986, 1247, 2311, 3047, 3223, 3223, 3223, 2050, 2271, 2271, 2271, 370, 2452, 2439, 2439, 43, 43, 337, 2764, 2764, 1648, 3264, 1199, 3316, 3316, 1386, 295, 1979, 1979, 226, 2973, 1287, 1807, 2201, 840, 3018, 161, 3031, 1428, 2979, 724, 724, 3238, 1159, 2968, 2280, 455, 1403, 2611, 3318, 2562, 2429, 2911, 914, 916, 91, 91, 818, 818, 2223, 2223, 642, 3236, 757, 3322, 3322, 1496, 3141, 868, 868, 3034, 1196, 3261, 1885, 928, 2019, 2019, 188, 3030, 1543, 1733, 2714, 3199, 3199, 3163, 400, 2733, 278, 2353, 2338, 2900, 1048, 109, 144, 930, 930, 1802, 1802, 14, 2018, 2766, 1955, 1435, 2087, 1338, 2608, 2608, 190, 190, 2180, 2235, 2103, 1803, 1803, 79, 3095, 1515, 1696, 1696, 1696, 2486, 2710, 732, 1808, 405, 2425, 1931, 245, 1983, 2684, 2248, 239, 2368, 2368, 3211, 25, 1574, 1574, 3325, 3338, 1092, 1092, 468, 468, 1188, 2921, 1427, 426, 1078, 1879, 2548, 557, 1900, 2992, 2992, 2890, 1333, 1954, 1736, 1736, 2475, 302, 2898, 199, 199, 201, 1250, 355, 355, 2634, 334, 428, 1903, 1618, 1358, 2858, 272, 252, 2102, 3273, 2751, 711, 2866, 1433, 163, 1203, 128, 195, 2971, 540, 1919, 3045, 1761, 2055, 2424, 2424, 1095, 1023, 3368, 3368, 927, 699, 1182, 1625, 3014, 2449, 3362, 3362, 3362, 1570, 1570, 1209, 2162, 1895, 1537, 77, 2579, 2579, 2940, 2940, 1019, 2857, 2203, 2203, 2584, 2278, 3139, 2243, 2081, 2081, 1206, 1951, 1951, 429, 1880, 1155, 3152, 2143, 2458, 1599, 1742, 150, 1102, 1404, 1731, 567, 2530, 1842, 1842, 2411, 2270, 2270, 286, 2167, 2894, 1215, 523, 1638, 2056, 519, 356, 1856, 678, 678, 322, 2008, 1456, 2916, 1593, 1593, 3124, 1356, 3204, 1611, 1065, 607, 3162, 1639, 860, 3218, 1926, 1926, 3288, 267, 1719, 3302, 2561, 1074, 1191, 703, 437, 3209, 3320, 956, 956, 1907, 2489, 419, 459, 945, 945, 2869, 2959, 738, 738, 1587, 1587, 1266, 2362, 259, 259, 3077, 3277, 2298, 2298, 3361, 2204, 139, 2174, 26, 939, 145, 943, 2172, 562, 614, 614, 2930, 982, 2717, 2865, 3178, 3178, 1424, 2809, 1828, 2598, 3231, 3158, 3158, 1375, 1375, 1103, 1103, 1264, 1264, 1264, 829, 1027, 2742, 2742, 3169, 1473, 2880, 306, 2633, 3366, 3366, 294, 1845, 2445, 1024, 3289, 2711, 865, 2722, 3171, 959, 959, 1920, 1562, 323, 323, 323, 2398, 985, 1999, 1395, 3336, 2622, 785, 2062, 640, 1081, 2659, 994, 1603, 1418, 1418, 1418, 319, 2502, 3284, 2419, 1984, 3123, 1899, 2021, 2021, 3065, 3065, 1068, 3365, 3037, 2781, 2781, 338, 1559, 2274, 398, 679, 1612, 2148, 717, 717, 2247, 175, 2378, 1582, 35, 1482, 2208, 1564, 2299, 2299, 2708, 1323, 637, 1290, 688, 3195, 3195, 2588, 1097, 2320, 2320, 2320, 2997, 3107, 2465, 1148, 2784, 1279, 261, 261, 2660, 2660, 2721, 3349, 1918, 1750, 1185, 1185, 2178, 1277, 1341, 483, 1627, 799, 2238, 825, 900, 1503, 1503, 1503, 2432, 2432, 514, 1876, 104, 2099, 2987, 788, 788, 1315, 1522, 568, 3346, 999, 999, 2666, 1321, 1460, 2361, 1073, 2690, 1809, 3262, 3000, 850, 34, 124, 2011, 2011, 2011, 1569, 2688, 3063, 2934, 2934, 381, 2797, 2797, 1464, 344, 807, 807, 807, 2871, 3089, 3104, 1325, 3237, 2020, 3357, 1676, 1241, 1241, 572, 172, 672, 2605, 733, 1131, 2808, 2808, 814, 1507, 1383, 2903, 2534, 3114, 474, 2291, 1010, 2671, 3102, 2438, 2960, 1772, 1510, 744, 551, 551, 1993, 1606, 694, 715, 1336, 2471, 146, 2395, 536, 374, 587, 148, 1715, 1715, 2437, 790, 1125, 1125, 1722, 3177, 595, 2914, 1214, 771, 771, 413, 317, 3153, 3153, 740, 740, 657, 2132, 1738, 1067, 1067, 1067, 652, 2358, 2367, 1588, 1588, 1177, 372, 1370, 2179, 1376, 2144, 2144, 2072, 1632, 911, 1881, 2303, 2816, 282, 2863, 3088, 2950, 2950, 3081, 858, 2746, 497, 993, 1576, 1576, 1235, 1511, 1770, 1091, 1091, 1285, 3101, 3194, 3194, 2517, 2517, 2517, 3254, 3254, 6, 2281, 1368, 1368, 2762, 780, 1822, 877, 1673, 3105, 3085, 3310, 3260, 1036, 951, 1176, 2793, 3189, 542, 1698, 1054, 3060, 2313, 3186, 2743, 1921, 1831, 2905, 1500, 912, 2767, 3164, 3164, 2073, 2073, 1962, 2720, 1047, 2258, 2129, 2447, 1332, 2327, 692, 1883, 1883, 1669, 1669, 555, 957, 268, 886, 2222, 1539, 1779, 961, 938, 1441, 1441, 1441, 61, 1982, 1516, 1516, 2257, 1767, 3347, 3347, 2371, 2371, 2881, 2775, 486, 1860, 2342, 2342, 2342, 2342, 3332, 82, 326, 1536, 876, 1369, 2985, 821, 173, 2381, 734, 734, 2867, 443, 1757, 2878, 2215, 3146, 2319, 1806, 1806, 2800, 1820, 460, 2549, 204, 204, 593, 1813, 1457, 1953, 862, 222, 2893, 3200, 425, 2610, 972, 2761, 1488, 1234, 2182, 2262, 1432, 1432, 1432, 2607, 3067, 103, 103, 3239, 538, 538, 538, 538, 922, 922, 3355, 1547, 219, 548, 548, 1045, 3278, 3278, 3012, 1769, 2302, 2302, 576, 1132, 1481, 1481, 1866, 3312, 2606, 1101, 1545, 1545, 1545, 1864, 1864, 2776, 273, 2060, 3324, 2170, 2861, 1688, 2630, 3092, 2470, 342, 629, 2133, 492, 1324, 1935, 2106, 2106, 2106, 2106, 3202, 1535, 2337, 2499, 2499, 122, 1701, 3142, 2272, 590, 590, 2941, 151, 151, 2041, 1617, 853, 1122, 1026, 456, 2750, 2511, 2328, 2585, 2585, 31, 31, 668, 3055, 3027, 2654, 2654, 1184, 349, 3075, 2571, 2571, 1985, 1112, 3175, 2160, 662, 1069, 2065, 1490, 2426, 2426, 3198, 3198, 1275, 1275, 2121, 2121, 15, 2241, 1737, 1737, 509, 1355, 2962, 2568, 823, 832, 100, 1991, 1991, 1934, 1469, 490, 2285, 1994, 1518, 2942, 2195, 2498, 2496, 2496, 2496, 673, 966, 1502, 1529, 1862, 1960, 2343, 2673, 1194, 1961, 2346, 2013, 774, 774, 2412, 1113, 794, 794, 968, 1071, 706, 2333, 2650, 2650, 2650, 1372, 3093, 214, 2806, 765, 2286, 843, 464, 3235, 3235, 3235, 1975, 2870, 2870, 1816, 1816, 2612, 3147, 2390, 1607, 2724, 2745, 1164, 185, 2535, 2535, 1932, 1015, 2529, 2529, 1957, 3323, 2220, 796, 796, 796, 1440, 180, 3187, 2756, 3327, 1270, 3121, 3, 3, 2126, 1674, 560, 1437, 1257, 2945, 2945, 2747, 2791, 98, 223, 2086, 2626, 2329, 198, 3082, 475, 1558, 1878, 2477, 2477, 1052, 929, 2824, 2824, 2824, 2563, 2949, 649, 1334, 368, 368, 60, 1354, 2955, 1298, 813, 2538, 1255, 2792, 2224, 1692, 1835, 1211, 3004, 2616, 2306, 2902, 2884, 1966, 1398, 39, 125, 1521, 1521, 2636, 546, 918, 2859, 2234, 1320, 1012, 1170, 3111, 2110, 430, 158, 441, 1314, 2734, 2814, 2814, 2289, 1759, 3374, 646, 2680, 2680, 2580, 1129, 1877, 2774, 220, 303, 303, 631, 2876, 3287, 3287, 2628, 2628, 1117, 2044, 168, 168, 168, 1183, 143, 2993, 1498, 1498, 2641, 2641, 2820, 1686, 3350, 2152, 1373, 1419, 971, 971, 873, 1474, 1450, 30, 1675, 1556, 2026, 2026, 2681, 563, 563, 3367, 2400, 2400, 1198, 1198, 1928, 763, 2115, 3041, 3041, 72, 72, 52, 1058, 2397, 2522, 893, 1743, 1179, 2114, 2114, 1173, 1173, 1173, 1381, 1381, 2124, 1871, 1753, 1753, 1409, 1409, 1390, 1390, 753, 976, 976, 2108, 2787, 2888, 2888, 1681, 1589, 1589, 892, 892, 2936, 2936, 2798, 2798, 1734, 1734, 726, 1852, 401, 193, 1659, 3241, 3116, 162, 1347, 2492, 2492, 2297, 389, 3193, 3193, 2811, 1242, 1455, 582, 2402, 480, 2855, 2570, 2082, 2082, 2651, 2421, 2031, 795, 2130, 1489, 427, 81, 81, 81, 995, 995, 701, 1863, 2068, 2068, 2476, 2476, 3233, 2318, 2318, 2318, 845, 1348, 1805, 819, 772, 2560, 2560, 2193, 232, 3359, 2455, 1152, 1939, 1939, 1939, 3016, 1005, 1172, 2078, 2078, 2119, 1246, 1295, 1295, 1956, 1453, 1453, 1453, 2540, 2830, 1690, 2920, 1925, 269, 902, 902, 650, 3056, 1513, 1762, 3070, 3128, 1326, 3208, 720, 328, 1201, 1201, 2263, 2263, 2263, 2422, 654, 654, 654, 654, 2854, 864, 602, 1353, 1353, 718, 1190, 831, 581, 990, 2088, 1549, 987, 987, 395, 410, 2265, 382, 1600, 1267, 687, 1990, 841, 2514, 2853, 1619, 1619, 2718, 1145, 1683, 2349, 1747, 1538, 2840, 3292, 2000, 1181, 330, 330, 281, 1732, 666, 2104, 2122, 2559, 1044, 2290, 2012, 2532, 3205, 2675, 2675, 2675, 2531, 1754, 59, 270, 883, 883, 2048, 2079, 882, 3039, 3039, 170, 3086, 1228, 2287, 2287, 1512, 238, 2027, 2652, 1089, 2005, 906, 926, 800, 1127, 1127, 2308, 1892, 1940, 1682, 2023, 3057, 2780, 618, 584, 584, 2925, 1346, 1303, 2599, 1400, 3130, 863, 1927, 2885, 2030, 2372, 2372, 2991, 1461, 1118, 3344, 3344, 352, 1020, 1020, 527, 1256, 1429, 1429, 3197, 2577, 1859, 1859, 983, 2952, 608, 354, 1138, 2575, 2977, 2159, 231, 2544, 1568, 1393, 2233, 838, 2679, 1501, 899, 1451, 2661, 2420, 76, 2370, 2003, 889, 889, 2694, 2091, 2091, 2091, 2391, 1446, 1274, 2221, 2221, 760, 2053, 3315, 2173, 2032, 1322, 1322, 1447, 2713, 1137, 1575, 2738, 2738, 1463, 1752, 1752, 1752, 1896, 2096, 1829, 511, 1629, 2097, 2493, 2493, 953, 2668, 2827, 2139, 225, 3154, 3154, 2860, 131, 1218, 669, 2715, 140, 1086, 2543, 3180, 377, 2250, 1905, 1905, 1826, 2879, 933, 3143, 1060, 2046, 1812, 1352, 1352, 3185, 422, 1416, 2284, 2284, 2284, 2029, 416, 857, 3048, 3145, 1998, 3369, 3369, 1897, 101, 3042, 318, 318, 2047, 2623, 2245, 291, 291, 1581, 2430, 2703, 1666, 2555, 2461, 2461, 2972, 2380, 2638, 2539, 1967, 3054, 3054, 4, 1633, 1800, 1000, 2100, 3181, 2077, 3010, 3010, 1195, 920, 1509, 1509, 1007, 1571, 2687, 1942, 2268, 809, 2414, 2414, 2414, 495, 141, 2163, 2163, 2192, 613, 1973, 29, 357, 357, 2875, 271, 1260, 3020, 3020, 1784, 2481, 2481, 2481, 2481, 1672, 1672, 2526, 1011, 2873, 1439, 45, 1174, 633, 2443, 2443, 3226, 3226, 2214, 402, 2882, 2882, 1679, 2989, 471, 471, 192, 3370, 1944, 526, 949, 2788, 1296, 3339, 450, 3295, 1823, 907, 2483, 3255, 2943, 2386, 836, 545, 387, 947, 947, 304, 728, 728, 2795, 1037, 755, 2418, 2418, 2821, 628, 1304, 1493, 1751, 665, 391, 2937, 2678, 292, 1466, 2141, 1309, 2198, 2210, 784, 932, 2487, 179, 996, 385, 1452, 1452, 2807, 325, 2309, 421, 1062, 3373, 2545, 2545, 2938, 409, 409, 2399, 2399, 2399, 1917, 909, 1217, 1225, 2528, 681, 681, 681, 510, 3356, 3216, 1415, 1415, 1415, 436, 436, 1811, 622, 1707, 2849, 2849, 713, 1263, 2695, 2582, 99, 99, 300, 390, 390, 390, 390, 167, 1454, 2757, 1531, 1741, 697, 697, 2389, 1664, 1402, 86, 3337, 3286, 1775, 1775, 1775, 1223, 2725, 3363, 901, 2384, 2907, 88, 138, 2207, 83, 1289, 285, 2042, 518, 2324, 2910, 2910, 3113, 2385, 3301, 1499, 2472, 18, 1660, 2454, 1114, 2347, 1134, 2189, 1335, 3212, 350, 2495, 992, 2600, 2600, 921, 3090, 1216, 2040, 3215, 2804, 3096, 2786, 1706, 87, 2279, 2279, 501, 885, 3282, 1765, 2236, 17, 447, 1830, 1830, 287, 481, 1817, 2036, 2036, 606, 3329, 3329, 837, 2749, 1210, 2981, 1846, 2014, 1661, 1739, 47, 1359, 3224, 1261, 1889, 779, 779, 3083, 3291, 1815, 1725, 1684, 1594, 1875, 1875, 1865, 1797, 716, 940, 1649, 1924, 776, 2433, 2433, 1785, 1785, 1709, 1628, 476, 2908, 1283, 2589, 1567, 1567, 1691, 1691, 1746, 2194, 2194, 683, 683, 2944, 991, 2823, 341, 2516, 1768, 1768, 1517, 1517, 116, 2431, 1230, 1230, 1442, 1442, 1442, 2469, 822, 2533, 1597, 1695, 1972, 917, 13, 2407, 2407, 3311, 403, 3008, 820, 820, 3074, 224, 224, 3005, 3240, 2218, 1401, 1825, 1150, 3341, 764, 764, 347, 1483, 913, 423, 213, 496, 1008, 433, 433, 1646, 1563, 1197, 2253, 636, 3167, 1379, 2187, 28, 28, 3213, 3213, 478, 361, 186, 438, 438, 58, 2892, 2892, 1714, 1726, 1874, 2387, 1144, 1144, 1430, 2294, 2326, 3144, 1613, 1888, 493, 1867, 1126, 3138, 343, 534, 670, 597, 2552, 766, 2512, 1207, 2889, 1480, 619, 619, 619, 2369, 1911, 612, 2466, 293, 2191, 2069, 107, 2994, 1258, 2864, 1387, 68, 1491, 2089, 2672, 3206, 2899, 2899, 2899, 2441, 2441, 1788, 1121, 2574, 1557, 3191, 1677, 396, 2136, 1780, 2592, 375, 1665, 1449, 1085, 1085, 431, 2154, 210, 210, 2904, 2503, 2037, 2995, 2995, 1406, 2276, 2642, 2851, 815, 2590, 2590, 2590, 2590, 3110, 1249, 1467, 1339, 2473, 2473, 32, 2137, 782, 3173, 1781, 1478, 1964, 1064, 1059, 3176, 2557, 57, 57, 1887, 2896, 1141, 878, 878, 1847, 1847, 1614, 1721, 1, 2670, 2670, 1417, 1801, 1506, 1506, 1506, 1821, 1821, 693, 2996, 2727, 3035, 2547, 1901, 1116, 80, 1284, 1930, 735, 1099, 1099, 274, 2491, 2485, 248, 1524, 3293, 1485, 1166, 1590, 1590, 512, 1604, 682, 682, 327, 2307, 1407, 1407, 23, 23, 2886, 3155, 960, 960, 960, 3304, 3304, 1115, 1115, 729, 1445, 2035, 1161, 1818, 1818, 1818, 1744, 2460, 2551, 1980, 2705, 2705, 695, 65, 2161, 830, 3150, 1302, 266, 2075, 288, 2828, 1525, 1189, 1189, 2071, 2071, 1229, 1229, 2153, 658, 2617, 3263, 3263, 1119, 1119, 2982, 111, 479, 2010, 3129, 3119, 1421, 2442, 2442, 2692, 1124, 1014, 2810, 1946, 648, 3244, 722, 722, 1056, 2336, 2839, 329, 2965, 1854, 739, 3351, 2760, 2760, 1479, 2024, 2228, 118, 1365, 2150, 3371, 919, 1477, 1477, 1094, 404, 2405, 775, 661, 97, 97, 97, 1006, 639, 147, 314, 2728, 359, 2701, 2701, 449, 3046, 2168, 2752, 2752, 3174, 174, 1886, 1886, 1749, 3188, 3188, 2748, 2748, 2844, 2844, 1459, 773, 721, 1766, 2740, 603, 2581, 2596, 507, 458, 1364, 3166, 1465, 1465, 1243, 1030, 2700, 924, 228, 731, 1349, 1193, 1193, 559, 559, 690, 1988, 3044, 989, 2394, 2394, 3127, 2691, 2768, 407, 1391, 601, 935, 2025, 1311, 1035, 1476, 644, 3266, 2980, 40, 2202, 1893, 888, 1641, 2123, 477, 3247, 1299, 2360, 2360, 1396, 1204, 1533, 1533, 1013, 1363, 1061, 805, 2790, 958, 946, 1650, 3050, 3050, 2230, 411, 1573, 615, 505, 2586, 539, 2765, 1906, 3280, 2595, 2595, 1313, 263, 3326, 1644, 290, 290, 2427, 3151, 78, 574, 3112, 2613, 1423, 2374, 1487, 133, 289, 307, 2249, 1297, 446, 1025, 1002, 1787, 1171, 1276, 627, 767, 632, 1425, 1470, 1232, 2043, 770, 611, 2107, 643, 257, 257, 1221, 3058, 1898, 3059, 1288, 1286, 973, 752, 752, 1504, 592, 2459, 1031, 1307, 1869, 1869, 1411, 789, 2758, 218, 254, 1837, 2500, 3290, 3087, 564, 3017, 1343, 1343, 2007, 197, 2730, 2939, 2209, 2209, 2736, 2736, 358, 75, 75, 3125, 2578, 2935, 2403, 494, 521, 1849, 566, 467, 3098, 376, 376, 2836, 570, 620, 620, 979, 2497, 808, 808, 833, 3258, 3258, 2707, 2363, 741, 308, 2618, 2618, 2618, 1399, 1631, 1259, 2576, 2457, 2457, 3079, 2546, 1716, 1716, 1051, 684, 332, 609, 380, 2277, 2277, 1745, 1186, 2507, 384, 2034, 1076, 324, 324, 265, 96, 2295, 1950, 3333, 2200, 1093, 105, 1340, 817, 2049, 3358, 3358, 1222, 727, 727, 331, 331, 2479, 2625, 2909, 2273, 2273, 1151, 1717, 3051, 2196, 577, 577, 577, 412, 1530, 365, 284, 284, 2417, 1070, 1018, 2778, 1640, 596, 569, 714, 3076, 846, 846, 3256, 1123, 2406, 2061, 762, 762, 1908, 183, 1028, 3372, 1910, 1149, 3006, 1663, 2354, 1729, 3268, 1938, 3103, 578, 250, 554, 554, 2976, 3183, 2686, 2789, 2663, 967, 3117, 2444, 1839, 1839, 1839, 1839, 1839, 1839, 3071, 1251, 1431, 1922, 1468, 165, 217, 537, 1579, 1705, 415, 1615, 2785, 2375, 2662, 3328, 1763, 2131, 64, 3053, 1053, 3219, 2467, 1248, 2322, 974, 948, 787, 2897, 383, 2357, 2357, 3023, 1032, 1032, 2954, 653, 801, 2964, 379, 1909, 880, 2732, 3168, 3078, 3078, 3078, 1160, 811, 1494, 1055, 1239, 1388, 2975, 826, 685, 1948, 3279, 212, 529, 1783, 1783, 191, 2434, 941, 264, 2647, 90, 2712, 1836, 2520, 3300, 3300, 3300, 2961, 3069, 2188, 1890, 998, 487, 311, 963, 565, 1687, 2829, 1158, 70, 364, 1133, 3084, 3134, 3134, 2933, 1471, 3115, 2822, 2822, 645, 2169, 2847, 2325, 498, 2083, 2177, 2177, 3296, 3296, 3148, 3148, 1240, 798, 2619, 1548, 1678, 660, 2856, 448, 1004, 2658, 2658, 1624, 2966, 157, 2646, 1596, 473, 1508, 1508, 849, 71, 1995, 262, 1577, 707, 707, 707, 707, 866, 253, 440, 3272, 1252, 3149, 3149, 1605, 2176, 816, 3136, 689, 434, 1996, 1840, 234, 3024, 1544, 1544, 2305, 33, 221, 85, 1791, 2601, 305, 305, 1329, 1540, 824, 2947, 1657, 1233, 1233, 27, 1602, 2837, 2261, 2261, 2261, 2261, 3294, 2834, 472, 1063, 1947, 1475, 2112, 2259, 600, 600, 1647, 1941, 2569, 2957, 861, 16, 3317, 2657, 1016, 1016, 936, 936, 2967, 2967, 2843, 2843, 3013, 630, 1163, 2190, 277, 925, 680, 2983, 2983, 2365, 1075, 159, 159, 2609, 2456, 1153, 439, 439, 3007, 194, 2312, 1704, 544, 1989, 2364, 1168, 1168, 3203, 3203, 301, 1827, 1273, 1273, 1527, 137, 184, 2999, 2815, 1620, 53, 53, 3064, 915, 915, 1374, 2919, 1965, 3126, 2219, 1080, 1080, 1774, 2763, 2556, 844, 1552, 3242, 1238, 2874, 2874, 2874, 2536, 2536, 1553, 2090, 1792, 1316, 2922, 835, 777, 155, 155, 2564, 2906, 1278, 1278, 3281, 2401, 2401, 905, 3108, 247, 556, 667, 393, 598, 2315, 3061, 2300, 2300, 828, 1740, 2301, 2693, 997, 1551, 1997, 1997, 2970, 1735, 1735, 1776, 1634, 962, 55, 1220, 1220, 1220, 2348, 2321, 2321, 2321, 879, 2076, 2076, 2649, 3032, 903, 2604, 2998, 3001, 3001, 2232, 2314, 3353, 3353, 3252, 2344, 1718, 1718, 1718, 2832, 22, 22, 599, 1981, 216, 508, 5, 834, 2826, 1377, 208, 1237, 2206, 3094, 3094, 2, 561, 1484, 1484, 2111, 2111, 182, 182, 0, 0, 1434, 1670, 931, 3297, 3159, 3159, 3159, 3159, 3159, 1318, 2841, 3073, 2436, 1448, 504, 2058, 2058, 95, 2216, 2216, 2216, 1916, 1916, 2116, 2782, 671, 1833, 48, 1855, 1300, 243, 1621, 1621, 2537, 362, 2113]\n",
      "gender_current\n",
      "[0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "images, labels, gender = get_images_and_labels('Label_Images_Gender')\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"tmp_gender.pickle\", \"rb\") as f:\n",
    "    images, labels, gender = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"tmp_gender.pickle\", \"wb\") as f:\n",
    "    pickle.dump((images,labels,gender), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#res_images = []\n",
    "res_gender = []\n",
    "\n",
    "#for img in images:\n",
    "#    res_images.append(img.reshape(1,20000))    # (100*100 image size)\n",
    "\n",
    "\n",
    "for gen in gender:\n",
    "    res_gender.append(np.array(gen))\n",
    "\n",
    "res_gender = np.array(res_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2908,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_gender.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model parameters as external flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic model parameters as external flags.\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer('hidden1', 1500, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 1000, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_integer('hidden3', 500, 'Number of units in hidden layer 3.')\n",
    "flags.DEFINE_integer('batch_size', 100, 'Batch size.  '\n",
    "                     'Must divide evenly into the dataset sizes.')\n",
    "flags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')\n",
    "flags.DEFINE_boolean('fake_data', False, 'If true, uses fake data '\n",
    "                     'for unit testing.')\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 100\n",
    "#CHANNELS = 3\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_nodes = [IMAGE_PIXELS, 1500, 1000, 500, NUM_CLASSES]\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def inference(images, hidden1_units, hidden2_units):\n",
    "#     # Hidden 1\n",
    "#     with tf.name_scope('hidden1'):\n",
    "#         weights = tf.Variable(\n",
    "#             tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "#                                 stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "#             name='weights')\n",
    "#         biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "#                              name='biases')\n",
    "#         hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "#     # Hidden 2\n",
    "#     with tf.name_scope('hidden2'):\n",
    "#         weights = tf.Variable(\n",
    "#             tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "#                                 stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "#             name='weights')\n",
    "#         biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "#                              name='biases')\n",
    "#         hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "#     # Linear\n",
    "#     with tf.name_scope('softmax_linear'):\n",
    "#         weights = tf.Variable(\n",
    "#             tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "#                                 stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "#             name='weights')\n",
    "#         biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "#                              name='biases')\n",
    "#         logits = tf.matmul(hidden2, weights) + biases\n",
    "    \n",
    "#     return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    n_hidden_layers = 3\n",
    "    # define the layers\n",
    "    layers = [] \n",
    "    for i in range(n_hidden_layers + 1):\n",
    "        layers.append( {'weights':tf.Variable(tf.random_normal([n_nodes[i], n_nodes[i+1]])), \n",
    "                        'biases':tf.Variable(tf.random_normal([n_nodes[i+1]]))} )\n",
    "    \n",
    "    # calculate the nodal values for each layer\n",
    "    calcs = [data]\n",
    "    for i in range(n_hidden_layers):\n",
    "        calcs.append( tf.nn.relu(tf.matmul(calcs[i], layers[i]['weights']) + layers[i]['biases']) )\n",
    "\n",
    "    #  return the last layer of nodes\n",
    "    return tf.matmul(calcs[-1], layers[-1]['weights']) + layers[-1]['biases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def cal_loss(logits, labels):\n",
    "#     #labels = tf.to_int64(labels)\n",
    "#     cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#         logits, labels, name='xentropy')\n",
    "#     loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "  \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def training(loss, learning_rate):\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#     global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "#     train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "  \n",
    "#     return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "  \n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs():\n",
    "    images_placeholder = tf.placeholder(tf.float32, [None,IMAGE_PIXELS])\n",
    "    labels_placeholder = tf.placeholder(tf.float32, [None,NUM_CLASSES])\n",
    "    \n",
    "    return images_placeholder, labels_placeholder\n",
    "\n",
    "\n",
    "def fill_feed_dict(images_feed,labels_feed, images_pl, labels_pl):\n",
    "    feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "    }\n",
    "  \n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def do_eval(sess, eval_correct, images_placeholder, labels_placeholder, data_set):\n",
    "#     # And run one epoch of eval.\n",
    "#     true_count = 0  # Counts the number of correct predictions.\n",
    "#     steps_per_epoch = 47 // FLAGS.batch_size\n",
    "#     num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "#     for step in xrange(steps_per_epoch):\n",
    "#         feed_dict = fill_feed_dict(train_images,train_labels,\n",
    "#                                images_placeholder,\n",
    "#                                labels_placeholder)\n",
    "#         true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "#     precision = true_count / num_examples\n",
    "#     print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "#         (num_examples, true_count, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    \n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    with tf.Graph().as_default():\n",
    "        # Generate placeholders for the images and labels.\n",
    "        images_placeholder, labels_placeholder = placeholder_inputs()\n",
    "        \n",
    "        \n",
    "        #test_images_placeholder, test_labels_placeholder = placeholder_inputs(2)\n",
    "        # Build a Graph that computes predictions from the inference model.\n",
    "        \n",
    "#         logits = inference(images_placeholder,\n",
    "#                                  FLAGS.hidden1,\n",
    "#                                  FLAGS.hidden2)\n",
    "        \n",
    "        logits = neural_network_model(images_placeholder)\n",
    "        \n",
    "        \n",
    "        cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits,labels_placeholder) )\n",
    "        training_acc = []\n",
    "        testing_acc = []\n",
    "       \n",
    "        #print cost\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "\n",
    "#             for epoch in range(n_epochs):\n",
    "#                 epoch_loss = 0\n",
    "#                 for _ in range( int(train_images.shape[0] / FLAGS.batch_size) ):\n",
    "#                     feed_dict = fill_feed_dict(train_images,train_labels,\n",
    "#                                            images_placeholder,\n",
    "#                                            labels_placeholder)\n",
    "                    \n",
    "#                     #epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "#                     _, c = sess.run([optimizer, cost], feed_dict = feed_dict)\n",
    "#                     epoch_loss += c\n",
    "#                 print 'Epoch', epoch, 'completed out of', n_epochs, 'loss:', epoch_loss\n",
    "\n",
    "#             correct = tf.equal(tf.argmax(logits,1), tf.argmax(labels_placeholder,1))\n",
    "#             #print correct\n",
    "#             accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "#             print 'Accuracy:', accuracy.eval({images_placeholder: test_images, labels_placeholder: test_labels})\n",
    "    \n",
    "#         # Add to the Graph the Ops for loss calculation.\n",
    "#         loss = cal_loss(logits, labels_placeholder)\n",
    "\n",
    "#         # Add to the Graph the Ops that calculate and apply gradients.\n",
    "#         train_op = training(loss, FLAGS.learning_rate)\n",
    "\n",
    "#         # Add the Op to compare the logits to the labels during evaluation.\n",
    "#         eval_correct = evaluation(logits, labels_placeholder)\n",
    "\n",
    "#         # Create a saver for writing training checkpoints.\n",
    "#         saver = tf.train.Saver()\n",
    "\n",
    "#         # Create a session for running Ops on the Graph.\n",
    "#         sess = tf.Session()\n",
    "\n",
    "#         # Run the Op to initialize the variables.\n",
    "#         init = tf.initialize_all_variables()\n",
    "#         sess.run(init)\n",
    "        \n",
    "        # And then after everything is built, start the training loop.\n",
    "            \n",
    "            subset_size = 256\n",
    "            for step in xrange(1000):\n",
    "                start_time = time.time()\n",
    "                total_loss = 0\n",
    "                for i in range(int(train_images.shape[0] / subset_size) ):\n",
    "                    \n",
    "                    epoch_x = train_images[i * subset_size:][:subset_size]\n",
    "                    epoch_y = train_labels[i * subset_size:][:subset_size]\n",
    "                    \n",
    "                    feed_dict = fill_feed_dict(epoch_x, epoch_y, images_placeholder, labels_placeholder)\n",
    "                    \n",
    "                    _, loss_value = sess.run([optimizer, cost],\n",
    "                                           feed_dict=feed_dict)\n",
    "                    \n",
    "                    total_loss+=loss_value\n",
    "                    \n",
    "                duration = time.time() - start_time\n",
    "                #if step % 10 == 0:\n",
    "                    #Print status to stdout.\n",
    "                correct = tf.equal(tf.argmax(logits,1), tf.argmax(labels_placeholder,1))\n",
    "                #print correct\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "                \n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, total_loss, duration)),\n",
    "                \n",
    "                current_train_acc = accuracy.eval({images_placeholder: train_images, labels_placeholder: train_labels})\n",
    "                current_test_acc = accuracy.eval({images_placeholder: test_images, labels_placeholder: test_labels})\n",
    "                \n",
    "                training_acc.append(current_train_acc)\n",
    "                testing_acc.append(current_test_acc)\n",
    "                \n",
    "                \n",
    "                print('Training Accuracy:', current_train_acc),\n",
    "                print('Testing Accuracy:', current_test_acc)\n",
    "    \n",
    "    return training_acc, testing_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the sets of images and labels for training, validation, and\n",
    "\n",
    "images = np.array(images)\n",
    "images = images.reshape(images.shape[0],IMAGE_PIXELS)\n",
    "\n",
    "#label = res_gender\n",
    "labels = dense_to_one_hot(res_gender,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images = images[:-300]\n",
    "train_labels = labels[:-300]\n",
    "test_images = images[-300:]\n",
    "test_labels = labels[-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2808, 10000)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2808, 2)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 149615210.00 (7.744 sec) ('Training Accuracy:', 0.63765335) ('Testing Accuracy:', 0.63999999)\n",
      "Step 1: loss = 102048881.50 (7.492 sec) ('Training Accuracy:', 0.6917178) ('Testing Accuracy:', 0.66000003)\n",
      "Step 2: loss = 79181780.00 (7.512 sec) ('Training Accuracy:', 0.70897239) ('Testing Accuracy:', 0.69)\n",
      "Step 3: loss = 64834446.00 (8.049 sec) ('Training Accuracy:', 0.71434051) ('Testing Accuracy:', 0.71333331)\n",
      "Step 4: loss = 55655517.25 (8.163 sec) ('Training Accuracy:', 0.73082823) ('Testing Accuracy:', 0.72666669)\n",
      "Step 5: loss = 48624089.50 (8.768 sec) ('Training Accuracy:', 0.74194783) ('Testing Accuracy:', 0.72333336)\n",
      "Step 6: loss = 42754566.75 (9.470 sec) ('Training Accuracy:', 0.74846625) ('Testing Accuracy:', 0.71666664)\n",
      "Step 7: loss = 37611691.00 (9.248 sec) ('Training Accuracy:', 0.75115031) ('Testing Accuracy:', 0.71666664)\n",
      "Step 8: loss = 34229123.25 (8.506 sec) ('Training Accuracy:', 0.75843561) ('Testing Accuracy:', 0.70333332)\n",
      "Step 9: loss = 30942868.00 (8.558 sec) ('Training Accuracy:', 0.7684049) ('Testing Accuracy:', 0.71666664)\n",
      "Step 10: loss = 27866782.12 (8.666 sec) ('Training Accuracy:', 0.76533741) ('Testing Accuracy:', 0.70666665)\n",
      "Step 11: loss = 25397823.88 (8.522 sec) ('Training Accuracy:', 0.77952456) ('Testing Accuracy:', 0.71333331)\n",
      "Step 12: loss = 23362850.62 (17.086 sec) ('Training Accuracy:', 0.78911042) ('Testing Accuracy:', 0.71666664)\n",
      "Step 13: loss = 21468629.12 (16.450 sec) ('Training Accuracy:', 0.80789876) ('Testing Accuracy:', 0.73000002)\n",
      "Step 14: loss = 20261081.25 (16.036 sec) ('Training Accuracy:', 0.78565949) ('Testing Accuracy:', 0.71666664)\n",
      "Step 15: loss = 18485799.25 (16.025 sec) ('Training Accuracy:', 0.78834355) ('Testing Accuracy:', 0.69999999)\n",
      "Step 16: loss = 17213558.00 (15.423 sec) ('Training Accuracy:', 0.81096625) ('Testing Accuracy:', 0.69999999)\n",
      "Step 17: loss = 16864923.25 (9.268 sec) ('Training Accuracy:', 0.82438648) ('Testing Accuracy:', 0.71333331)\n",
      "Step 18: loss = 14757082.25 (7.974 sec) ('Training Accuracy:', 0.82208592) ('Testing Accuracy:', 0.70999998)\n",
      "Step 19: loss = 14045743.50 (7.978 sec) ('Training Accuracy:', 0.82975459) ('Testing Accuracy:', 0.73333335)\n",
      "Step 20: loss = 13491564.56 (7.975 sec) ('Training Accuracy:', 0.78565949) ('Testing Accuracy:', 0.69)\n",
      "Step 21: loss = 13891957.75 (7.939 sec) ('Training Accuracy:', 0.84892637) ('Testing Accuracy:', 0.70999998)\n",
      "Step 22: loss = 14932830.50 (7.964 sec) ('Training Accuracy:', 0.85927916) ('Testing Accuracy:', 0.72333336)\n",
      "Step 23: loss = 11253507.25 (8.127 sec) ('Training Accuracy:', 0.80636501) ('Testing Accuracy:', 0.69)\n",
      "Step 24: loss = 10564097.62 (7.962 sec) ('Training Accuracy:', 0.84317487) ('Testing Accuracy:', 0.70333332)\n",
      "Step 25: loss = 10034640.12 (8.110 sec) ('Training Accuracy:', 0.85966259) ('Testing Accuracy:', 0.70666665)\n",
      "Step 26: loss = 11100590.25 (8.144 sec) ('Training Accuracy:', 0.86733127) ('Testing Accuracy:', 0.74000001)\n",
      "Step 27: loss = 9812365.09 (7.957 sec) ('Training Accuracy:', 0.79792947) ('Testing Accuracy:', 0.69666666)\n",
      "Step 28: loss = 9227550.12 (7.943 sec) ('Training Accuracy:', 0.85582823) ('Testing Accuracy:', 0.70666665)\n",
      "Step 29: loss = 8439853.88 (7.933 sec) ('Training Accuracy:', 0.86119634) ('Testing Accuracy:', 0.70999998)\n",
      "Step 30: loss = 7307391.31 (7.909 sec) ('Training Accuracy:', 0.83397239) ('Testing Accuracy:', 0.69666666)\n",
      "Step 31: loss = 6678611.81 (7.908 sec) ('Training Accuracy:', 0.84969324) ('Testing Accuracy:', 0.69)\n",
      "Step 32: loss = 6510093.44 (7.918 sec) ('Training Accuracy:', 0.87154907) ('Testing Accuracy:', 0.69666666)\n",
      "Step 33: loss = 7545212.44 (7.898 sec) ('Training Accuracy:', 0.88381904) ('Testing Accuracy:', 0.72000003)\n",
      "Step 34: loss = 8769619.69 (7.918 sec) ('Training Accuracy:', 0.79217792) ('Testing Accuracy:', 0.69666666)\n",
      "Step 35: loss = 8367435.59 (8.626 sec) ('Training Accuracy:', 0.88573617) ('Testing Accuracy:', 0.69999999)\n",
      "Step 36: loss = 9017080.25 (14.527 sec) ('Training Accuracy:', 0.82707053) ('Testing Accuracy:', 0.69666666)\n",
      "Step 37: loss = 6581539.78 (17.275 sec) ('Training Accuracy:', 0.88957053) ('Testing Accuracy:', 0.68333334)\n",
      "Step 38: loss = 7816914.75 (15.534 sec) ('Training Accuracy:', 0.83205521) ('Testing Accuracy:', 0.70333332)\n",
      "Step 39: loss = 6033557.88 (15.460 sec) ('Training Accuracy:', 0.8861196) ('Testing Accuracy:', 0.68333334)\n",
      "Step 40: loss = 7718957.72 (16.256 sec) ('Training Accuracy:', 0.81595093) ('Testing Accuracy:', 0.69333333)\n",
      "Step 41: loss = 6790084.81 (16.757 sec) ('Training Accuracy:', 0.87883437) ('Testing Accuracy:', 0.70666665)\n",
      "Step 42: loss = 9513518.69 (8.455 sec) ('Training Accuracy:', 0.81173313) ('Testing Accuracy:', 0.68666667)\n",
      "Step 43: loss = 6670420.62 (10.494 sec) ('Training Accuracy:', 0.90874231) ('Testing Accuracy:', 0.71666664)\n",
      "Step 44: loss = 5730735.05 (19.206 sec) ('Training Accuracy:', 0.84394169) ('Testing Accuracy:', 0.70333332)\n",
      "Step 45: loss = 5155215.75 (18.409 sec) ('Training Accuracy:', 0.88650304) ('Testing Accuracy:', 0.70999998)\n",
      "Step 46: loss = 4170738.30 (17.994 sec) ('Training Accuracy:', 0.8822853) ('Testing Accuracy:', 0.71333331)\n",
      "Step 47: loss = 4250098.94 (16.292 sec) ('Training Accuracy:', 0.87806749) ('Testing Accuracy:', 0.69666666)\n",
      "Step 48: loss = 3956037.19 (16.481 sec) ('Training Accuracy:', 0.87653375) ('Testing Accuracy:', 0.69333333)\n",
      "Step 49: loss = 4058755.28 (9.079 sec) ('Training Accuracy:', 0.89033741) ('Testing Accuracy:', 0.69999999)\n",
      "Step 50: loss = 4466724.00 (8.874 sec) ('Training Accuracy:', 0.87116563) ('Testing Accuracy:', 0.69999999)\n",
      "Step 51: loss = 4182312.42 (7.946 sec) ('Training Accuracy:', 0.86119634) ('Testing Accuracy:', 0.69666666)\n",
      "Step 52: loss = 3807351.59 (9.946 sec) ('Training Accuracy:', 0.89225459) ('Testing Accuracy:', 0.69333333)\n",
      "Step 53: loss = 4269515.16 (9.652 sec) ('Training Accuracy:', 0.89263803) ('Testing Accuracy:', 0.69999999)\n",
      "Step 54: loss = 4971688.36 (9.592 sec) ('Training Accuracy:', 0.82285279) ('Testing Accuracy:', 0.69)\n",
      "Step 55: loss = 4499614.20 (9.664 sec) ('Training Accuracy:', 0.91296011) ('Testing Accuracy:', 0.70666665)\n",
      "Step 56: loss = 7741993.33 (9.910 sec) ('Training Accuracy:', 0.8309049) ('Testing Accuracy:', 0.68666667)\n",
      "Step 57: loss = 4330149.56 (9.526 sec) ('Training Accuracy:', 0.90337425) ('Testing Accuracy:', 0.70666665)\n",
      "Step 58: loss = 7682196.53 (11.158 sec) ('Training Accuracy:', 0.85276073) ('Testing Accuracy:', 0.69333333)\n",
      "Step 59: loss = 6030529.44 (8.986 sec) ('Training Accuracy:', 0.89838958) ('Testing Accuracy:', 0.71333331)\n",
      "Step 60: loss = 7611002.31 (9.181 sec) ('Training Accuracy:', 0.8792178) ('Testing Accuracy:', 0.70999998)\n",
      "Step 61: loss = 8253829.61 (8.681 sec) ('Training Accuracy:', 0.8861196) ('Testing Accuracy:', 0.69999999)\n",
      "Step 62: loss = 9299121.09 (15.294 sec) ('Training Accuracy:', 0.89915645) ('Testing Accuracy:', 0.73666668)\n",
      "Step 63: loss = 11839501.66 (15.431 sec) ('Training Accuracy:', 0.90260738) ('Testing Accuracy:', 0.74333334)\n",
      "Step 64: loss = 16603555.09 (15.812 sec) ('Training Accuracy:', 0.90260738) ('Testing Accuracy:', 0.74333334)\n",
      "Step 65: loss = 25058475.47 (15.615 sec) ('Training Accuracy:', 0.79601228) ('Testing Accuracy:', 0.72333336)\n",
      "Step 66: loss = 29820206.03 (16.137 sec) ('Training Accuracy:', 0.71894169) ('Testing Accuracy:', 0.68666667)\n",
      "Step 67: loss = 31755467.38 (15.711 sec) ('Training Accuracy:', 0.82438648) ('Testing Accuracy:', 0.72000003)\n",
      "Step 68: loss = 24663052.25 (8.784 sec) ('Training Accuracy:', 0.90989262) ('Testing Accuracy:', 0.75333333)\n",
      "Step 69: loss = 14153195.06 (8.031 sec) ('Training Accuracy:', 0.91717792) ('Testing Accuracy:', 0.73333335)\n",
      "Step 70: loss = 8037711.97 (8.116 sec) ('Training Accuracy:', 0.92254603) ('Testing Accuracy:', 0.73333335)\n",
      "Step 71: loss = 4603804.95 (8.033 sec) ('Training Accuracy:', 0.93289876) ('Testing Accuracy:', 0.73000002)\n",
      "Step 72: loss = 3133156.06 (8.268 sec) ('Training Accuracy:', 0.92484665) ('Testing Accuracy:', 0.73000002)\n",
      "Step 73: loss = 3980990.45 (8.020 sec) ('Training Accuracy:', 0.88650304) ('Testing Accuracy:', 0.73333335)\n",
      "Step 74: loss = 5771108.75 (8.013 sec) ('Training Accuracy:', 0.8523773) ('Testing Accuracy:', 0.72666669)\n",
      "Step 75: loss = 6822436.42 (8.037 sec) ('Training Accuracy:', 0.84470856) ('Testing Accuracy:', 0.73000002)\n",
      "Step 76: loss = 6981655.91 (8.028 sec) ('Training Accuracy:', 0.85467792) ('Testing Accuracy:', 0.72666669)\n",
      "Step 77: loss = 7306953.22 (8.031 sec) ('Training Accuracy:', 0.88381904) ('Testing Accuracy:', 0.73333335)\n",
      "Step 78: loss = 8112533.62 (8.019 sec) ('Training Accuracy:', 0.91794479) ('Testing Accuracy:', 0.75666666)\n",
      "Step 79: loss = 8996614.50 (8.091 sec) ('Training Accuracy:', 0.94670248) ('Testing Accuracy:', 0.73666668)\n",
      "Step 80: loss = 9254870.53 (8.027 sec) ('Training Accuracy:', 0.92829752) ('Testing Accuracy:', 0.73333335)\n",
      "Step 81: loss = 8321652.48 (8.035 sec) ('Training Accuracy:', 0.8638804) ('Testing Accuracy:', 0.70999998)\n",
      "Step 82: loss = 5825987.52 (8.057 sec) ('Training Accuracy:', 0.92101228) ('Testing Accuracy:', 0.72333336)\n",
      "Step 83: loss = 2765685.79 (8.170 sec) ('Training Accuracy:', 0.95053679) ('Testing Accuracy:', 0.72333336)\n",
      "Step 84: loss = 2584070.40 (7.986 sec) ('Training Accuracy:', 0.9375) ('Testing Accuracy:', 0.73666668)\n",
      "Step 85: loss = 3409881.30 (8.038 sec) ('Training Accuracy:', 0.89685583) ('Testing Accuracy:', 0.74333334)\n",
      "Step 86: loss = 3617102.52 (8.083 sec) ('Training Accuracy:', 0.90720856) ('Testing Accuracy:', 0.74333334)\n",
      "Step 87: loss = 3219117.08 (8.012 sec) ('Training Accuracy:', 0.94708592) ('Testing Accuracy:', 0.72666669)\n",
      "Step 88: loss = 2944459.55 (8.050 sec) ('Training Accuracy:', 0.9601227) ('Testing Accuracy:', 0.72333336)\n",
      "Step 89: loss = 2615126.61 (8.041 sec) ('Training Accuracy:', 0.95398772) ('Testing Accuracy:', 0.71666664)\n",
      "Step 90: loss = 1976369.91 (8.235 sec) ('Training Accuracy:', 0.9559049) ('Testing Accuracy:', 0.71666664)\n",
      "Step 91: loss = 1013710.87 (8.016 sec) ('Training Accuracy:', 0.96280676) ('Testing Accuracy:', 0.70999998)\n",
      "Step 92: loss = 888990.84 (8.050 sec) ('Training Accuracy:', 0.96434051) ('Testing Accuracy:', 0.71666664)\n",
      "Step 93: loss = 1191153.13 (8.052 sec) ('Training Accuracy:', 0.95667177) ('Testing Accuracy:', 0.71666664)\n",
      "Step 94: loss = 1601987.91 (8.133 sec) ('Training Accuracy:', 0.92944783) ('Testing Accuracy:', 0.73000002)\n",
      "Step 95: loss = 1653693.72 (8.046 sec) ('Training Accuracy:', 0.9417178) ('Testing Accuracy:', 0.73000002)\n",
      "Step 96: loss = 1674820.35 (8.094 sec) ('Training Accuracy:', 0.95398772) ('Testing Accuracy:', 0.72333336)\n",
      "Step 97: loss = 1602686.73 (8.036 sec) ('Training Accuracy:', 0.96855831) ('Testing Accuracy:', 0.72000003)\n",
      "Step 98: loss = 1880488.11 (8.066 sec) ('Training Accuracy:', 0.97200918) ('Testing Accuracy:', 0.71666664)\n",
      "Step 99: loss = 1624145.80 (8.044 sec) ('Training Accuracy:', 0.96472394) ('Testing Accuracy:', 0.70999998)\n",
      "Step 100: loss = 1922308.96 (8.149 sec) ('Training Accuracy:', 0.95667177) ('Testing Accuracy:', 0.71333331)\n",
      "Step 101: loss = 1466087.58 (8.036 sec) ('Training Accuracy:', 0.94670248) ('Testing Accuracy:', 0.71333331)\n",
      "Step 102: loss = 1209408.93 (8.068 sec) ('Training Accuracy:', 0.95092022) ('Testing Accuracy:', 0.70666665)\n",
      "Step 103: loss = 1084458.23 (8.037 sec) ('Training Accuracy:', 0.96855831) ('Testing Accuracy:', 0.70999998)\n",
      "Step 104: loss = 1634679.52 (8.010 sec) ('Training Accuracy:', 0.95858896) ('Testing Accuracy:', 0.73333335)\n",
      "Step 105: loss = 2285668.73 (8.228 sec) ('Training Accuracy:', 0.90912575) ('Testing Accuracy:', 0.73666668)\n",
      "Step 106: loss = 3445930.90 (8.056 sec) ('Training Accuracy:', 0.91104293) ('Testing Accuracy:', 0.73000002)\n",
      "Step 107: loss = 3656409.98 (8.105 sec) ('Training Accuracy:', 0.85506135) ('Testing Accuracy:', 0.72000003)\n",
      "Step 108: loss = 4918563.38 (8.063 sec) ('Training Accuracy:', 0.83588958) ('Testing Accuracy:', 0.72000003)\n",
      "Step 109: loss = 5692589.54 (8.075 sec) ('Training Accuracy:', 0.80406439) ('Testing Accuracy:', 0.73000002)\n",
      "Step 110: loss = 6295619.16 (8.035 sec) ('Training Accuracy:', 0.80789876) ('Testing Accuracy:', 0.73333335)\n",
      "Step 111: loss = 6670660.20 (8.030 sec) ('Training Accuracy:', 0.82860428) ('Testing Accuracy:', 0.72000003)\n",
      "Step 112: loss = 8123977.20 (8.075 sec) ('Training Accuracy:', 0.875) ('Testing Accuracy:', 0.72666669)\n",
      "Step 113: loss = 11653653.11 (8.090 sec) ('Training Accuracy:', 0.94401842) ('Testing Accuracy:', 0.73666668)\n",
      "Step 114: loss = 12076433.08 (8.048 sec) ('Training Accuracy:', 0.92024541) ('Testing Accuracy:', 0.73000002)\n",
      "Step 115: loss = 10697971.96 (8.085 sec) ('Training Accuracy:', 0.82745397) ('Testing Accuracy:', 0.68333334)\n",
      "Step 116: loss = 7741758.38 (8.267 sec) ('Training Accuracy:', 0.84854293) ('Testing Accuracy:', 0.69666666)\n",
      "Step 117: loss = 4842078.47 (8.701 sec) ('Training Accuracy:', 0.95513803) ('Testing Accuracy:', 0.73333335)\n",
      "Step 118: loss = 3164124.16 (8.873 sec) ('Training Accuracy:', 0.94325155) ('Testing Accuracy:', 0.74333334)\n",
      "Step 119: loss = 2704391.60 (8.292 sec) ('Training Accuracy:', 0.92599696) ('Testing Accuracy:', 0.73000002)\n",
      "Step 120: loss = 2329417.89 (8.272 sec) ('Training Accuracy:', 0.970092) ('Testing Accuracy:', 0.72666669)\n",
      "Step 121: loss = 1085794.76 (8.516 sec) ('Training Accuracy:', 0.97277606) ('Testing Accuracy:', 0.72333336)\n",
      "Step 122: loss = 532621.10 (13.773 sec) ('Training Accuracy:', 0.97200918) ('Testing Accuracy:', 0.72666669)\n",
      "Step 123: loss = 437978.82 (15.760 sec) ('Training Accuracy:', 0.97699386) ('Testing Accuracy:', 0.73000002)\n",
      "Step 124: loss = 574974.62 (15.780 sec) ('Training Accuracy:', 0.97622699) ('Testing Accuracy:', 0.72000003)\n",
      "Step 125: loss = 674222.87 (15.641 sec) ('Training Accuracy:', 0.96855831) ('Testing Accuracy:', 0.70999998)\n",
      "Step 126: loss = 578714.20 (16.043 sec) ('Training Accuracy:', 0.97162575) ('Testing Accuracy:', 0.70999998)\n",
      "Step 127: loss = 464105.74 (15.275 sec) ('Training Accuracy:', 0.97852761) ('Testing Accuracy:', 0.71666664)\n",
      "Step 128: loss = 271571.95 (15.199 sec) ('Training Accuracy:', 0.98044479) ('Testing Accuracy:', 0.73000002)\n",
      "Step 129: loss = 261389.13 (15.825 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.72666669)\n",
      "Step 130: loss = 308253.23 (15.963 sec) ('Training Accuracy:', 0.98082823) ('Testing Accuracy:', 0.72000003)\n",
      "Step 131: loss = 383750.03 (15.717 sec) ('Training Accuracy:', 0.97546011) ('Testing Accuracy:', 0.72666669)\n",
      "Step 132: loss = 354734.89 (16.173 sec) ('Training Accuracy:', 0.97200918) ('Testing Accuracy:', 0.72333336)\n",
      "Step 133: loss = 255021.83 (17.763 sec) ('Training Accuracy:', 0.98044479) ('Testing Accuracy:', 0.73666668)\n",
      "Step 134: loss = 203113.57 (17.551 sec) ('Training Accuracy:', 0.98082823) ('Testing Accuracy:', 0.73000002)\n",
      "Step 135: loss = 179042.29 (20.175 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.72333336)\n",
      "Step 136: loss = 200369.73 (15.762 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.72666669)\n",
      "Step 137: loss = 281991.21 (16.088 sec) ('Training Accuracy:', 0.98121166) ('Testing Accuracy:', 0.72666669)\n",
      "Step 138: loss = 409861.41 (19.586 sec) ('Training Accuracy:', 0.97699386) ('Testing Accuracy:', 0.72666669)\n",
      "Step 139: loss = 440636.33 (18.218 sec) ('Training Accuracy:', 0.96549082) ('Testing Accuracy:', 0.72000003)\n",
      "Step 140: loss = 487041.93 (16.814 sec) ('Training Accuracy:', 0.96203989) ('Testing Accuracy:', 0.71666664)\n",
      "Step 141: loss = 615213.52 (19.218 sec) ('Training Accuracy:', 0.95858896) ('Testing Accuracy:', 0.72333336)\n",
      "Step 142: loss = 1037737.41 (18.156 sec) ('Training Accuracy:', 0.96970856) ('Testing Accuracy:', 0.71333331)\n",
      "Step 143: loss = 1718864.02 (18.748 sec) ('Training Accuracy:', 0.97392637) ('Testing Accuracy:', 0.70333332)\n",
      "Step 144: loss = 1818136.44 (18.261 sec) ('Training Accuracy:', 0.95245397) ('Testing Accuracy:', 0.73000002)\n",
      "Step 145: loss = 1632458.92 (16.185 sec) ('Training Accuracy:', 0.93213189) ('Testing Accuracy:', 0.72333336)\n",
      "Step 146: loss = 1306893.34 (16.399 sec) ('Training Accuracy:', 0.967408) ('Testing Accuracy:', 0.71666664)\n",
      "Step 147: loss = 813889.51 (18.645 sec) ('Training Accuracy:', 0.97967792) ('Testing Accuracy:', 0.72666669)\n",
      "Step 148: loss = 1078290.67 (17.647 sec) ('Training Accuracy:', 0.97047544) ('Testing Accuracy:', 0.72333336)\n",
      "Step 149: loss = 614544.16 (15.639 sec) ('Training Accuracy:', 0.97852761) ('Testing Accuracy:', 0.72000003)\n",
      "Step 150: loss = 384337.10 (15.559 sec) ('Training Accuracy:', 0.97929448) ('Testing Accuracy:', 0.71666664)\n",
      "Step 151: loss = 318319.93 (15.555 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.71666664)\n",
      "Step 152: loss = 282212.17 (15.546 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.72000003)\n",
      "Step 153: loss = 238471.82 (15.526 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.71666664)\n",
      "Step 154: loss = 149070.35 (17.735 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.71333331)\n",
      "Step 155: loss = 182569.42 (18.905 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.71333331)\n",
      "Step 156: loss = 323056.71 (19.680 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.72333336)\n",
      "Step 157: loss = 478827.39 (23.342 sec) ('Training Accuracy:', 0.9773773) ('Testing Accuracy:', 0.71666664)\n",
      "Step 158: loss = 303100.70 (21.259 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.72333336)\n",
      "Step 159: loss = 236348.99 (20.606 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.72333336)\n",
      "Step 160: loss = 203510.53 (22.182 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.72000003)\n",
      "Step 161: loss = 238092.69 (21.511 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.72333336)\n",
      "Step 162: loss = 300521.34 (23.855 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.72000003)\n",
      "Step 163: loss = 294144.86 (22.436 sec) ('Training Accuracy:', 0.98082823) ('Testing Accuracy:', 0.71333331)\n",
      "Step 164: loss = 202809.41 (22.487 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.72333336)\n",
      "Step 165: loss = 193694.78 (21.543 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.72666669)\n",
      "Step 166: loss = 221362.49 (22.349 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.72666669)\n",
      "Step 167: loss = 217531.31 (21.161 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.72666669)\n",
      "Step 168: loss = 265276.88 (21.268 sec) ('Training Accuracy:', 0.9815951) ('Testing Accuracy:', 0.72333336)\n",
      "Step 169: loss = 283692.27 (21.846 sec) ('Training Accuracy:', 0.97891104) ('Testing Accuracy:', 0.72000003)\n",
      "Step 170: loss = 316708.44 (22.127 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.73000002)\n",
      "Step 171: loss = 249792.36 (20.897 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.72000003)\n",
      "Step 172: loss = 294613.45 (21.646 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.72000003)\n",
      "Step 173: loss = 350924.01 (21.921 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.72333336)\n",
      "Step 174: loss = 411357.89 (21.332 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.70999998)\n",
      "Step 175: loss = 291353.32 (20.886 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.70666665)\n",
      "Step 176: loss = 191426.43 (21.039 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.71666664)\n",
      "Step 177: loss = 173544.73 (21.223 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.72000003)\n",
      "Step 178: loss = 227530.18 (21.044 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.72333336)\n",
      "Step 179: loss = 343796.78 (20.832 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.72000003)\n",
      "Step 180: loss = 532897.36 (21.568 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.71333331)\n",
      "Step 181: loss = 463710.93 (21.381 sec) ('Training Accuracy:', 0.98121166) ('Testing Accuracy:', 0.72333336)\n",
      "Step 182: loss = 469961.24 (16.444 sec) ('Training Accuracy:', 0.970092) ('Testing Accuracy:', 0.71666664)\n",
      "Step 183: loss = 449558.75 (18.196 sec) ('Training Accuracy:', 0.97277606) ('Testing Accuracy:', 0.71333331)\n",
      "Step 184: loss = 410182.77 (22.455 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.71333331)\n",
      "Step 185: loss = 408793.88 (16.675 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.72000003)\n",
      "Step 186: loss = 507499.40 (15.639 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.72000003)\n",
      "Step 187: loss = 436412.98 (15.509 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.70333332)\n",
      "Step 188: loss = 247426.74 (16.218 sec) ('Training Accuracy:', 0.98274541) ('Testing Accuracy:', 0.70999998)\n",
      "Step 189: loss = 253194.34 (16.016 sec) ('Training Accuracy:', 0.98006135) ('Testing Accuracy:', 0.71333331)\n",
      "Step 190: loss = 349329.97 (15.983 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.73000002)\n",
      "Step 191: loss = 444206.82 (16.252 sec) ('Training Accuracy:', 0.97814417) ('Testing Accuracy:', 0.71666664)\n",
      "Step 192: loss = 338868.52 (15.768 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.72666669)\n",
      "Step 193: loss = 223039.01 (16.262 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.73000002)\n",
      "Step 194: loss = 236511.79 (16.030 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.73000002)\n",
      "Step 195: loss = 247023.21 (15.901 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.73000002)\n",
      "Step 196: loss = 223159.23 (16.320 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.72666669)\n",
      "Step 197: loss = 235469.21 (16.892 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.72666669)\n",
      "Step 198: loss = 240130.73 (16.633 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.72333336)\n",
      "Step 199: loss = 304689.22 (16.512 sec) ('Training Accuracy:', 0.97699386) ('Testing Accuracy:', 0.72666669)\n",
      "Step 200: loss = 265575.24 (16.445 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.72666669)\n",
      "Step 201: loss = 228171.33 (16.412 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.73666668)\n",
      "Step 202: loss = 203834.09 (16.522 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.73333335)\n",
      "Step 203: loss = 189629.31 (16.294 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.73666668)\n",
      "Step 204: loss = 228641.09 (16.188 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.73333335)\n",
      "Step 205: loss = 205892.55 (15.960 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.73666668)\n",
      "Step 206: loss = 206269.98 (16.226 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.72666669)\n",
      "Step 207: loss = 217416.35 (16.363 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.73666668)\n",
      "Step 208: loss = 264040.40 (16.617 sec) ('Training Accuracy:', 0.98274541) ('Testing Accuracy:', 0.73000002)\n",
      "Step 209: loss = 271352.98 (16.469 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.73000002)\n",
      "Step 210: loss = 230610.75 (16.320 sec) ('Training Accuracy:', 0.9815951) ('Testing Accuracy:', 0.73000002)\n",
      "Step 211: loss = 302662.85 (16.518 sec) ('Training Accuracy:', 0.97929448) ('Testing Accuracy:', 0.73000002)\n",
      "Step 212: loss = 340613.30 (16.024 sec) ('Training Accuracy:', 0.9631902) ('Testing Accuracy:', 0.71666664)\n",
      "Step 213: loss = 377170.84 (16.041 sec) ('Training Accuracy:', 0.96779144) ('Testing Accuracy:', 0.72000003)\n",
      "Step 214: loss = 356632.88 (15.931 sec) ('Training Accuracy:', 0.97929448) ('Testing Accuracy:', 0.72666669)\n",
      "Step 215: loss = 402767.17 (16.218 sec) ('Training Accuracy:', 0.97661042) ('Testing Accuracy:', 0.72666669)\n",
      "Step 216: loss = 496494.61 (16.391 sec) ('Training Accuracy:', 0.97929448) ('Testing Accuracy:', 0.72000003)\n",
      "Step 217: loss = 582484.95 (16.072 sec) ('Training Accuracy:', 0.97507668) ('Testing Accuracy:', 0.72333336)\n",
      "Step 218: loss = 523393.53 (16.254 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.70999998)\n",
      "Step 219: loss = 514596.55 (16.635 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.72666669)\n",
      "Step 220: loss = 604733.02 (16.292 sec) ('Training Accuracy:', 0.97929448) ('Testing Accuracy:', 0.71333331)\n",
      "Step 221: loss = 518846.11 (17.106 sec) ('Training Accuracy:', 0.97967792) ('Testing Accuracy:', 0.71333331)\n",
      "Step 222: loss = 284472.21 (16.284 sec) ('Training Accuracy:', 0.98082823) ('Testing Accuracy:', 0.71333331)\n",
      "Step 223: loss = 236366.21 (16.553 sec) ('Training Accuracy:', 0.96050614) ('Testing Accuracy:', 0.72000003)\n",
      "Step 224: loss = 422506.00 (16.711 sec) ('Training Accuracy:', 0.95245397) ('Testing Accuracy:', 0.71333331)\n",
      "Step 225: loss = 514909.95 (16.306 sec) ('Training Accuracy:', 0.96510738) ('Testing Accuracy:', 0.72333336)\n",
      "Step 226: loss = 673787.08 (16.294 sec) ('Training Accuracy:', 0.9302147) ('Testing Accuracy:', 0.72333336)\n",
      "Step 227: loss = 380421.52 (16.568 sec) ('Training Accuracy:', 0.9773773) ('Testing Accuracy:', 0.72000003)\n",
      "Step 228: loss = 768006.61 (15.965 sec) ('Training Accuracy:', 0.96664113) ('Testing Accuracy:', 0.72333336)\n",
      "Step 229: loss = 936199.87 (18.575 sec) ('Training Accuracy:', 0.970092) ('Testing Accuracy:', 0.70999998)\n",
      "Step 230: loss = 1766876.97 (20.578 sec) ('Training Accuracy:', 0.88803679) ('Testing Accuracy:', 0.72333336)\n",
      "Step 231: loss = 1164892.73 (18.395 sec) ('Training Accuracy:', 0.97392637) ('Testing Accuracy:', 0.71666664)\n",
      "Step 232: loss = 2456160.03 (21.672 sec) ('Training Accuracy:', 0.97929448) ('Testing Accuracy:', 0.73000002)\n",
      "Step 233: loss = 2748591.46 (15.315 sec) ('Training Accuracy:', 0.95820552) ('Testing Accuracy:', 0.71666664)\n",
      "Step 234: loss = 1965994.60 (15.706 sec) ('Training Accuracy:', 0.95437115) ('Testing Accuracy:', 0.72333336)\n",
      "Step 235: loss = 3786190.15 (15.820 sec) ('Training Accuracy:', 0.97124231) ('Testing Accuracy:', 0.72333336)\n",
      "Step 236: loss = 2723517.49 (16.332 sec) ('Training Accuracy:', 0.94900304) ('Testing Accuracy:', 0.72333336)\n",
      "Step 237: loss = 3908325.78 (17.776 sec) ('Training Accuracy:', 0.97085887) ('Testing Accuracy:', 0.74000001)\n",
      "Step 238: loss = 3730204.05 (18.471 sec) ('Training Accuracy:', 0.96472394) ('Testing Accuracy:', 0.73666668)\n",
      "Step 239: loss = 3469560.20 (19.973 sec) ('Training Accuracy:', 0.95092022) ('Testing Accuracy:', 0.72333336)\n",
      "Step 240: loss = 4796195.07 (21.254 sec) ('Training Accuracy:', 0.95245397) ('Testing Accuracy:', 0.73666668)\n",
      "Step 241: loss = 4751495.75 (16.616 sec) ('Training Accuracy:', 0.94670248) ('Testing Accuracy:', 0.73000002)\n",
      "Step 242: loss = 4958784.94 (16.075 sec) ('Training Accuracy:', 0.93634969) ('Testing Accuracy:', 0.71666664)\n",
      "Step 243: loss = 6549356.99 (15.783 sec) ('Training Accuracy:', 0.93366563) ('Testing Accuracy:', 0.72666669)\n",
      "Step 244: loss = 6685171.11 (16.295 sec) ('Training Accuracy:', 0.89570552) ('Testing Accuracy:', 0.74000001)\n",
      "Step 245: loss = 9048022.16 (15.887 sec) ('Training Accuracy:', 0.92983127) ('Testing Accuracy:', 0.74000001)\n",
      "Step 246: loss = 9816228.44 (16.228 sec) ('Training Accuracy:', 0.82822084) ('Testing Accuracy:', 0.72666669)\n",
      "Step 247: loss = 16838418.77 (18.692 sec) ('Training Accuracy:', 0.82055217) ('Testing Accuracy:', 0.72666669)\n",
      "Step 248: loss = 17711119.59 (17.521 sec) ('Training Accuracy:', 0.75460124) ('Testing Accuracy:', 0.71333331)\n",
      "Step 249: loss = 23618249.45 (18.350 sec) ('Training Accuracy:', 0.70398772) ('Testing Accuracy:', 0.68000001)\n",
      "Step 250: loss = 27693423.44 (15.430 sec) ('Training Accuracy:', 0.83665645) ('Testing Accuracy:', 0.74333334)\n",
      "Step 251: loss = 26590548.62 (16.091 sec) ('Training Accuracy:', 0.9375) ('Testing Accuracy:', 0.75333333)\n",
      "Step 252: loss = 20141987.19 (15.523 sec) ('Training Accuracy:', 0.82937115) ('Testing Accuracy:', 0.68333334)\n",
      "Step 253: loss = 11170600.70 (15.478 sec) ('Training Accuracy:', 0.90567487) ('Testing Accuracy:', 0.73333335)\n",
      "Step 254: loss = 3313257.94 (15.384 sec) ('Training Accuracy:', 0.96127301) ('Testing Accuracy:', 0.72666669)\n",
      "Step 255: loss = 1858224.73 (15.469 sec) ('Training Accuracy:', 0.94210124) ('Testing Accuracy:', 0.75)\n",
      "Step 256: loss = 2158569.36 (15.189 sec) ('Training Accuracy:', 0.96280676) ('Testing Accuracy:', 0.75)\n",
      "Step 257: loss = 2139547.84 (15.105 sec) ('Training Accuracy:', 0.97239262) ('Testing Accuracy:', 0.73666668)\n",
      "Step 258: loss = 1247277.16 (15.797 sec) ('Training Accuracy:', 0.97162575) ('Testing Accuracy:', 0.73000002)\n",
      "Step 259: loss = 390778.94 (15.701 sec) ('Training Accuracy:', 0.97507668) ('Testing Accuracy:', 0.73666668)\n",
      "Step 260: loss = 279300.77 (15.459 sec) ('Training Accuracy:', 0.97699386) ('Testing Accuracy:', 0.73333335)\n",
      "Step 261: loss = 525753.45 (15.663 sec) ('Training Accuracy:', 0.97546011) ('Testing Accuracy:', 0.75)\n",
      "Step 262: loss = 625524.78 (17.092 sec) ('Training Accuracy:', 0.97354293) ('Testing Accuracy:', 0.75)\n",
      "Step 263: loss = 574535.01 (16.291 sec) ('Training Accuracy:', 0.97546011) ('Testing Accuracy:', 0.75666666)\n",
      "Step 264: loss = 810472.28 (15.297 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.72333336)\n",
      "Step 265: loss = 404392.03 (15.932 sec) ('Training Accuracy:', 0.98274541) ('Testing Accuracy:', 0.73000002)\n",
      "Step 266: loss = 180062.51 (15.790 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.73000002)\n",
      "Step 267: loss = 205664.66 (15.309 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.73333335)\n",
      "Step 268: loss = 342747.57 (15.828 sec) ('Training Accuracy:', 0.98044479) ('Testing Accuracy:', 0.75333333)\n",
      "Step 269: loss = 340971.01 (15.104 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.73666668)\n",
      "Step 270: loss = 175730.53 (15.249 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.73666668)\n",
      "Step 271: loss = 130358.75 (15.008 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.73000002)\n",
      "Step 272: loss = 132368.39 (15.918 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.73000002)\n",
      "Step 273: loss = 180781.57 (15.863 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.73000002)\n",
      "Step 274: loss = 286804.48 (15.561 sec) ('Training Accuracy:', 0.9815951) ('Testing Accuracy:', 0.74333334)\n",
      "Step 275: loss = 311186.94 (15.673 sec) ('Training Accuracy:', 0.98274541) ('Testing Accuracy:', 0.73333335)\n",
      "Step 276: loss = 196269.99 (15.363 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.72666669)\n",
      "Step 277: loss = 109347.11 (15.328 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.72333336)\n",
      "Step 278: loss = 150136.02 (15.228 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.73666668)\n",
      "Step 279: loss = 226526.18 (15.772 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.74666667)\n",
      "Step 280: loss = 263055.65 (15.567 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.75)\n",
      "Step 281: loss = 290191.97 (16.963 sec) ('Training Accuracy:', 0.98274541) ('Testing Accuracy:', 0.75333333)\n",
      "Step 282: loss = 261345.57 (15.207 sec) ('Training Accuracy:', 0.97967792) ('Testing Accuracy:', 0.74666667)\n",
      "Step 283: loss = 273124.01 (15.541 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.73666668)\n",
      "Step 284: loss = 262691.47 (15.193 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.73000002)\n",
      "Step 285: loss = 186199.69 (15.893 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.72666669)\n",
      "Step 286: loss = 164363.65 (15.434 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.73333335)\n",
      "Step 287: loss = 129373.76 (15.468 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.74333334)\n",
      "Step 288: loss = 86667.70 (15.178 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.74000001)\n",
      "Step 289: loss = 74607.41 (14.816 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.72666669)\n",
      "Step 290: loss = 90565.92 (15.741 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.72333336)\n",
      "Step 291: loss = 124098.56 (15.648 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.73000002)\n",
      "Step 292: loss = 198372.78 (15.832 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.75)\n",
      "Step 293: loss = 369354.24 (14.988 sec) ('Training Accuracy:', 0.97814417) ('Testing Accuracy:', 0.75)\n",
      "Step 294: loss = 392314.24 (15.480 sec) ('Training Accuracy:', 0.97507668) ('Testing Accuracy:', 0.75333333)\n",
      "Step 295: loss = 201552.74 (15.371 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.74666667)\n",
      "Step 296: loss = 103945.32 (15.582 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.74333334)\n",
      "Step 297: loss = 153401.19 (15.465 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.74666667)\n",
      "Step 298: loss = 229415.32 (15.177 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.75)\n",
      "Step 299: loss = 273998.21 (15.048 sec) ('Training Accuracy:', 0.98044479) ('Testing Accuracy:', 0.75333333)\n",
      "Step 300: loss = 207601.79 (15.389 sec) ('Training Accuracy:', 0.98121166) ('Testing Accuracy:', 0.74666667)\n",
      "Step 301: loss = 116952.37 (15.604 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.73333335)\n",
      "Step 302: loss = 84569.51 (15.653 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.72000003)\n",
      "Step 303: loss = 145566.23 (15.857 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.73333335)\n",
      "Step 304: loss = 161065.35 (15.150 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.73333335)\n",
      "Step 305: loss = 163462.98 (15.339 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.73666668)\n",
      "Step 306: loss = 219533.62 (15.288 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.74666667)\n",
      "Step 307: loss = 272717.97 (15.749 sec) ('Training Accuracy:', 0.97967792) ('Testing Accuracy:', 0.75)\n",
      "Step 308: loss = 291073.59 (15.735 sec) ('Training Accuracy:', 0.98006135) ('Testing Accuracy:', 0.75333333)\n",
      "Step 309: loss = 232995.95 (14.997 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.74666667)\n",
      "Step 310: loss = 130984.49 (15.221 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.74000001)\n",
      "Step 311: loss = 107598.95 (15.698 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.73000002)\n",
      "Step 312: loss = 159487.39 (15.545 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.75666666)\n",
      "Step 313: loss = 242721.36 (15.431 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.75666666)\n",
      "Step 314: loss = 190749.68 (15.394 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.75)\n",
      "Step 315: loss = 182550.77 (15.141 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.75)\n",
      "Step 316: loss = 226533.81 (15.682 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.75)\n",
      "Step 317: loss = 178093.21 (15.789 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.75333333)\n",
      "Step 318: loss = 163404.44 (15.426 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.75)\n",
      "Step 319: loss = 230819.49 (15.536 sec) ('Training Accuracy:', 0.98044479) ('Testing Accuracy:', 0.75)\n",
      "Step 320: loss = 180651.37 (15.546 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.75333333)\n",
      "Step 321: loss = 191238.04 (15.251 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.75)\n",
      "Step 322: loss = 241253.50 (16.567 sec) ('Training Accuracy:', 0.98082823) ('Testing Accuracy:', 0.75)\n",
      "Step 323: loss = 205008.07 (16.046 sec) ('Training Accuracy:', 0.97967792) ('Testing Accuracy:', 0.75)\n",
      "Step 324: loss = 232331.94 (15.979 sec) ('Training Accuracy:', 0.98044479) ('Testing Accuracy:', 0.75333333)\n",
      "Step 325: loss = 176316.54 (16.064 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.75)\n",
      "Step 326: loss = 135169.05 (15.533 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.74666667)\n",
      "Step 327: loss = 169210.08 (15.786 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.75333333)\n",
      "Step 328: loss = 248299.94 (16.386 sec) ('Training Accuracy:', 0.9815951) ('Testing Accuracy:', 0.75666666)\n",
      "Step 329: loss = 275486.28 (16.564 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.75999999)\n",
      "Step 330: loss = 262977.38 (16.710 sec) ('Training Accuracy:', 0.97891104) ('Testing Accuracy:', 0.75999999)\n",
      "Step 331: loss = 223728.98 (17.033 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.75666666)\n",
      "Step 332: loss = 126333.94 (16.335 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.74666667)\n",
      "Step 333: loss = 124583.20 (16.481 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.74666667)\n",
      "Step 334: loss = 133202.68 (16.543 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.74333334)\n",
      "Step 335: loss = 163712.48 (16.982 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.75333333)\n",
      "Step 336: loss = 261827.66 (17.116 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.76333332)\n",
      "Step 337: loss = 270889.66 (17.343 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.76333332)\n",
      "Step 338: loss = 309309.71 (17.400 sec) ('Training Accuracy:', 0.97622699) ('Testing Accuracy:', 0.75666666)\n",
      "Step 339: loss = 349562.68 (17.591 sec) ('Training Accuracy:', 0.97239262) ('Testing Accuracy:', 0.75)\n",
      "Step 340: loss = 314214.69 (17.424 sec) ('Training Accuracy:', 0.97891104) ('Testing Accuracy:', 0.75999999)\n",
      "Step 341: loss = 375273.20 (17.237 sec) ('Training Accuracy:', 0.97661042) ('Testing Accuracy:', 0.75)\n",
      "Step 342: loss = 482146.35 (17.036 sec) ('Training Accuracy:', 0.97891104) ('Testing Accuracy:', 0.74666667)\n",
      "Step 343: loss = 736861.85 (16.895 sec) ('Training Accuracy:', 0.98082823) ('Testing Accuracy:', 0.75333333)\n",
      "Step 344: loss = 1212664.03 (16.494 sec) ('Training Accuracy:', 0.9743098) ('Testing Accuracy:', 0.74666667)\n",
      "Step 345: loss = 1242282.93 (17.104 sec) ('Training Accuracy:', 0.97776073) ('Testing Accuracy:', 0.75333333)\n",
      "Step 346: loss = 1771030.02 (16.483 sec) ('Training Accuracy:', 0.97776073) ('Testing Accuracy:', 0.74666667)\n",
      "Step 347: loss = 2395387.10 (16.929 sec) ('Training Accuracy:', 0.97699386) ('Testing Accuracy:', 0.73333335)\n",
      "Step 348: loss = 1748945.65 (16.076 sec) ('Training Accuracy:', 0.9601227) ('Testing Accuracy:', 0.73000002)\n",
      "Step 349: loss = 3569449.16 (16.313 sec) ('Training Accuracy:', 0.97546011) ('Testing Accuracy:', 0.74666667)\n",
      "Step 350: loss = 2365223.03 (15.916 sec) ('Training Accuracy:', 0.93634969) ('Testing Accuracy:', 0.75)\n",
      "Step 351: loss = 3508915.28 (15.588 sec) ('Training Accuracy:', 0.93366563) ('Testing Accuracy:', 0.73333335)\n",
      "Step 352: loss = 3920795.44 (16.148 sec) ('Training Accuracy:', 0.9417178) ('Testing Accuracy:', 0.73666668)\n",
      "Step 353: loss = 3476316.55 (16.232 sec) ('Training Accuracy:', 0.89685583) ('Testing Accuracy:', 0.72666669)\n",
      "Step 354: loss = 4546066.29 (16.394 sec) ('Training Accuracy:', 0.91219324) ('Testing Accuracy:', 0.72333336)\n",
      "Step 355: loss = 4405293.64 (15.856 sec) ('Training Accuracy:', 0.84700918) ('Testing Accuracy:', 0.71333331)\n",
      "Step 356: loss = 4264267.64 (16.242 sec) ('Training Accuracy:', 0.83934051) ('Testing Accuracy:', 0.69666666)\n",
      "Step 357: loss = 6504622.35 (15.477 sec) ('Training Accuracy:', 0.75613499) ('Testing Accuracy:', 0.68000001)\n",
      "Step 358: loss = 6798703.55 (15.546 sec) ('Training Accuracy:', 0.80176383) ('Testing Accuracy:', 0.65333331)\n",
      "Step 359: loss = 6748232.59 (15.694 sec) ('Training Accuracy:', 0.74654907) ('Testing Accuracy:', 0.66666669)\n",
      "Step 360: loss = 8621281.30 (15.939 sec) ('Training Accuracy:', 0.81595093) ('Testing Accuracy:', 0.68666667)\n",
      "Step 361: loss = 7634028.56 (16.140 sec) ('Training Accuracy:', 0.92791408) ('Testing Accuracy:', 0.73333335)\n",
      "Step 362: loss = 9400271.80 (15.462 sec) ('Training Accuracy:', 0.9559049) ('Testing Accuracy:', 0.76333332)\n",
      "Step 363: loss = 11726260.17 (15.180 sec) ('Training Accuracy:', 0.9263804) ('Testing Accuracy:', 0.75333333)\n",
      "Step 364: loss = 13148936.64 (16.115 sec) ('Training Accuracy:', 0.81288344) ('Testing Accuracy:', 0.72333336)\n",
      "Step 365: loss = 14399726.31 (16.006 sec) ('Training Accuracy:', 0.77530676) ('Testing Accuracy:', 0.71333331)\n",
      "Step 366: loss = 12775191.94 (15.518 sec) ('Training Accuracy:', 0.91334355) ('Testing Accuracy:', 0.75333333)\n",
      "Step 367: loss = 7412940.75 (15.812 sec) ('Training Accuracy:', 0.96817487) ('Testing Accuracy:', 0.73666668)\n",
      "Step 368: loss = 2135192.49 (15.374 sec) ('Training Accuracy:', 0.95897239) ('Testing Accuracy:', 0.75)\n",
      "Step 369: loss = 1086431.46 (15.080 sec) ('Training Accuracy:', 0.97814417) ('Testing Accuracy:', 0.76333332)\n",
      "Step 370: loss = 1140550.33 (15.418 sec) ('Training Accuracy:', 0.97354293) ('Testing Accuracy:', 0.75333333)\n",
      "Step 371: loss = 515095.81 (15.735 sec) ('Training Accuracy:', 0.98044479) ('Testing Accuracy:', 0.74666667)\n",
      "Step 372: loss = 242984.78 (15.690 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.75333333)\n",
      "Step 373: loss = 316438.56 (15.459 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.75333333)\n",
      "Step 374: loss = 381231.50 (15.129 sec) ('Training Accuracy:', 0.98274541) ('Testing Accuracy:', 0.75666666)\n",
      "Step 375: loss = 253299.00 (16.024 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.75)\n",
      "Step 376: loss = 157768.22 (15.793 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.75)\n",
      "Step 377: loss = 144995.57 (15.482 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.75666666)\n",
      "Step 378: loss = 323416.71 (15.530 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.75666666)\n",
      "Step 379: loss = 287997.49 (15.754 sec) ('Training Accuracy:', 0.98274541) ('Testing Accuracy:', 0.75999999)\n",
      "Step 380: loss = 183106.22 (20.424 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.75666666)\n",
      "Step 381: loss = 172508.22 (19.420 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.75333333)\n",
      "Step 382: loss = 242915.94 (18.511 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.75666666)\n",
      "Step 383: loss = 284588.74 (19.227 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.75333333)\n",
      "Step 384: loss = 310549.10 (20.665 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.75999999)\n",
      "Step 385: loss = 217299.51 (14.621 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.75333333)\n",
      "Step 386: loss = 181983.98 (18.654 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.75333333)\n",
      "Step 387: loss = 126472.74 (16.267 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.75)\n",
      "Step 388: loss = 118928.24 (18.962 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.75333333)\n",
      "Step 389: loss = 157856.56 (16.039 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.75999999)\n",
      "Step 390: loss = 327530.54 (16.498 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.75333333)\n",
      "Step 391: loss = 281875.93 (15.502 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.75333333)\n",
      "Step 392: loss = 228678.32 (15.763 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.75)\n",
      "Step 393: loss = 152187.97 (15.950 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.75666666)\n",
      "Step 394: loss = 161492.39 (15.592 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.76333332)\n",
      "Step 395: loss = 240743.59 (15.465 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.75999999)\n",
      "Step 396: loss = 204200.64 (15.743 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.75999999)\n",
      "Step 397: loss = 159746.67 (15.440 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.75666666)\n",
      "Step 398: loss = 165951.79 (15.479 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.76333332)\n",
      "Step 399: loss = 214732.89 (15.973 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.76333332)\n",
      "Step 400: loss = 236886.74 (15.863 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.75333333)\n",
      "Step 401: loss = 216397.10 (15.635 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.76333332)\n",
      "Step 402: loss = 173587.73 (15.864 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.76333332)\n",
      "Step 403: loss = 143674.33 (15.529 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.76333332)\n",
      "Step 404: loss = 155452.40 (15.637 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76333332)\n",
      "Step 405: loss = 214119.14 (16.352 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.75999999)\n",
      "Step 406: loss = 230426.18 (15.869 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.75666666)\n",
      "Step 407: loss = 192422.67 (15.714 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76666665)\n",
      "Step 408: loss = 148685.49 (15.852 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.75666666)\n",
      "Step 409: loss = 142222.31 (15.461 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.75666666)\n",
      "Step 410: loss = 187414.81 (15.261 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.75666666)\n",
      "Step 411: loss = 205826.33 (15.711 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.75666666)\n",
      "Step 412: loss = 217228.75 (16.009 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.75999999)\n",
      "Step 413: loss = 208505.71 (15.936 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.76333332)\n",
      "Step 414: loss = 135074.69 (15.813 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.75666666)\n",
      "Step 415: loss = 158956.09 (15.714 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.75999999)\n",
      "Step 416: loss = 166617.81 (15.434 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76333332)\n",
      "Step 417: loss = 234901.89 (19.318 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.75999999)\n",
      "Step 418: loss = 218553.21 (18.722 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.75999999)\n",
      "Step 419: loss = 220114.45 (18.873 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.76333332)\n",
      "Step 420: loss = 216876.89 (20.854 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.75666666)\n",
      "Step 421: loss = 217522.26 (19.372 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.75666666)\n",
      "Step 422: loss = 193438.17 (16.743 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76333332)\n",
      "Step 423: loss = 105954.90 (16.357 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.75333333)\n",
      "Step 424: loss = 73233.60 (14.358 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.75333333)\n",
      "Step 425: loss = 165855.24 (15.337 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76333332)\n",
      "Step 426: loss = 273402.04 (15.843 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.76333332)\n",
      "Step 427: loss = 242935.99 (15.787 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.75999999)\n",
      "Step 428: loss = 193638.58 (15.678 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.75666666)\n",
      "Step 429: loss = 251439.73 (15.866 sec) ('Training Accuracy:', 0.98274541) ('Testing Accuracy:', 0.75)\n",
      "Step 430: loss = 227368.38 (16.080 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.75999999)\n",
      "Step 431: loss = 149009.22 (15.117 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.75333333)\n",
      "Step 432: loss = 112307.33 (15.881 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.75333333)\n",
      "Step 433: loss = 230011.47 (15.838 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.75666666)\n",
      "Step 434: loss = 309432.49 (16.327 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.75666666)\n",
      "Step 435: loss = 285633.21 (15.997 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.76666665)\n",
      "Step 436: loss = 145793.15 (15.644 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.75333333)\n",
      "Step 437: loss = 139324.21 (16.075 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.75666666)\n",
      "Step 438: loss = 278345.04 (15.821 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.75)\n",
      "Step 439: loss = 255833.61 (15.758 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.75333333)\n",
      "Step 440: loss = 231695.56 (16.528 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.75666666)\n",
      "Step 441: loss = 160930.21 (16.634 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.75999999)\n",
      "Step 442: loss = 84581.47 (16.806 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.76333332)\n",
      "Step 443: loss = 170559.69 (17.108 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76999998)\n",
      "Step 444: loss = 202313.34 (17.404 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.76999998)\n",
      "Step 445: loss = 331338.89 (17.311 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.75999999)\n",
      "Step 446: loss = 352929.19 (17.558 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.76666665)\n",
      "Step 447: loss = 297294.34 (16.975 sec) ('Training Accuracy:', 0.98121166) ('Testing Accuracy:', 0.76333332)\n",
      "Step 448: loss = 146627.90 (17.438 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.76333332)\n",
      "Step 449: loss = 58354.01 (17.379 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.75999999)\n",
      "Step 450: loss = 83993.73 (17.317 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.75999999)\n",
      "Step 451: loss = 149755.76 (17.142 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.76333332)\n",
      "Step 452: loss = 232665.39 (17.773 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.76666665)\n",
      "Step 453: loss = 247345.68 (17.595 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.76666665)\n",
      "Step 454: loss = 290473.66 (17.907 sec) ('Training Accuracy:', 0.98082823) ('Testing Accuracy:', 0.75999999)\n",
      "Step 455: loss = 250735.57 (17.942 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.76666665)\n",
      "Step 456: loss = 267560.93 (18.008 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.76333332)\n",
      "Step 457: loss = 211097.30 (18.012 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.76666665)\n",
      "Step 458: loss = 222518.24 (17.539 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.76666665)\n",
      "Step 459: loss = 203616.69 (17.384 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.76999998)\n",
      "Step 460: loss = 178372.51 (17.565 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.76666665)\n",
      "Step 461: loss = 215390.42 (17.231 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.76666665)\n",
      "Step 462: loss = 215150.28 (17.420 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.77333331)\n",
      "Step 463: loss = 215237.95 (17.098 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76333332)\n",
      "Step 464: loss = 183228.54 (17.326 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.76666665)\n",
      "Step 465: loss = 177372.51 (16.748 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.76333332)\n",
      "Step 466: loss = 246141.05 (17.014 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76333332)\n",
      "Step 467: loss = 337962.97 (16.794 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.75666666)\n",
      "Step 468: loss = 266877.48 (16.962 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.75333333)\n",
      "Step 469: loss = 336493.45 (17.032 sec) ('Training Accuracy:', 0.98121166) ('Testing Accuracy:', 0.76333332)\n",
      "Step 470: loss = 389815.16 (17.310 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.75999999)\n",
      "Step 471: loss = 325004.78 (16.877 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.76333332)\n",
      "Step 472: loss = 185079.39 (16.922 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.75666666)\n",
      "Step 473: loss = 199371.00 (16.889 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76999998)\n",
      "Step 474: loss = 356424.95 (16.690 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.76999998)\n",
      "Step 475: loss = 434156.27 (17.134 sec) ('Training Accuracy:', 0.97967792) ('Testing Accuracy:', 0.76999998)\n",
      "Step 476: loss = 332501.94 (17.310 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.77333331)\n",
      "Step 477: loss = 175254.23 (17.006 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.76999998)\n",
      "Step 478: loss = 189950.42 (17.297 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.77333331)\n",
      "Step 479: loss = 312989.43 (16.374 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.77333331)\n",
      "Step 480: loss = 434315.96 (16.626 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.75999999)\n",
      "Step 481: loss = 442194.70 (16.679 sec) ('Training Accuracy:', 0.98044479) ('Testing Accuracy:', 0.76333332)\n",
      "Step 482: loss = 416947.65 (17.280 sec) ('Training Accuracy:', 0.98274541) ('Testing Accuracy:', 0.76333332)\n",
      "Step 483: loss = 445010.36 (17.271 sec) ('Training Accuracy:', 0.9773773) ('Testing Accuracy:', 0.76333332)\n",
      "Step 484: loss = 1037223.98 (17.192 sec) ('Training Accuracy:', 0.97315949) ('Testing Accuracy:', 0.75)\n",
      "Step 485: loss = 1115221.40 (16.852 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.76333332)\n",
      "Step 486: loss = 781099.50 (16.556 sec) ('Training Accuracy:', 0.98121166) ('Testing Accuracy:', 0.76333332)\n",
      "Step 487: loss = 469427.15 (16.366 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.76333332)\n",
      "Step 488: loss = 214253.46 (16.515 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76333332)\n",
      "Step 489: loss = 82185.14 (16.809 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.76666665)\n",
      "Step 490: loss = 86181.84 (16.958 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.75666666)\n",
      "Step 491: loss = 83591.59 (16.851 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.75666666)\n",
      "Step 492: loss = 198546.42 (17.064 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.75666666)\n",
      "Step 493: loss = 544979.62 (17.088 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.76333332)\n",
      "Step 494: loss = 835513.06 (16.386 sec) ('Training Accuracy:', 0.97814417) ('Testing Accuracy:', 0.76999998)\n",
      "Step 495: loss = 1149931.39 (16.311 sec) ('Training Accuracy:', 0.96970856) ('Testing Accuracy:', 0.75333333)\n",
      "Step 496: loss = 1031089.58 (17.440 sec) ('Training Accuracy:', 0.96280676) ('Testing Accuracy:', 0.75999999)\n",
      "Step 497: loss = 806629.46 (19.415 sec) ('Training Accuracy:', 0.97661042) ('Testing Accuracy:', 0.77999997)\n",
      "Step 498: loss = 694052.50 (23.999 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.74333334)\n",
      "Step 499: loss = 292374.76 (20.627 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.75666666)\n",
      "Step 500: loss = 166227.91 (19.581 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.75666666)\n",
      "Step 501: loss = 147709.24 (17.462 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.75666666)\n",
      "Step 502: loss = 187912.64 (14.687 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.76333332)\n",
      "Step 503: loss = 159865.65 (16.355 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.76333332)\n",
      "Step 504: loss = 387696.67 (16.087 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.75999999)\n",
      "Step 505: loss = 465968.95 (16.604 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.76666665)\n",
      "Step 506: loss = 295339.20 (16.704 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.75999999)\n",
      "Step 507: loss = 259685.06 (16.749 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.76333332)\n",
      "Step 508: loss = 233937.62 (16.590 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.75666666)\n",
      "Step 509: loss = 276100.28 (16.818 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.75333333)\n",
      "Step 510: loss = 414664.75 (16.943 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.75999999)\n",
      "Step 511: loss = 225872.90 (16.233 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76333332)\n",
      "Step 512: loss = 237528.70 (15.985 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.75666666)\n",
      "Step 513: loss = 356340.63 (16.223 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.76666665)\n",
      "Step 514: loss = 284191.86 (17.217 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.75333333)\n",
      "Step 515: loss = 244479.59 (16.493 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.75999999)\n",
      "Step 516: loss = 205006.74 (16.584 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.75666666)\n",
      "Step 517: loss = 268737.29 (16.557 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.75999999)\n",
      "Step 518: loss = 344566.31 (16.442 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.75999999)\n",
      "Step 519: loss = 278437.99 (16.361 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.75999999)\n",
      "Step 520: loss = 250163.45 (16.275 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.76333332)\n",
      "Step 521: loss = 260029.16 (16.504 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.76666665)\n",
      "Step 522: loss = 306519.63 (16.595 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.76333332)\n",
      "Step 523: loss = 225777.52 (16.261 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.76333332)\n",
      "Step 524: loss = 266535.96 (16.655 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.76333332)\n",
      "Step 525: loss = 330566.47 (17.172 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.76666665)\n",
      "Step 526: loss = 197050.25 (16.319 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.75333333)\n",
      "Step 527: loss = 228403.35 (16.089 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.75666666)\n",
      "Step 528: loss = 362505.66 (16.401 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.76333332)\n",
      "Step 529: loss = 472480.98 (16.003 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.76333332)\n",
      "Step 530: loss = 217011.78 (16.941 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.75999999)\n",
      "Step 531: loss = 201413.60 (16.647 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.75999999)\n",
      "Step 532: loss = 275829.31 (16.757 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.75666666)\n",
      "Step 533: loss = 265998.61 (16.553 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.75999999)\n",
      "Step 534: loss = 344738.79 (16.801 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.76333332)\n",
      "Step 535: loss = 318773.97 (16.710 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.75999999)\n",
      "Step 536: loss = 324117.88 (16.441 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.76666665)\n",
      "Step 537: loss = 258640.46 (15.894 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.75999999)\n",
      "Step 538: loss = 242160.20 (16.818 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.75666666)\n",
      "Step 539: loss = 389893.79 (16.662 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.75333333)\n",
      "Step 540: loss = 280339.05 (16.731 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.75999999)\n",
      "Step 541: loss = 220218.07 (16.752 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.75999999)\n",
      "Step 542: loss = 308810.56 (16.409 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.75666666)\n",
      "Step 543: loss = 351672.47 (16.429 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.75666666)\n",
      "Step 544: loss = 324368.46 (16.196 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.76333332)\n",
      "Step 545: loss = 314484.27 (16.387 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.76333332)\n",
      "Step 546: loss = 312245.08 (16.772 sec) ('Training Accuracy:', 0.98274541) ('Testing Accuracy:', 0.75)\n",
      "Step 547: loss = 375612.13 (16.895 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.75333333)\n",
      "Step 548: loss = 330314.03 (16.465 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.75333333)\n",
      "Step 549: loss = 349108.21 (16.041 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.76333332)\n",
      "Step 550: loss = 298393.28 (16.699 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.76999998)\n",
      "Step 551: loss = 272799.93 (16.224 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76999998)\n",
      "Step 552: loss = 223355.61 (16.499 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.77666664)\n",
      "Step 553: loss = 311370.63 (16.041 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.76999998)\n",
      "Step 554: loss = 488235.28 (20.221 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.76999998)\n",
      "Step 555: loss = 394148.06 (20.539 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.76999998)\n",
      "Step 556: loss = 312646.34 (20.373 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.77333331)\n",
      "Step 557: loss = 356140.07 (19.080 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.76999998)\n",
      "Step 558: loss = 427843.42 (16.935 sec) ('Training Accuracy:', 0.97929448) ('Testing Accuracy:', 0.76666665)\n",
      "Step 559: loss = 348301.95 (17.431 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76666665)\n",
      "Step 560: loss = 268444.15 (19.013 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76333332)\n",
      "Step 561: loss = 328534.95 (22.118 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.77333331)\n",
      "Step 562: loss = 425114.75 (21.931 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.76999998)\n",
      "Step 563: loss = 287460.99 (19.483 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.77666664)\n",
      "Step 564: loss = 233708.69 (20.025 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.77666664)\n",
      "Step 565: loss = 313849.68 (19.501 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.76999998)\n",
      "Step 566: loss = 410564.52 (20.176 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.75999999)\n",
      "Step 567: loss = 363596.85 (17.420 sec) ('Training Accuracy:', 0.98044479) ('Testing Accuracy:', 0.75666666)\n",
      "Step 568: loss = 336127.18 (16.984 sec) ('Training Accuracy:', 0.98082823) ('Testing Accuracy:', 0.76333332)\n",
      "Step 569: loss = 340185.21 (16.426 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.76999998)\n",
      "Step 570: loss = 247904.30 (16.803 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.77999997)\n",
      "Step 571: loss = 233959.42 (17.171 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.77999997)\n",
      "Step 572: loss = 434088.88 (16.880 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.76666665)\n",
      "Step 573: loss = 509884.02 (17.276 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.75999999)\n",
      "Step 574: loss = 474509.33 (17.395 sec) ('Training Accuracy:', 0.96932513) ('Testing Accuracy:', 0.75999999)\n",
      "Step 575: loss = 537524.60 (17.275 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.76999998)\n",
      "Step 576: loss = 292547.72 (17.465 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.76333332)\n",
      "Step 577: loss = 237308.36 (17.091 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.76666665)\n",
      "Step 578: loss = 136138.89 (16.740 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76333332)\n",
      "Step 579: loss = 144182.88 (17.173 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.76333332)\n",
      "Step 580: loss = 85131.88 (17.087 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.75666666)\n",
      "Step 581: loss = 171064.10 (16.491 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.75666666)\n",
      "Step 582: loss = 171049.78 (16.780 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.76666665)\n",
      "Step 583: loss = 297228.39 (16.748 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.77666664)\n",
      "Step 584: loss = 434767.39 (16.974 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.76666665)\n",
      "Step 585: loss = 487076.21 (16.883 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.75333333)\n",
      "Step 586: loss = 457121.68 (17.030 sec) ('Training Accuracy:', 0.9815951) ('Testing Accuracy:', 0.75333333)\n",
      "Step 587: loss = 395784.89 (17.213 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.75666666)\n",
      "Step 588: loss = 441804.85 (16.792 sec) ('Training Accuracy:', 0.9815951) ('Testing Accuracy:', 0.75333333)\n",
      "Step 589: loss = 397846.01 (16.871 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.75333333)\n",
      "Step 590: loss = 354172.24 (17.113 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.76333332)\n",
      "Step 591: loss = 332129.54 (16.386 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.75999999)\n",
      "Step 592: loss = 334424.39 (16.181 sec) ('Training Accuracy:', 0.98274541) ('Testing Accuracy:', 0.75666666)\n",
      "Step 593: loss = 371872.16 (16.431 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.75666666)\n",
      "Step 594: loss = 396951.53 (16.690 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.75999999)\n",
      "Step 595: loss = 320542.89 (16.833 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.75999999)\n",
      "Step 596: loss = 277869.65 (16.799 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.75999999)\n",
      "Step 597: loss = 378166.35 (16.693 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.75666666)\n",
      "Step 598: loss = 464888.59 (17.149 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.76333332)\n",
      "Step 599: loss = 528078.29 (16.872 sec) ('Training Accuracy:', 0.9815951) ('Testing Accuracy:', 0.76666665)\n",
      "Step 600: loss = 661507.48 (16.210 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.74333334)\n",
      "Step 601: loss = 420196.56 (16.015 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.79666668)\n",
      "Step 602: loss = 540246.96 (16.231 sec) ('Training Accuracy:', 0.96970856) ('Testing Accuracy:', 0.73000002)\n",
      "Step 603: loss = 572865.85 (16.523 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.75)\n",
      "Step 604: loss = 317507.52 (16.943 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.75666666)\n",
      "Step 605: loss = 220628.57 (16.823 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.75999999)\n",
      "Step 606: loss = 155404.57 (16.805 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.75333333)\n",
      "Step 607: loss = 300845.65 (16.423 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.76666665)\n",
      "Step 608: loss = 468183.18 (16.554 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.76999998)\n",
      "Step 609: loss = 663010.66 (16.446 sec) ('Training Accuracy:', 0.98082823) ('Testing Accuracy:', 0.75666666)\n",
      "Step 610: loss = 886064.42 (16.351 sec) ('Training Accuracy:', 0.97277606) ('Testing Accuracy:', 0.75)\n",
      "Step 611: loss = 1224149.79 (16.649 sec) ('Training Accuracy:', 0.9773773) ('Testing Accuracy:', 0.74333334)\n",
      "Step 612: loss = 1222953.77 (16.424 sec) ('Training Accuracy:', 0.96088958) ('Testing Accuracy:', 0.76333332)\n",
      "Step 613: loss = 1464775.11 (16.745 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.74000001)\n",
      "Step 614: loss = 682773.28 (17.108 sec) ('Training Accuracy:', 0.96855831) ('Testing Accuracy:', 0.75333333)\n",
      "Step 615: loss = 1289378.80 (16.430 sec) ('Training Accuracy:', 0.97239262) ('Testing Accuracy:', 0.74666667)\n",
      "Step 616: loss = 1435633.05 (17.029 sec) ('Training Accuracy:', 0.95705521) ('Testing Accuracy:', 0.75999999)\n",
      "Step 617: loss = 2006614.32 (17.087 sec) ('Training Accuracy:', 0.98044479) ('Testing Accuracy:', 0.72000003)\n",
      "Step 618: loss = 1547024.29 (16.199 sec) ('Training Accuracy:', 0.94823617) ('Testing Accuracy:', 0.75333333)\n",
      "Step 619: loss = 2523924.45 (15.922 sec) ('Training Accuracy:', 0.97085887) ('Testing Accuracy:', 0.74333334)\n",
      "Step 620: loss = 1386378.70 (16.298 sec) ('Training Accuracy:', 0.94708592) ('Testing Accuracy:', 0.74333334)\n",
      "Step 621: loss = 1811628.48 (16.312 sec) ('Training Accuracy:', 0.9302147) ('Testing Accuracy:', 0.74666667)\n",
      "Step 622: loss = 1887281.12 (16.229 sec) ('Training Accuracy:', 0.93174845) ('Testing Accuracy:', 0.75333333)\n",
      "Step 623: loss = 2770499.99 (16.463 sec) ('Training Accuracy:', 0.93174845) ('Testing Accuracy:', 0.75333333)\n",
      "Step 624: loss = 1691080.32 (16.569 sec) ('Training Accuracy:', 0.89378834) ('Testing Accuracy:', 0.72666669)\n",
      "Step 625: loss = 2944000.08 (16.318 sec) ('Training Accuracy:', 0.904908) ('Testing Accuracy:', 0.75)\n",
      "Step 626: loss = 2440457.34 (16.296 sec) ('Training Accuracy:', 0.90337425) ('Testing Accuracy:', 0.74000001)\n",
      "Step 627: loss = 2704452.21 (15.643 sec) ('Training Accuracy:', 0.91334355) ('Testing Accuracy:', 0.74666667)\n",
      "Step 628: loss = 2319489.21 (15.716 sec) ('Training Accuracy:', 0.88573617) ('Testing Accuracy:', 0.72666669)\n",
      "Step 629: loss = 3133031.70 (15.863 sec) ('Training Accuracy:', 0.88573617) ('Testing Accuracy:', 0.74000001)\n",
      "Step 630: loss = 3419650.23 (16.244 sec) ('Training Accuracy:', 0.8976227) ('Testing Accuracy:', 0.73333335)\n",
      "Step 631: loss = 3284816.16 (16.930 sec) ('Training Accuracy:', 0.88036811) ('Testing Accuracy:', 0.74333334)\n",
      "Step 632: loss = 3095276.41 (16.014 sec) ('Training Accuracy:', 0.92177916) ('Testing Accuracy:', 0.73000002)\n",
      "Step 633: loss = 2692744.85 (16.272 sec) ('Training Accuracy:', 0.81403375) ('Testing Accuracy:', 0.72333336)\n",
      "Step 634: loss = 5245819.22 (15.668 sec) ('Training Accuracy:', 0.88573617) ('Testing Accuracy:', 0.75333333)\n",
      "Step 635: loss = 3631456.17 (15.957 sec) ('Training Accuracy:', 0.80099696) ('Testing Accuracy:', 0.72666669)\n",
      "Step 636: loss = 8716245.45 (15.546 sec) ('Training Accuracy:', 0.92101228) ('Testing Accuracy:', 0.74333334)\n",
      "Step 637: loss = 2596495.17 (16.081 sec) ('Training Accuracy:', 0.87154907) ('Testing Accuracy:', 0.73000002)\n",
      "Step 638: loss = 6918493.18 (15.981 sec) ('Training Accuracy:', 0.80943251) ('Testing Accuracy:', 0.72666669)\n",
      "Step 639: loss = 7173773.16 (15.976 sec) ('Training Accuracy:', 0.77453989) ('Testing Accuracy:', 0.69999999)\n",
      "Step 640: loss = 6588563.66 (15.978 sec) ('Training Accuracy:', 0.84202456) ('Testing Accuracy:', 0.74666667)\n",
      "Step 641: loss = 8728124.51 (15.785 sec) ('Training Accuracy:', 0.75805217) ('Testing Accuracy:', 0.70999998)\n",
      "Step 642: loss = 10129220.72 (15.759 sec) ('Training Accuracy:', 0.81633437) ('Testing Accuracy:', 0.71333331)\n",
      "Step 643: loss = 11568853.80 (15.782 sec) ('Training Accuracy:', 0.85122699) ('Testing Accuracy:', 0.74000001)\n",
      "Step 644: loss = 21477687.28 (16.061 sec) ('Training Accuracy:', 0.92676383) ('Testing Accuracy:', 0.73666668)\n",
      "Step 645: loss = 29248307.93 (16.218 sec) ('Training Accuracy:', 0.92829752) ('Testing Accuracy:', 0.74333334)\n",
      "Step 646: loss = 31255475.14 (15.657 sec) ('Training Accuracy:', 0.7101227) ('Testing Accuracy:', 0.64999998)\n",
      "Step 647: loss = 23604085.62 (15.750 sec) ('Training Accuracy:', 0.8351227) ('Testing Accuracy:', 0.68333334)\n",
      "Step 648: loss = 13110365.11 (15.402 sec) ('Training Accuracy:', 0.96817487) ('Testing Accuracy:', 0.78333336)\n",
      "Step 649: loss = 4663894.02 (15.162 sec) ('Training Accuracy:', 0.91679448) ('Testing Accuracy:', 0.74666667)\n",
      "Step 650: loss = 3102786.92 (15.568 sec) ('Training Accuracy:', 0.97852761) ('Testing Accuracy:', 0.76999998)\n",
      "Step 651: loss = 1512873.03 (15.637 sec) ('Training Accuracy:', 0.97200918) ('Testing Accuracy:', 0.77333331)\n",
      "Step 652: loss = 1043458.64 (15.745 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.78333336)\n",
      "Step 653: loss = 497556.24 (15.562 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.78333336)\n",
      "Step 654: loss = 414179.28 (15.325 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.77666664)\n",
      "Step 655: loss = 315741.79 (15.137 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.76666665)\n",
      "Step 656: loss = 182661.32 (15.391 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.76666665)\n",
      "Step 657: loss = 102040.12 (15.699 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.77666664)\n",
      "Step 658: loss = 97021.46 (14.959 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.77999997)\n",
      "Step 659: loss = 84506.66 (15.141 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.77666664)\n",
      "Step 660: loss = 95396.69 (15.665 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.77999997)\n",
      "Step 661: loss = 188475.21 (15.013 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.78333336)\n",
      "Step 662: loss = 279278.51 (15.724 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.78333336)\n",
      "Step 663: loss = 263504.97 (15.131 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.77333331)\n",
      "Step 664: loss = 257912.96 (15.002 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.76666665)\n",
      "Step 665: loss = 112466.88 (15.627 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.75999999)\n",
      "Step 666: loss = 165510.29 (15.816 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.75999999)\n",
      "Step 667: loss = 221074.43 (15.794 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.77333331)\n",
      "Step 668: loss = 270457.09 (15.864 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.76999998)\n",
      "Step 669: loss = 163557.26 (15.155 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.77333331)\n",
      "Step 670: loss = 204629.75 (15.279 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.76666665)\n",
      "Step 671: loss = 246151.11 (15.987 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.76999998)\n",
      "Step 672: loss = 188031.99 (15.581 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.76666665)\n",
      "Step 673: loss = 147006.61 (15.565 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.76333332)\n",
      "Step 674: loss = 102928.07 (15.634 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.76999998)\n",
      "Step 675: loss = 73990.27 (15.007 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.76999998)\n",
      "Step 676: loss = 49800.62 (15.460 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.76666665)\n",
      "Step 677: loss = 90233.30 (15.775 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.76999998)\n",
      "Step 678: loss = 59731.51 (15.683 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.75999999)\n",
      "Step 679: loss = 61466.53 (15.876 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.76666665)\n",
      "Step 680: loss = 43612.65 (15.539 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.76333332)\n",
      "Step 681: loss = 49200.24 (15.632 sec) ('Training Accuracy:', 0.99003065) ('Testing Accuracy:', 0.75999999)\n",
      "Step 682: loss = 59615.53 (15.412 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.76333332)\n",
      "Step 683: loss = 47409.01 (15.342 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.77333331)\n",
      "Step 684: loss = 59573.72 (16.062 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.77333331)\n",
      "Step 685: loss = 56604.85 (15.391 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.75999999)\n",
      "Step 686: loss = 83051.88 (15.909 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.76333332)\n",
      "Step 687: loss = 83958.55 (15.985 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.76999998)\n",
      "Step 688: loss = 93187.77 (15.796 sec) ('Training Accuracy:', 0.99003065) ('Testing Accuracy:', 0.75666666)\n",
      "Step 689: loss = 272352.92 (15.365 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.76666665)\n",
      "Step 690: loss = 401540.03 (15.481 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.77333331)\n",
      "Step 691: loss = 323388.34 (16.046 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.77666664)\n",
      "Step 692: loss = 222383.52 (16.034 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.77333331)\n",
      "Step 693: loss = 83325.38 (15.758 sec) ('Training Accuracy:', 0.99003065) ('Testing Accuracy:', 0.76333332)\n",
      "Step 694: loss = 73952.77 (15.934 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.75999999)\n",
      "Step 695: loss = 81103.27 (15.185 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.76333332)\n",
      "Step 696: loss = 150055.85 (15.183 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.76333332)\n",
      "Step 697: loss = 271164.34 (15.893 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.76666665)\n",
      "Step 698: loss = 354458.65 (15.595 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.77666664)\n",
      "Step 699: loss = 242337.95 (15.893 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.77333331)\n",
      "Step 700: loss = 163491.45 (15.834 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.76666665)\n",
      "Step 701: loss = 223950.10 (15.516 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76999998)\n",
      "Step 702: loss = 322584.45 (15.485 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.77666664)\n",
      "Step 703: loss = 267673.78 (15.180 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.77666664)\n",
      "Step 704: loss = 78101.95 (15.881 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.77666664)\n",
      "Step 705: loss = 101671.88 (15.765 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.78333336)\n",
      "Step 706: loss = 178794.17 (15.745 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.79000002)\n",
      "Step 707: loss = 277723.81 (15.698 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.79000002)\n",
      "Step 708: loss = 325283.96 (14.842 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.78666669)\n",
      "Step 709: loss = 168360.10 (15.354 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.77999997)\n",
      "Step 710: loss = 150797.86 (15.681 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.78333336)\n",
      "Step 711: loss = 231669.96 (15.686 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.77999997)\n",
      "Step 712: loss = 168387.34 (15.834 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.77999997)\n",
      "Step 713: loss = 209580.54 (15.557 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.78333336)\n",
      "Step 714: loss = 229083.19 (15.756 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.78333336)\n",
      "Step 715: loss = 197980.59 (15.295 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.77999997)\n",
      "Step 716: loss = 83163.92 (15.386 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.76999998)\n",
      "Step 717: loss = 138358.01 (15.906 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.77999997)\n",
      "Step 718: loss = 248042.31 (15.963 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.77333331)\n",
      "Step 719: loss = 277716.22 (15.614 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.78333336)\n",
      "Step 720: loss = 206119.85 (15.475 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.78333336)\n",
      "Step 721: loss = 185011.61 (15.468 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.77666664)\n",
      "Step 722: loss = 226608.00 (16.088 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.77666664)\n",
      "Step 723: loss = 155407.67 (16.371 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.77999997)\n",
      "Step 724: loss = 204238.19 (16.423 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.77999997)\n",
      "Step 725: loss = 263793.75 (16.360 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.77999997)\n",
      "Step 726: loss = 207128.27 (16.431 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.77333331)\n",
      "Step 727: loss = 246368.92 (16.427 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.77666664)\n",
      "Step 728: loss = 199037.02 (21.034 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.76999998)\n",
      "Step 729: loss = 164283.07 (21.954 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.77333331)\n",
      "Step 730: loss = 206449.55 (18.019 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.76999998)\n",
      "Step 731: loss = 287246.75 (20.552 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.78333336)\n",
      "Step 732: loss = 282243.66 (22.334 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.77666664)\n",
      "Step 733: loss = 153983.76 (18.525 sec) ('Training Accuracy:', 0.99003065) ('Testing Accuracy:', 0.76666665)\n",
      "Step 734: loss = 53269.58 (20.102 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.76666665)\n",
      "Step 735: loss = 55288.85 (20.106 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.76999998)\n",
      "Step 736: loss = 52744.38 (20.852 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.77333331)\n",
      "Step 737: loss = 64554.35 (18.004 sec) ('Training Accuracy:', 0.99156439) ('Testing Accuracy:', 0.76666665)\n",
      "Step 738: loss = 47777.70 (21.748 sec) ('Training Accuracy:', 0.99194783) ('Testing Accuracy:', 0.76666665)\n",
      "Step 739: loss = 41558.98 (20.983 sec) ('Training Accuracy:', 0.99233127) ('Testing Accuracy:', 0.76666665)\n",
      "Step 740: loss = 40656.86 (19.794 sec) ('Training Accuracy:', 0.99156439) ('Testing Accuracy:', 0.76666665)\n",
      "Step 741: loss = 59121.63 (25.165 sec) ('Training Accuracy:', 0.99309814) ('Testing Accuracy:', 0.76666665)\n",
      "Step 742: loss = 73241.26 (19.795 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.77333331)\n",
      "Step 743: loss = 94626.30 (16.980 sec) ('Training Accuracy:', 0.99194783) ('Testing Accuracy:', 0.76666665)\n",
      "Step 744: loss = 59185.72 (17.519 sec) ('Training Accuracy:', 0.99156439) ('Testing Accuracy:', 0.76666665)\n",
      "Step 745: loss = 86444.54 (17.734 sec) ('Training Accuracy:', 0.99233127) ('Testing Accuracy:', 0.76999998)\n",
      "Step 746: loss = 75953.83 (17.251 sec) ('Training Accuracy:', 0.99233127) ('Testing Accuracy:', 0.77333331)\n",
      "Step 747: loss = 124536.68 (16.518 sec) ('Training Accuracy:', 0.9927147) ('Testing Accuracy:', 0.76666665)\n",
      "Step 748: loss = 192168.75 (16.335 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.76999998)\n",
      "Step 749: loss = 318995.38 (17.092 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.77666664)\n",
      "Step 750: loss = 288907.25 (16.893 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.77333331)\n",
      "Step 751: loss = 294515.66 (17.326 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.77999997)\n",
      "Step 752: loss = 253495.79 (16.718 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.77999997)\n",
      "Step 753: loss = 289501.77 (16.772 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.78333336)\n",
      "Step 754: loss = 218476.42 (17.094 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.78333336)\n",
      "Step 755: loss = 306533.79 (16.653 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.78333336)\n",
      "Step 756: loss = 181670.67 (16.184 sec) ('Training Accuracy:', 0.99194783) ('Testing Accuracy:', 0.77333331)\n",
      "Step 757: loss = 176954.26 (16.337 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.77666664)\n",
      "Step 758: loss = 250630.79 (16.944 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.77333331)\n",
      "Step 759: loss = 327117.06 (16.755 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.77666664)\n",
      "Step 760: loss = 227284.84 (16.499 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.77666664)\n",
      "Step 761: loss = 186976.20 (16.551 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.77333331)\n",
      "Step 762: loss = 251332.26 (16.951 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.77666664)\n",
      "Step 763: loss = 294165.15 (16.711 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.77666664)\n",
      "Step 764: loss = 272238.88 (16.872 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.77333331)\n",
      "Step 765: loss = 271325.78 (16.279 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.77333331)\n",
      "Step 766: loss = 233530.42 (16.541 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.77333331)\n",
      "Step 767: loss = 108874.29 (16.981 sec) ('Training Accuracy:', 0.99309814) ('Testing Accuracy:', 0.76999998)\n",
      "Step 768: loss = 194266.96 (17.237 sec) ('Training Accuracy:', 0.99003065) ('Testing Accuracy:', 0.76999998)\n",
      "Step 769: loss = 289043.43 (16.929 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.77333331)\n",
      "Step 770: loss = 249608.00 (16.647 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.77333331)\n",
      "Step 771: loss = 269402.45 (16.801 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.77666664)\n",
      "Step 772: loss = 278728.89 (16.976 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.77666664)\n",
      "Step 773: loss = 241454.79 (16.736 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.77333331)\n",
      "Step 774: loss = 189208.49 (16.323 sec) ('Training Accuracy:', 0.99003065) ('Testing Accuracy:', 0.77666664)\n",
      "Step 775: loss = 252111.04 (16.578 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.77333331)\n",
      "Step 776: loss = 446448.53 (16.797 sec) ('Training Accuracy:', 0.9815951) ('Testing Accuracy:', 0.77666664)\n",
      "Step 777: loss = 416776.77 (16.873 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.77666664)\n",
      "Step 778: loss = 364669.02 (16.803 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.76999998)\n",
      "Step 779: loss = 379970.37 (17.027 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.77999997)\n",
      "Step 780: loss = 398196.84 (16.881 sec) ('Training Accuracy:', 0.99233127) ('Testing Accuracy:', 0.76666665)\n",
      "Step 781: loss = 837868.42 (16.973 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.77333331)\n",
      "Step 782: loss = 799905.00 (16.061 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.78333336)\n",
      "Step 783: loss = 601225.84 (16.203 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.78333336)\n",
      "Step 784: loss = 286857.16 (16.491 sec) ('Training Accuracy:', 0.99003065) ('Testing Accuracy:', 0.77333331)\n",
      "Step 785: loss = 104501.02 (16.765 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.77666664)\n",
      "Step 786: loss = 86144.56 (16.450 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.77333331)\n",
      "Step 787: loss = 92731.32 (16.481 sec) ('Training Accuracy:', 0.99156439) ('Testing Accuracy:', 0.77333331)\n",
      "Step 788: loss = 196687.37 (16.506 sec) ('Training Accuracy:', 0.99194783) ('Testing Accuracy:', 0.77333331)\n",
      "Step 789: loss = 266303.15 (16.824 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.76999998)\n",
      "Step 790: loss = 453029.74 (16.258 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.77999997)\n",
      "Step 791: loss = 553492.30 (16.296 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.77333331)\n",
      "Step 792: loss = 396416.66 (16.062 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.77333331)\n",
      "Step 793: loss = 359553.13 (16.646 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.77999997)\n",
      "Step 794: loss = 163191.91 (16.450 sec) ('Training Accuracy:', 0.99003065) ('Testing Accuracy:', 0.77333331)\n",
      "Step 795: loss = 224431.26 (16.948 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.76999998)\n",
      "Step 796: loss = 262758.61 (16.585 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.76666665)\n",
      "Step 797: loss = 356418.20 (16.642 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.76666665)\n",
      "Step 798: loss = 508175.02 (15.831 sec) ('Training Accuracy:', 0.98121166) ('Testing Accuracy:', 0.75999999)\n",
      "Step 799: loss = 506361.32 (16.212 sec) ('Training Accuracy:', 0.97967792) ('Testing Accuracy:', 0.75666666)\n",
      "Step 800: loss = 139626.44 (16.712 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.75333333)\n",
      "Step 801: loss = 134825.03 (16.233 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.76333332)\n",
      "Step 802: loss = 267808.02 (16.554 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.77666664)\n",
      "Step 803: loss = 453780.82 (16.371 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.77333331)\n",
      "Step 804: loss = 466837.87 (16.585 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.77999997)\n",
      "Step 805: loss = 332189.79 (16.589 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.77333331)\n",
      "Step 806: loss = 377374.08 (16.422 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.76999998)\n",
      "Step 807: loss = 229773.62 (15.704 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.76999998)\n",
      "Step 808: loss = 261292.05 (17.243 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.76999998)\n",
      "Step 809: loss = 283067.43 (16.086 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.76999998)\n",
      "Step 810: loss = 303321.78 (16.373 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.77999997)\n",
      "Step 811: loss = 383698.67 (16.670 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.77666664)\n",
      "Step 812: loss = 343656.43 (16.221 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.76999998)\n",
      "Step 813: loss = 343423.04 (16.272 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.76999998)\n",
      "Step 814: loss = 339450.86 (15.955 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.76999998)\n",
      "Step 815: loss = 345105.42 (16.177 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.76999998)\n",
      "Step 816: loss = 344262.54 (15.751 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.76999998)\n",
      "Step 817: loss = 340637.00 (16.787 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.76999998)\n",
      "Step 818: loss = 348028.10 (16.643 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.77333331)\n",
      "Step 819: loss = 346418.54 (16.313 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.77666664)\n",
      "Step 820: loss = 349445.10 (16.215 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.77666664)\n",
      "Step 821: loss = 346548.93 (15.962 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.77666664)\n",
      "Step 822: loss = 349603.62 (15.854 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.77666664)\n",
      "Step 823: loss = 349491.91 (16.531 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.77666664)\n",
      "Step 824: loss = 348889.33 (16.338 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.77666664)\n",
      "Step 825: loss = 340642.00 (16.465 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.77333331)\n",
      "Step 826: loss = 344476.38 (16.305 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.77666664)\n",
      "Step 827: loss = 347079.09 (16.611 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.77666664)\n",
      "Step 828: loss = 349461.45 (16.384 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.77666664)\n",
      "Step 829: loss = 339418.22 (16.159 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.77666664)\n",
      "Step 830: loss = 287226.86 (15.851 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.76999998)\n",
      "Step 831: loss = 124171.04 (15.832 sec) ('Training Accuracy:', 0.99194783) ('Testing Accuracy:', 0.75999999)\n",
      "Step 832: loss = 154309.54 (16.091 sec) ('Training Accuracy:', 0.99233127) ('Testing Accuracy:', 0.75999999)\n",
      "Step 833: loss = 207581.44 (16.757 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.76666665)\n",
      "Step 834: loss = 516976.65 (16.438 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.76666665)\n",
      "Step 835: loss = 600584.61 (16.436 sec) ('Training Accuracy:', 0.97661042) ('Testing Accuracy:', 0.76333332)\n",
      "Step 836: loss = 386964.00 (16.326 sec) ('Training Accuracy:', 0.97776073) ('Testing Accuracy:', 0.76666665)\n",
      "Step 837: loss = 412703.14 (16.502 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.77999997)\n",
      "Step 838: loss = 218838.34 (16.261 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.77999997)\n",
      "Step 839: loss = 290916.34 (16.002 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.77333331)\n",
      "Step 840: loss = 324707.15 (16.020 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.76999998)\n",
      "Step 841: loss = 301401.58 (16.545 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.77333331)\n",
      "Step 842: loss = 334155.97 (16.464 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.77333331)\n",
      "Step 843: loss = 434680.95 (16.718 sec) ('Training Accuracy:', 0.98082823) ('Testing Accuracy:', 0.76333332)\n",
      "Step 844: loss = 299398.01 (16.565 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.77333331)\n",
      "Step 845: loss = 278710.23 (15.983 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.77333331)\n",
      "Step 846: loss = 397820.48 (16.122 sec) ('Training Accuracy:', 0.98197854) ('Testing Accuracy:', 0.76999998)\n",
      "Step 847: loss = 377462.88 (16.141 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.77666664)\n",
      "Step 848: loss = 352895.34 (16.113 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.76666665)\n",
      "Step 849: loss = 313139.23 (16.197 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.77666664)\n",
      "Step 850: loss = 258871.52 (16.569 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.77666664)\n",
      "Step 851: loss = 314765.55 (16.294 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.76666665)\n",
      "Step 852: loss = 406423.43 (16.671 sec) ('Training Accuracy:', 0.97967792) ('Testing Accuracy:', 0.76666665)\n",
      "Step 853: loss = 408472.07 (16.448 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.76999998)\n",
      "Step 854: loss = 352429.38 (16.444 sec) ('Training Accuracy:', 0.98274541) ('Testing Accuracy:', 0.75333333)\n",
      "Step 855: loss = 348882.50 (16.310 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.76999998)\n",
      "Step 856: loss = 366194.75 (15.687 sec) ('Training Accuracy:', 0.98389572) ('Testing Accuracy:', 0.76333332)\n",
      "Step 857: loss = 381320.08 (16.399 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.76333332)\n",
      "Step 858: loss = 348630.60 (16.528 sec) ('Training Accuracy:', 0.98427916) ('Testing Accuracy:', 0.76333332)\n",
      "Step 859: loss = 393741.01 (16.234 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.75999999)\n",
      "Step 860: loss = 325241.85 (16.196 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.75999999)\n",
      "Step 861: loss = 388039.43 (16.555 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.75666666)\n",
      "Step 862: loss = 303783.27 (16.402 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.76999998)\n",
      "Step 863: loss = 199744.69 (16.558 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.76333332)\n",
      "Step 864: loss = 384153.45 (16.481 sec) ('Training Accuracy:', 0.9858129) ('Testing Accuracy:', 0.76999998)\n",
      "Step 865: loss = 411395.47 (16.114 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.77999997)\n",
      "Step 866: loss = 467564.21 (16.251 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.77666664)\n",
      "Step 867: loss = 304615.84 (15.950 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.78333336)\n",
      "Step 868: loss = 409444.05 (16.434 sec) ('Training Accuracy:', 0.9815951) ('Testing Accuracy:', 0.76666665)\n",
      "Step 869: loss = 403349.38 (16.418 sec) ('Training Accuracy:', 0.98466259) ('Testing Accuracy:', 0.77666664)\n",
      "Step 870: loss = 305332.78 (16.377 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.77999997)\n",
      "Step 871: loss = 274366.84 (16.374 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.77999997)\n",
      "Step 872: loss = 386643.80 (16.840 sec) ('Training Accuracy:', 0.98351228) ('Testing Accuracy:', 0.77999997)\n",
      "Step 873: loss = 570079.68 (16.810 sec) ('Training Accuracy:', 0.98236197) ('Testing Accuracy:', 0.76666665)\n",
      "Step 874: loss = 484832.48 (16.342 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.77999997)\n",
      "Step 875: loss = 230180.47 (16.295 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.77333331)\n",
      "Step 876: loss = 213455.38 (16.172 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.78333336)\n",
      "Step 877: loss = 476501.04 (16.657 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.77999997)\n",
      "Step 878: loss = 427106.59 (16.567 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.78333336)\n",
      "Step 879: loss = 525568.92 (16.537 sec) ('Training Accuracy:', 0.97507668) ('Testing Accuracy:', 0.75)\n",
      "Step 880: loss = 530581.80 (16.664 sec) ('Training Accuracy:', 0.97162575) ('Testing Accuracy:', 0.74333334)\n",
      "Step 881: loss = 581924.00 (16.543 sec) ('Training Accuracy:', 0.97085887) ('Testing Accuracy:', 0.74333334)\n",
      "Step 882: loss = 903682.84 (16.243 sec) ('Training Accuracy:', 0.967408) ('Testing Accuracy:', 0.74666667)\n",
      "Step 883: loss = 1131359.34 (16.255 sec) ('Training Accuracy:', 0.96625769) ('Testing Accuracy:', 0.75)\n",
      "Step 884: loss = 1145620.37 (15.660 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.74666667)\n",
      "Step 885: loss = 1770073.33 (15.931 sec) ('Training Accuracy:', 0.96434051) ('Testing Accuracy:', 0.74000001)\n",
      "Step 886: loss = 1614989.03 (16.393 sec) ('Training Accuracy:', 0.97392637) ('Testing Accuracy:', 0.77666664)\n",
      "Step 887: loss = 3128050.27 (16.092 sec) ('Training Accuracy:', 0.96395707) ('Testing Accuracy:', 0.73666668)\n",
      "Step 888: loss = 3350454.29 (16.168 sec) ('Training Accuracy:', 0.967408) ('Testing Accuracy:', 0.77666664)\n",
      "Step 889: loss = 2464078.83 (15.763 sec) ('Training Accuracy:', 0.97852761) ('Testing Accuracy:', 0.75999999)\n",
      "Step 890: loss = 3657618.90 (15.555 sec) ('Training Accuracy:', 0.97277606) ('Testing Accuracy:', 0.73666668)\n",
      "Step 891: loss = 4259670.39 (15.170 sec) ('Training Accuracy:', 0.97239262) ('Testing Accuracy:', 0.76333332)\n",
      "Step 892: loss = 4625948.05 (15.516 sec) ('Training Accuracy:', 0.97124231) ('Testing Accuracy:', 0.74000001)\n",
      "Step 893: loss = 5182862.20 (15.396 sec) ('Training Accuracy:', 0.96165645) ('Testing Accuracy:', 0.75333333)\n",
      "Step 894: loss = 5178054.74 (15.683 sec) ('Training Accuracy:', 0.97162575) ('Testing Accuracy:', 0.76999998)\n",
      "Step 895: loss = 8231117.93 (15.648 sec) ('Training Accuracy:', 0.95897239) ('Testing Accuracy:', 0.75333333)\n",
      "Step 896: loss = 10553357.23 (15.630 sec) ('Training Accuracy:', 0.96280676) ('Testing Accuracy:', 0.76333332)\n",
      "Step 897: loss = 10903804.48 (15.379 sec) ('Training Accuracy:', 0.95283741) ('Testing Accuracy:', 0.74666667)\n",
      "Step 898: loss = 16341778.12 (15.139 sec) ('Training Accuracy:', 0.9148773) ('Testing Accuracy:', 0.73666668)\n",
      "Step 899: loss = 23784936.06 (15.201 sec) ('Training Accuracy:', 0.73044479) ('Testing Accuracy:', 0.65333331)\n",
      "Step 900: loss = 25721950.09 (15.627 sec) ('Training Accuracy:', 0.65950918) ('Testing Accuracy:', 0.61666667)\n",
      "Step 901: loss = 29556763.00 (15.470 sec) ('Training Accuracy:', 0.89685583) ('Testing Accuracy:', 0.70666665)\n",
      "Step 902: loss = 20712815.67 (15.295 sec) ('Training Accuracy:', 0.96088958) ('Testing Accuracy:', 0.75999999)\n",
      "Step 903: loss = 9509180.02 (15.131 sec) ('Training Accuracy:', 0.88381904) ('Testing Accuracy:', 0.74333334)\n",
      "Step 904: loss = 5675734.20 (15.356 sec) ('Training Accuracy:', 0.9773773) ('Testing Accuracy:', 0.81)\n",
      "Step 905: loss = 1786341.86 (14.990 sec) ('Training Accuracy:', 0.970092) ('Testing Accuracy:', 0.78333336)\n",
      "Step 906: loss = 1488930.92 (15.851 sec) ('Training Accuracy:', 0.98082823) ('Testing Accuracy:', 0.80000001)\n",
      "Step 907: loss = 772198.43 (15.757 sec) ('Training Accuracy:', 0.98312885) ('Testing Accuracy:', 0.79666668)\n",
      "Step 908: loss = 431526.92 (15.651 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.77666664)\n",
      "Step 909: loss = 264731.94 (14.994 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.78666669)\n",
      "Step 910: loss = 199786.99 (15.017 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.79000002)\n",
      "Step 911: loss = 179540.44 (14.930 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.78666669)\n",
      "Step 912: loss = 216413.24 (15.400 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.80333334)\n",
      "Step 913: loss = 290376.39 (15.342 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.80333334)\n",
      "Step 914: loss = 352099.42 (15.557 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.80333334)\n",
      "Step 915: loss = 158701.32 (50.585 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.80000001)\n",
      "Step 916: loss = 186947.76 (50.498 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.80333334)\n",
      "Step 917: loss = 215095.81 (51.499 sec) ('Training Accuracy:', 0.98619634) ('Testing Accuracy:', 0.80333334)\n",
      "Step 918: loss = 203484.16 (51.123 sec) ('Training Accuracy:', 0.98696321) ('Testing Accuracy:', 0.80333334)\n",
      "Step 919: loss = 149486.70 (64.766 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.80333334)\n",
      "Step 920: loss = 167287.53 (51.625 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.80333334)\n",
      "Step 921: loss = 237970.02 (50.477 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.80666667)\n",
      "Step 922: loss = 296839.75 (14.119 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.80666667)\n",
      "Step 923: loss = 147180.08 (14.075 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.80666667)\n",
      "Step 924: loss = 65817.32 (15.878 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.79333335)\n",
      "Step 925: loss = 56739.60 (15.954 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.79000002)\n",
      "Step 926: loss = 67702.86 (15.785 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.80333334)\n",
      "Step 927: loss = 51158.92 (15.790 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.79666668)\n",
      "Step 928: loss = 54121.71 (15.328 sec) ('Training Accuracy:', 0.99194783) ('Testing Accuracy:', 0.79666668)\n",
      "Step 929: loss = 111737.06 (15.967 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.80000001)\n",
      "Step 930: loss = 100038.44 (16.642 sec) ('Training Accuracy:', 0.99156439) ('Testing Accuracy:', 0.79666668)\n",
      "Step 931: loss = 91607.05 (15.302 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.78666669)\n",
      "Step 932: loss = 140593.12 (15.579 sec) ('Training Accuracy:', 0.99003065) ('Testing Accuracy:', 0.79000002)\n",
      "Step 933: loss = 131904.41 (15.563 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.79000002)\n",
      "Step 934: loss = 76742.04 (15.691 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.78666669)\n",
      "Step 935: loss = 73616.88 (15.720 sec) ('Training Accuracy:', 0.99233127) ('Testing Accuracy:', 0.79000002)\n",
      "Step 936: loss = 49795.94 (18.419 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.79000002)\n",
      "Step 937: loss = 83097.99 (20.747 sec) ('Training Accuracy:', 0.99156439) ('Testing Accuracy:', 0.78666669)\n",
      "Step 938: loss = 80209.98 (18.905 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.79333335)\n",
      "Step 939: loss = 63382.80 (18.101 sec) ('Training Accuracy:', 0.99156439) ('Testing Accuracy:', 0.80000001)\n",
      "Step 940: loss = 68727.46 (20.196 sec) ('Training Accuracy:', 0.99156439) ('Testing Accuracy:', 0.79000002)\n",
      "Step 941: loss = 86339.88 (20.249 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.78666669)\n",
      "Step 942: loss = 133634.04 (18.492 sec) ('Training Accuracy:', 0.99194783) ('Testing Accuracy:', 0.79333335)\n",
      "Step 943: loss = 253748.71 (19.759 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.79666668)\n",
      "Step 944: loss = 301997.22 (20.678 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.79333335)\n",
      "Step 945: loss = 174907.17 (18.585 sec) ('Training Accuracy:', 0.99156439) ('Testing Accuracy:', 0.79333335)\n",
      "Step 946: loss = 145435.61 (18.126 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.79666668)\n",
      "Step 947: loss = 210362.95 (21.128 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.79333335)\n",
      "Step 948: loss = 187265.68 (19.736 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.79333335)\n",
      "Step 949: loss = 162208.44 (18.779 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.79000002)\n",
      "Step 950: loss = 157107.62 (20.040 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.79333335)\n",
      "Step 951: loss = 185906.85 (19.844 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.79000002)\n",
      "Step 952: loss = 177369.32 (18.825 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.79333335)\n",
      "Step 953: loss = 190501.64 (19.359 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.79000002)\n",
      "Step 954: loss = 102393.36 (20.783 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.79000002)\n",
      "Step 955: loss = 136369.12 (19.409 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.79000002)\n",
      "Step 956: loss = 126601.13 (19.906 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.79000002)\n",
      "Step 957: loss = 173762.99 (20.650 sec) ('Training Accuracy:', 0.99194783) ('Testing Accuracy:', 0.79000002)\n",
      "Step 958: loss = 154758.02 (18.066 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.79666668)\n",
      "Step 959: loss = 211198.05 (18.696 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.79333335)\n",
      "Step 960: loss = 198891.52 (20.389 sec) ('Training Accuracy:', 0.98849696) ('Testing Accuracy:', 0.79333335)\n",
      "Step 961: loss = 181584.09 (20.374 sec) ('Training Accuracy:', 0.99003065) ('Testing Accuracy:', 0.79000002)\n",
      "Step 962: loss = 138800.89 (18.553 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.78333336)\n",
      "Step 963: loss = 157462.68 (20.768 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.78666669)\n",
      "Step 964: loss = 194284.55 (20.092 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.78333336)\n",
      "Step 965: loss = 126715.71 (18.340 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.77999997)\n",
      "Step 966: loss = 159846.59 (20.433 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.78666669)\n",
      "Step 967: loss = 219624.05 (20.129 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.78666669)\n",
      "Step 968: loss = 275549.22 (19.046 sec) ('Training Accuracy:', 0.98542947) ('Testing Accuracy:', 0.79000002)\n",
      "Step 969: loss = 237994.53 (20.762 sec) ('Training Accuracy:', 0.99003065) ('Testing Accuracy:', 0.78333336)\n",
      "Step 970: loss = 98797.75 (19.904 sec) ('Training Accuracy:', 0.99233127) ('Testing Accuracy:', 0.78333336)\n",
      "Step 971: loss = 73281.19 (18.753 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.78333336)\n",
      "Step 972: loss = 80295.14 (20.671 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.78333336)\n",
      "Step 973: loss = 105440.00 (18.903 sec) ('Training Accuracy:', 0.99233127) ('Testing Accuracy:', 0.77666664)\n",
      "Step 974: loss = 161538.99 (19.206 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.78666669)\n",
      "Step 975: loss = 243688.99 (21.330 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.78666669)\n",
      "Step 976: loss = 181862.80 (19.186 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.78666669)\n",
      "Step 977: loss = 252845.38 (21.650 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.79333335)\n",
      "Step 978: loss = 233702.25 (20.689 sec) ('Training Accuracy:', 0.98773009) ('Testing Accuracy:', 0.79000002)\n",
      "Step 979: loss = 105263.04 (22.337 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.77666664)\n",
      "Step 980: loss = 101194.72 (19.618 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.77666664)\n",
      "Step 981: loss = 122076.48 (22.138 sec) ('Training Accuracy:', 0.99041408) ('Testing Accuracy:', 0.78333336)\n",
      "Step 982: loss = 194413.23 (20.456 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.78666669)\n",
      "Step 983: loss = 214229.11 (22.859 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.78666669)\n",
      "Step 984: loss = 183920.43 (22.553 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.78666669)\n",
      "Step 985: loss = 225233.76 (21.007 sec) ('Training Accuracy:', 0.98811352) ('Testing Accuracy:', 0.79333335)\n",
      "Step 986: loss = 201985.09 (22.739 sec) ('Training Accuracy:', 0.99079752) ('Testing Accuracy:', 0.79000002)\n",
      "Step 987: loss = 127715.62 (23.456 sec) ('Training Accuracy:', 0.99233127) ('Testing Accuracy:', 0.78333336)\n",
      "Step 988: loss = 164407.84 (22.292 sec) ('Training Accuracy:', 0.98964721) ('Testing Accuracy:', 0.78333336)\n",
      "Step 989: loss = 267322.68 (22.673 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.78666669)\n",
      "Step 990: loss = 279009.65 (23.327 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.79333335)\n",
      "Step 991: loss = 242184.71 (23.427 sec) ('Training Accuracy:', 0.98734665) ('Testing Accuracy:', 0.79666668)\n",
      "Step 992: loss = 286437.59 (22.378 sec) ('Training Accuracy:', 0.98504603) ('Testing Accuracy:', 0.79000002)\n",
      "Step 993: loss = 254739.10 (23.009 sec) ('Training Accuracy:', 0.98657978) ('Testing Accuracy:', 0.79333335)\n",
      "Step 994: loss = 196562.36 (22.848 sec) ('Training Accuracy:', 0.9888804) ('Testing Accuracy:', 0.79333335)\n",
      "Step 995: loss = 142743.24 (21.613 sec) ('Training Accuracy:', 0.99118096) ('Testing Accuracy:', 0.78666669)\n",
      "Step 996: loss = 79282.45 (21.591 sec) ('Training Accuracy:', 0.99194783) ('Testing Accuracy:', 0.77999997)\n",
      "Step 997: loss = 90243.40 (22.383 sec) ('Training Accuracy:', 0.99233127) ('Testing Accuracy:', 0.77999997)\n",
      "Step 998: loss = 66654.23 (21.584 sec) ('Training Accuracy:', 0.9927147) ('Testing Accuracy:', 0.77666664)\n",
      "Step 999: loss = 55116.77 (22.819 sec) ('Training Accuracy:', 0.98926383) ('Testing Accuracy:', 0.77999997)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_acc, test_acc = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99309814"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w85t2MQ3F3qzAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77f6832b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1000)\n",
    "\n",
    "plt.plot(x, train_acc)\n",
    "#plt.plot(x, test_acc)\n",
    "\n",
    "\n",
    "plt.legend(['y = training_samples', 'y = testing_samples'], loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noiiKoiiKoihtyP8HbGLJV8UB4kYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77c7f5ae50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1000)\n",
    "\n",
    "plt.plot(x, test_acc)\n",
    "\n",
    "\n",
    "plt.legend(['y = testing_samples'], loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_list = pd.DataFrame(\n",
    "    {'Training_Acc': train_acc,\n",
    "     'Testing_Acc': test_acc,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes.AxesSubplot at 0x7f7825f85f90>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f77d00342d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_list.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
