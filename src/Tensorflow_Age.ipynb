{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import os,sys, shutil\n",
    "import time\n",
    "from datetime import date\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import pprint\n",
    "from collections import deque\n",
    "from shutil import copyfile\n",
    "import random\n",
    "import glob\n",
    "# Import the required modules\n",
    "import cv2, os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Logistic Regression\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import math\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_fl = open(\"linkedin_profiles.pickle\",\"rb\")\n",
    "my_original_list=pickle.load(pkl_fl) # errors out here\n",
    "pkl_fl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = \"Male\"\n",
    "    \n",
    "if os.path.exists(directory):\n",
    "    shutil.rmtree(directory)\n",
    "    os.makedirs(directory)     \n",
    "else:\n",
    "    os.makedirs(directory) \n",
    "\n",
    "directory1 = \"Female\"\n",
    "\n",
    "if os.path.exists(directory1):\n",
    "    shutil.rmtree(directory1)\n",
    "    os.makedirs(directory1)     \n",
    "else:\n",
    "    os.makedirs(directory1)     \n",
    "\n",
    "directory2 = \"Label_Images_Age\"\n",
    "\n",
    "if os.path.exists(directory2):\n",
    "    shutil.rmtree(directory2)\n",
    "    os.makedirs(directory2)     \n",
    "else:\n",
    "    os.makedirs(directory2)     \n",
    "    \n",
    "fileList = glob.glob(\"./Images/*.*\")\n",
    "\n",
    "for id,fp in enumerate(fileList):\n",
    "    filename, file_extension = os.path.splitext(fp)\n",
    "    uid = filename.split('/')[-1]\n",
    "    #print fp\n",
    "    for prof in my_original_list:\n",
    "        if prof['User_ID'] == uid:\n",
    "            prof_age = prof['age']\n",
    "            \n",
    "            if (0 <= prof_age <= 30):\n",
    "                new_file_extension = 'Youth'\n",
    "            else:\n",
    "                new_file_extension = 'Senior'\n",
    "            \n",
    "            copyfile(filename + \".jpg\", './Label_Images_Age/'+ uid + '.' + str(id) + \".\" + new_file_extension +'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For face detection we will use the Haar Cascade provided by OpenCV.\n",
    "cascadePath = \"haarcascade_frontalface_default.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cascadePath)\n",
    "\n",
    "# For face recognition we will the the LBPH Face Recognizer \n",
    "recognizer = cv2.createLBPHFaceRecognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_images_and_labels(path):\n",
    "    # Append all the absolute image paths in a list image_paths\n",
    "    \n",
    "    image_paths = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "    # images will contains face images\n",
    "    images = []\n",
    "    # labels will contains the label that is assigned to the image\n",
    "    labels = []\n",
    "    #gender will contains 1 or 0 indecating male or female\n",
    "    age =[]\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        # Read the image and convert to grayscale\n",
    "        try:\n",
    "            image_pil = Image.open(image_path).convert('L')\n",
    "            # Convert the image format into numpy array\n",
    "            image = np.array(image_pil, 'uint8')\n",
    "            # Get the label of the image\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        nbr = int(os.path.split(image_path)[1].split(\".\")[1])\n",
    "        age_current = os.path.split(image_path)[1].split(\".\")[2]\n",
    "        print nbr\n",
    "        \n",
    "        # Detect the face in the image\n",
    "        faces = faceCascade.detectMultiScale(image)\n",
    "        # If face is detected, append the face to images and the label to labels\n",
    "        try:\n",
    "            for (x, y, w, h) in faces:\n",
    "\n",
    "                ref_image = image[y: y + h, x: x + w]\n",
    "                resized = cv2.resize(ref_image, (100, 100), interpolation = cv2.INTER_AREA)\n",
    "                #edge_images = cv2.Canny(resized,100,200)\n",
    "                \n",
    "                images.append(np.array(resized))   #resized.reshape(1,10000)\n",
    "                labels.append(nbr)\n",
    "\n",
    "                if age_current == 'Youth':\n",
    "                    age.append(0)\n",
    "                \n",
    "                else:\n",
    "                    age.append(1)\n",
    "                \n",
    "                #face_file_name = \"faces/face_\" + str(y) + \".jpg\"\n",
    "                #cv2.imwrite(face_file_name, sub_face)\n",
    "                \n",
    "                cv2.imshow(\"Adding faces to traning set...\", resized)\n",
    "                cv2.waitKey(1)\n",
    "        except:\n",
    "            pass\n",
    "    # return the images list and labels list\n",
    "    print \"lables\"\n",
    "    print labels\n",
    "    print \"Age_current\"\n",
    "    print age\n",
    "    \n",
    "    return images, labels, age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1023\n",
      "151\n",
      "2801\n",
      "1864\n",
      "1183\n",
      "1687\n",
      "260\n",
      "1922\n",
      "1745\n",
      "2045\n",
      "2411\n",
      "3274\n",
      "83\n",
      "1912\n",
      "1024\n",
      "2653\n",
      "2954\n",
      "3187\n",
      "2717\n",
      "938\n",
      "296\n",
      "1131\n",
      "3063\n",
      "3403\n",
      "1525\n",
      "1498\n",
      "1458\n",
      "804\n",
      "516\n",
      "450\n",
      "1246\n",
      "2758\n",
      "3282\n",
      "2423\n",
      "3316\n",
      "1722\n",
      "1930\n",
      "2835\n",
      "2263\n",
      "2431\n",
      "2602\n",
      "433\n",
      "1790\n",
      "482\n",
      "3250\n",
      "2910\n",
      "2354\n",
      "2325\n",
      "1094\n",
      "2446\n",
      "3027\n",
      "2128\n",
      "3016\n",
      "412\n",
      "427\n",
      "1882\n",
      "1874\n",
      "2604\n",
      "709\n",
      "4\n",
      "2903\n",
      "3389\n",
      "1985\n",
      "3339\n",
      "3340\n",
      "345\n",
      "664\n",
      "140\n",
      "2439\n",
      "1940\n",
      "277\n",
      "3424\n",
      "673\n",
      "1358\n",
      "1962\n",
      "512\n",
      "3373\n",
      "2388\n",
      "3400\n",
      "3032\n",
      "2657\n",
      "2028\n",
      "1433\n",
      "1213\n",
      "1771\n",
      "2387\n",
      "2929\n",
      "2540\n",
      "3057\n",
      "2157\n",
      "2748\n",
      "844\n",
      "834\n",
      "2789\n",
      "2867\n",
      "2168\n",
      "2249\n",
      "541\n",
      "2067\n",
      "53\n",
      "2991\n",
      "2137\n",
      "2526\n",
      "1574\n",
      "880\n",
      "2995\n",
      "1396\n",
      "2236\n",
      "2412\n",
      "1816\n",
      "2460\n",
      "704\n",
      "1953\n",
      "840\n",
      "3112\n",
      "953\n",
      "1756\n",
      "26\n",
      "328\n",
      "3308\n",
      "2269\n",
      "2545\n",
      "1806\n",
      "214\n",
      "663\n",
      "3370\n",
      "43\n",
      "1400\n",
      "666\n",
      "2480\n",
      "2170\n",
      "1852\n",
      "2169\n",
      "1505\n",
      "2456\n",
      "2352\n",
      "364\n",
      "1929\n",
      "3053\n",
      "1144\n",
      "2880\n",
      "2290\n",
      "2516\n",
      "2984\n",
      "3017\n",
      "538\n",
      "676\n",
      "1516\n",
      "1724\n",
      "1439\n",
      "3056\n",
      "2724\n",
      "394\n",
      "1450\n",
      "660\n",
      "3206\n",
      "1758\n",
      "680\n",
      "2251\n",
      "1744\n",
      "1827\n",
      "2590\n",
      "3252\n",
      "2827\n",
      "1028\n",
      "2727\n",
      "2609\n",
      "2030\n",
      "977\n",
      "3167\n",
      "1329\n",
      "838\n",
      "2646\n",
      "1014\n",
      "3058\n",
      "1964\n",
      "2049\n",
      "3009\n",
      "1123\n",
      "2227\n",
      "1061\n",
      "802\n",
      "3182\n",
      "796\n",
      "100\n",
      "2406\n",
      "3285\n",
      "337\n",
      "1888\n",
      "254\n",
      "3205\n",
      "1969\n",
      "2109\n",
      "643\n",
      "659\n",
      "2691\n",
      "1609\n",
      "951\n",
      "705\n",
      "1906\n",
      "2264\n",
      "609\n",
      "274\n",
      "2875\n",
      "1611\n",
      "1846\n",
      "803\n",
      "1022\n",
      "2435\n",
      "413\n",
      "1455\n",
      "2287\n",
      "2053\n",
      "2191\n",
      "2253\n",
      "1631\n",
      "2615\n",
      "3099\n",
      "687\n",
      "1149\n",
      "670\n",
      "2099\n",
      "897\n",
      "1685\n",
      "2661\n",
      "130\n",
      "385\n",
      "1207\n",
      "3365\n",
      "1670\n",
      "105\n",
      "1051\n",
      "1677\n",
      "1010\n",
      "2241\n",
      "1407\n",
      "418\n",
      "783\n",
      "81\n",
      "671\n",
      "3158\n",
      "1283\n",
      "470\n",
      "2614\n",
      "1618\n",
      "1416\n",
      "2126\n",
      "652\n",
      "460\n",
      "396\n",
      "3143\n",
      "1564\n",
      "2935\n",
      "237\n",
      "1657\n",
      "1026\n",
      "1343\n",
      "1665\n",
      "926\n",
      "1151\n",
      "2065\n",
      "3171\n",
      "1524\n",
      "59\n",
      "3278\n",
      "1271\n",
      "2704\n",
      "827\n",
      "954\n",
      "1733\n",
      "1858\n",
      "2649\n",
      "3108\n",
      "3330\n",
      "317\n",
      "644\n",
      "824\n",
      "1169\n",
      "3359\n",
      "1053\n",
      "3048\n",
      "30\n",
      "443\n",
      "437\n",
      "1355\n",
      "3169\n",
      "1514\n",
      "2034\n",
      "3003\n",
      "2560\n",
      "1114\n",
      "1871\n",
      "1202\n",
      "1625\n",
      "149\n",
      "2596\n",
      "2355\n",
      "2757\n",
      "2847\n",
      "2705\n",
      "3267\n",
      "1411\n",
      "1142\n",
      "1152\n",
      "980\n",
      "2542\n",
      "2583\n",
      "2777\n",
      "2250\n",
      "2893\n",
      "755\n",
      "1461\n",
      "3111\n",
      "1016\n",
      "1765\n",
      "3391\n",
      "367\n",
      "1472\n",
      "1049\n",
      "2606\n",
      "1728\n",
      "686\n",
      "749\n",
      "1604\n",
      "1190\n",
      "1086\n",
      "2574\n",
      "957\n",
      "1597\n",
      "1789\n",
      "1825\n",
      "2340\n",
      "2058\n",
      "1796\n",
      "910\n",
      "2317\n",
      "589\n",
      "447\n",
      "2083\n",
      "2366\n",
      "2444\n",
      "1981\n",
      "863\n",
      "2749\n",
      "2402\n",
      "2633\n",
      "198\n",
      "2568\n",
      "2530\n",
      "886\n",
      "90\n",
      "792\n",
      "2369\n",
      "25\n",
      "295\n",
      "1366\n",
      "3133\n",
      "2042\n",
      "1374\n",
      "1739\n",
      "1546\n",
      "15\n",
      "847\n",
      "2122\n",
      "135\n",
      "1346\n",
      "3131\n",
      "2711\n",
      "2493\n",
      "3280\n",
      "1487\n",
      "2424\n",
      "2077\n",
      "925\n",
      "2732\n",
      "339\n",
      "590\n",
      "1173\n",
      "1070\n",
      "2410\n",
      "994\n",
      "1549\n",
      "399\n",
      "1310\n",
      "964\n",
      "1711\n",
      "949\n",
      "332\n",
      "2663\n",
      "893\n",
      "1707\n",
      "2421\n",
      "2382\n",
      "3345\n",
      "1847\n",
      "854\n",
      "862\n",
      "268\n",
      "161\n",
      "1743\n",
      "610\n",
      "1354\n",
      "2703\n",
      "603\n",
      "685\n",
      "2794\n",
      "1132\n",
      "1857\n",
      "2494\n",
      "1205\n",
      "402\n",
      "2860\n",
      "1719\n",
      "2237\n",
      "152\n",
      "113\n",
      "3246\n",
      "693\n",
      "1992\n",
      "1537\n",
      "1309\n",
      "1085\n",
      "200\n",
      "2395\n",
      "3356\n",
      "3262\n",
      "2802\n",
      "2219\n",
      "2783\n",
      "1576\n",
      "2182\n",
      "2082\n",
      "1204\n",
      "2998\n",
      "2409\n",
      "167\n",
      "179\n",
      "2489\n",
      "1469\n",
      "3284\n",
      "1710\n",
      "1529\n",
      "930\n",
      "942\n",
      "2335\n",
      "1180\n",
      "1960\n",
      "1299\n",
      "242\n",
      "1496\n",
      "909\n",
      "2292\n",
      "1403\n",
      "2662\n",
      "1570\n",
      "2721\n",
      "104\n",
      "1420\n",
      "56\n",
      "1634\n",
      "832\n",
      "2745\n",
      "2200\n",
      "1561\n",
      "2161\n",
      "310\n",
      "2846\n",
      "2210\n",
      "6\n",
      "1077\n",
      "466\n",
      "2380\n",
      "3114\n",
      "561\n",
      "1315\n",
      "1785\n",
      "89\n",
      "1815\n",
      "581\n",
      "1999\n",
      "2713\n",
      "484\n",
      "234\n",
      "1127\n",
      "758\n",
      "3268\n",
      "1003\n",
      "3224\n",
      "3266\n",
      "884\n",
      "469\n",
      "1475\n",
      "3001\n",
      "2273\n",
      "2284\n",
      "118\n",
      "3253\n",
      "1020\n",
      "785\n",
      "2377\n",
      "77\n",
      "287\n",
      "539\n",
      "808\n",
      "305\n",
      "1359\n",
      "2999\n",
      "2683\n",
      "1364\n",
      "730\n",
      "188\n",
      "1256\n",
      "2628\n",
      "1983\n",
      "2514\n",
      "850\n",
      "1801\n",
      "2898\n",
      "3243\n",
      "924\n",
      "593\n",
      "369\n",
      "349\n",
      "476\n",
      "1378\n",
      "32\n",
      "1196\n",
      "1961\n",
      "741\n",
      "1046\n",
      "3326\n",
      "2525\n",
      "1540\n",
      "2872\n",
      "3180\n",
      "232\n",
      "3110\n",
      "3039\n",
      "1399\n",
      "597\n",
      "1988\n",
      "1681\n",
      "2368\n",
      "312\n",
      "141\n",
      "2104\n",
      "552\n",
      "301\n",
      "391\n",
      "3390\n",
      "2427\n",
      "672\n",
      "1395\n",
      "3081\n",
      "2688\n",
      "1755\n",
      "734\n",
      "3309\n",
      "937\n",
      "694\n",
      "3357\n",
      "64\n",
      "2429\n",
      "60\n",
      "3087\n",
      "1296\n",
      "3228\n",
      "1958\n",
      "1885\n",
      "3122\n",
      "2488\n",
      "357\n",
      "2329\n",
      "2949\n",
      "2551\n",
      "1178\n",
      "311\n",
      "2891\n",
      "2009\n",
      "3321\n",
      "3005\n",
      "3407\n",
      "1154\n",
      "3430\n",
      "2367\n",
      "1104\n",
      "3414\n",
      "1520\n",
      "2171\n",
      "189\n",
      "2930\n",
      "3007\n",
      "483\n",
      "965\n",
      "3404\n",
      "1748\n",
      "946\n",
      "1963\n",
      "879\n",
      "2928\n",
      "3210\n",
      "2971\n",
      "1688\n",
      "1915\n",
      "386\n",
      "1862\n",
      "828\n",
      "641\n",
      "280\n",
      "690\n",
      "3220\n",
      "426\n",
      "3002\n",
      "3088\n",
      "2985\n",
      "2501\n",
      "2149\n",
      "2091\n",
      "265\n",
      "2471\n",
      "80\n",
      "2638\n",
      "575\n",
      "2788\n",
      "1607\n",
      "299\n",
      "3151\n",
      "2320\n",
      "3166\n",
      "3103\n",
      "3223\n",
      "882\n",
      "2350\n",
      "3410\n",
      "2238\n",
      "2311\n",
      "3188\n",
      "2391\n",
      "501\n",
      "2858\n",
      "488\n",
      "1782\n",
      "2980\n",
      "795\n",
      "1214\n",
      "78\n",
      "1606\n",
      "1233\n",
      "3383\n",
      "789\n",
      "2682\n",
      "2572\n",
      "1006\n",
      "1062\n",
      "547\n",
      "17\n",
      "1232\n",
      "2953\n",
      "1805\n",
      "3334\n",
      "3028\n",
      "1431\n",
      "579\n",
      "2616\n",
      "1584\n",
      "3019\n",
      "1440\n",
      "1808\n",
      "1418\n",
      "1937\n",
      "1423\n",
      "1129\n",
      "2199\n",
      "2097\n",
      "2747\n",
      "604\n",
      "1380\n",
      "2630\n",
      "1654\n",
      "2665\n",
      "1394\n",
      "3192\n",
      "1494\n",
      "2843\n",
      "1799\n",
      "2839\n",
      "2415\n",
      "2474\n",
      "799\n",
      "1466\n",
      "1018\n",
      "1982\n",
      "611\n",
      "1500\n",
      "830\n",
      "298\n",
      "2268\n",
      "2373\n",
      "2074\n",
      "1704\n",
      "2027\n",
      "1339\n",
      "2838\n",
      "409\n",
      "3385\n",
      "3439\n",
      "700\n",
      "963\n",
      "1803\n",
      "1678\n",
      "1120\n",
      "1147\n",
      "2900\n",
      "88\n",
      "2632\n",
      "1686\n",
      "2522\n",
      "2267\n",
      "1406\n",
      "771\n",
      "2363\n",
      "940\n",
      "2805\n",
      "2831\n",
      "1872\n",
      "2902\n",
      "2925\n",
      "3304\n",
      "970\n",
      "210\n",
      "2326\n",
      "1792\n",
      "878\n",
      "1146\n",
      "158\n",
      "3045\n",
      "918\n",
      "2697\n",
      "1261\n",
      "226\n",
      "1488\n",
      "2882\n",
      "973\n",
      "1269\n",
      "2968\n",
      "2327\n",
      "14\n",
      "972\n",
      "837\n",
      "93\n",
      "655\n",
      "950\n",
      "1713\n",
      "1074\n",
      "1624\n",
      "1155\n",
      "2414\n",
      "2133\n",
      "628\n",
      "1794\n",
      "439\n",
      "2020\n",
      "1387\n",
      "2659\n",
      "2482\n",
      "2151\n",
      "2348\n",
      "2734\n",
      "2702\n",
      "1063\n",
      "2136\n",
      "1909\n",
      "3142\n",
      "1478\n",
      "271\n",
      "3030\n",
      "2917\n",
      "2810\n",
      "2586\n",
      "2624\n",
      "633\n",
      "1769\n",
      "1544\n",
      "1605\n",
      "725\n",
      "3212\n",
      "2591\n",
      "3098\n",
      "101\n",
      "3232\n",
      "325\n",
      "1612\n",
      "205\n",
      "1630\n",
      "2081\n",
      "1090\n",
      "124\n",
      "614\n",
      "178\n",
      "3271\n",
      "489\n",
      "1793\n",
      "2936\n",
      "2461\n",
      "2775\n",
      "714\n",
      "1602\n",
      "1083\n",
      "479\n",
      "2188\n",
      "2982\n",
      "448\n",
      "1780\n",
      "2166\n",
      "2613\n",
      "853\n",
      "929\n",
      "196\n",
      "1331\n",
      "1045\n",
      "2160\n",
      "601\n",
      "707\n",
      "1341\n",
      "2277\n",
      "2407\n",
      "2652\n",
      "1381\n",
      "2507\n",
      "3194\n",
      "2712\n",
      "3044\n",
      "770\n",
      "1684\n",
      "1979\n",
      "1980\n",
      "3091\n",
      "960\n",
      "2361\n",
      "2629\n",
      "3433\n",
      "1837\n",
      "735\n",
      "2762\n",
      "3269\n",
      "2372\n",
      "1658\n",
      "507\n",
      "1186\n",
      "504\n",
      "2102\n",
      "2228\n",
      "620\n",
      "2859\n",
      "2498\n",
      "1336\n",
      "3074\n",
      "718\n",
      "530\n",
      "449\n",
      "85\n",
      "944\n",
      "1200\n",
      "1787\n",
      "2322\n",
      "3033\n",
      "3303\n",
      "3438\n",
      "1467\n",
      "246\n",
      "898\n",
      "822\n",
      "3203\n",
      "251\n",
      "1641\n",
      "227\n",
      "2812\n",
      "1593\n",
      "3363\n",
      "2864\n",
      "3367\n",
      "3297\n",
      "2093\n",
      "2033\n",
      "2362\n",
      "3159\n",
      "377\n",
      "2792\n",
      "1772\n",
      "452\n",
      "1289\n",
      "781\n",
      "2729\n",
      "2798\n",
      "1700\n",
      "2239\n",
      "2965\n",
      "184\n",
      "1521\n",
      "3248\n",
      "3302\n",
      "1948\n",
      "1666\n",
      "3062\n",
      "662\n",
      "2519\n",
      "1585\n",
      "1036\n",
      "2265\n",
      "956\n",
      "416\n",
      "927\n",
      "1843\n",
      "2491\n",
      "1506\n",
      "2576\n",
      "1860\n",
      "2534\n",
      "3270\n",
      "145\n",
      "1084\n",
      "521\n",
      "1513\n",
      "2743\n",
      "1457\n",
      "1577\n",
      "1998\n",
      "1445\n",
      "3422\n",
      "2681\n",
      "1865\n",
      "3406\n",
      "2107\n",
      "1671\n",
      "511\n",
      "2776\n",
      "2057\n",
      "1422\n",
      "1947\n",
      "290\n",
      "3024\n",
      "1991\n",
      "753\n",
      "1321\n",
      "3289\n",
      "2532\n",
      "2026\n",
      "390\n",
      "848\n",
      "959\n",
      "2255\n",
      "860\n",
      "182\n",
      "2222\n",
      "2578\n",
      "2658\n",
      "3217\n",
      "2790\n",
      "1881\n",
      "1845\n",
      "2271\n",
      "2598\n",
      "110\n",
      "2746\n",
      "2256\n",
      "457\n",
      "98\n",
      "1103\n",
      "1350\n",
      "2962\n",
      "2001\n",
      "1697\n",
      "441\n",
      "899\n",
      "2845\n",
      "3035\n",
      "3418\n",
      "2856\n",
      "2319\n",
      "2612\n",
      "1669\n",
      "2654\n",
      "1870\n",
      "571\n",
      "1172\n",
      "1802\n",
      "2154\n",
      "1425\n",
      "1474\n",
      "1473\n",
      "2481\n",
      "2147\n",
      "1778\n",
      "293\n",
      "904\n",
      "2850\n",
      "2224\n",
      "1295\n",
      "524\n",
      "3320\n",
      "424\n",
      "503\n",
      "1599\n",
      "2641\n",
      "2855\n",
      "667\n",
      "3279\n",
      "1919\n",
      "18\n",
      "1486\n",
      "272\n",
      "2206\n",
      "451\n",
      "1861\n",
      "698\n",
      "2492\n",
      "346\n",
      "1241\n",
      "2862\n",
      "677\n",
      "187\n",
      "180\n",
      "3189\n",
      "2190\n",
      "1373\n",
      "58\n",
      "943\n",
      "1646\n",
      "3095\n",
      "1161\n",
      "2869\n",
      "3160\n",
      "33\n",
      "1819\n",
      "1884\n",
      "580\n",
      "826\n",
      "1553\n",
      "1091\n",
      "1198\n",
      "2770\n",
      "653\n",
      "1938\n",
      "3429\n",
      "2556\n",
      "1211\n",
      "2640\n",
      "2908\n",
      "1189\n",
      "2487\n",
      "708\n",
      "403\n",
      "1527\n",
      "975\n",
      "2346\n",
      "3234\n",
      "2201\n",
      "1532\n",
      "3147\n",
      "751\n",
      "342\n",
      "2671\n",
      "622\n",
      "1386\n",
      "2005\n",
      "3358\n",
      "1518\n",
      "2403\n",
      "3298\n",
      "2601\n",
      "2438\n",
      "2164\n",
      "3417\n",
      "3258\n",
      "2116\n",
      "3046\n",
      "2645\n",
      "2926\n",
      "454\n",
      "1972\n",
      "1017\n",
      "1248\n",
      "3190\n",
      "2211\n",
      "1898\n",
      "1823\n",
      "1810\n",
      "166\n",
      "2307\n",
      "2719\n",
      "1434\n",
      "2483\n",
      "1754\n",
      "3004\n",
      "1917\n",
      "1672\n",
      "746\n",
      "3021\n",
      "732\n",
      "1613\n",
      "1187\n",
      "253\n",
      "805\n",
      "2209\n",
      "2773\n",
      "2772\n",
      "3242\n",
      "2664\n",
      "2760\n",
      "2837\n",
      "1266\n",
      "1703\n",
      "849\n",
      "625\n",
      "3344\n",
      "1702\n",
      "2694\n",
      "39\n",
      "1863\n",
      "162\n",
      "1160\n",
      "2799\n",
      "91\n",
      "3006\n",
      "505\n",
      "2243\n",
      "2884\n",
      "1901\n",
      "421\n",
      "2376\n",
      "3077\n",
      "478\n",
      "1603\n",
      "284\n",
      "35\n",
      "419\n",
      "1221\n",
      "267\n",
      "2134\n",
      "1290\n",
      "2231\n",
      "156\n",
      "3351\n",
      "2610\n",
      "3050\n",
      "1327\n",
      "3239\n",
      "1727\n",
      "1501\n",
      "557\n",
      "1259\n",
      "438\n",
      "2141\n",
      "2090\n",
      "3440\n",
      "2687\n",
      "654\n",
      "2215\n",
      "2163\n",
      "1216\n",
      "1841\n",
      "1698\n",
      "2819\n",
      "1661\n",
      "2072\n",
      "1701\n",
      "468\n",
      "933\n",
      "2024\n",
      "1033\n",
      "2715\n",
      "3360\n",
      "366\n",
      "2257\n",
      "1563\n",
      "2150\n",
      "1575\n",
      "2360\n",
      "996\n",
      "133\n",
      "523\n",
      "1464\n",
      "2564\n",
      "422\n",
      "2270\n",
      "2744\n",
      "2535\n",
      "1891\n",
      "2617\n",
      "1651\n",
      "2143\n",
      "2396\n",
      "2035\n",
      "519\n",
      "352\n",
      "1281\n",
      "1921\n",
      "3168\n",
      "2544\n",
      "2195\n",
      "1124\n",
      "1058\n",
      "215\n",
      "1371\n",
      "1130\n",
      "3275\n",
      "3078\n",
      "245\n",
      "674\n",
      "2722\n",
      "3064\n",
      "2302\n",
      "2889\n",
      "3069\n",
      "2071\n",
      "902\n",
      "1878\n",
      "3272\n",
      "2017\n",
      "1887\n",
      "1627\n",
      "3202\n",
      "1867\n",
      "2623\n",
      "392\n",
      "2605\n",
      "1087\n",
      "31\n",
      "2952\n",
      "2518\n",
      "2112\n",
      "2937\n",
      "923\n",
      "3178\n",
      "577\n",
      "2401\n",
      "106\n",
      "2735\n",
      "793\n",
      "351\n",
      "2816\n",
      "2225\n",
      "1946\n",
      "177\n",
      "572\n",
      "498\n",
      "3428\n",
      "1623\n",
      "1856\n",
      "1401\n",
      "1976\n",
      "1229\n",
      "3176\n",
      "3162\n",
      "134\n",
      "981\n",
      "3441\n",
      "1320\n",
      "2742\n",
      "266\n",
      "279\n",
      "3426\n",
      "2695\n",
      "2515\n",
      "2873\n",
      "16\n",
      "1097\n",
      "1633\n",
      "2881\n",
      "1223\n",
      "2297\n",
      "440\n",
      "1950\n",
      "1715\n",
      "756\n",
      "2226\n",
      "111\n",
      "1030\n",
      "3299\n",
      "3307\n",
      "163\n",
      "1338\n",
      "1568\n",
      "2283\n",
      "1716\n",
      "1945\n",
      "1876\n",
      "3265\n",
      "1714\n",
      "1004\n",
      "2669\n",
      "405\n",
      "326\n",
      "2960\n",
      "2990\n",
      "2196\n",
      "3366\n",
      "869\n",
      "2865\n",
      "1951\n",
      "543\n",
      "1054\n",
      "696\n",
      "2510\n",
      "2218\n",
      "2699\n",
      "3395\n",
      "2120\n",
      "2138\n",
      "1145\n",
      "1990\n",
      "1652\n",
      "97\n",
      "3106\n",
      "703\n",
      "2585\n",
      "1143\n",
      "2447\n",
      "1708\n",
      "2240\n",
      "1820\n",
      "1809\n",
      "731\n",
      "492\n",
      "2370\n",
      "3362\n",
      "1511\n",
      "2558\n",
      "414\n",
      "809\n",
      "1230\n",
      "3413\n",
      "2592\n",
      "2328\n",
      "2725\n",
      "2964\n",
      "1089\n",
      "1986\n",
      "165\n",
      "2584\n",
      "701\n",
      "2969\n",
      "2528\n",
      "1368\n",
      "635\n",
      "219\n",
      "1676\n",
      "841\n",
      "1628\n",
      "1495\n",
      "1485\n",
      "3416\n",
      "2920\n",
      "2180\n",
      "1013\n",
      "3124\n",
      "594\n",
      "1268\n",
      "68\n",
      "2103\n",
      "2639\n",
      "1390\n",
      "1966\n",
      "1082\n",
      "2\n",
      "836\n",
      "2452\n",
      "675\n",
      "2769\n",
      "619\n",
      "935\n",
      "645\n",
      "3\n",
      "665\n",
      "1492\n",
      "1995\n",
      "559\n",
      "969\n",
      "1773\n",
      "820\n",
      "2155\n",
      "126\n",
      "3198\n",
      "3256\n",
      "1347\n",
      "28\n",
      "1907\n",
      "2321\n",
      "1034\n",
      "2087\n",
      "2036\n",
      "2158\n",
      "508\n",
      "1762\n",
      "1125\n",
      "2948\n",
      "2506\n",
      "3432\n",
      "733\n",
      "2666\n",
      "1555\n",
      "1545\n",
      "2118\n",
      "372\n",
      "373\n",
      "1427\n",
      "2479\n",
      "2343\n",
      "1250\n",
      "791\n",
      "1791\n",
      "2853\n",
      "2177\n",
      "2358\n",
      "1977\n",
      "388\n",
      "1194\n",
      "172\n",
      "1695\n",
      "2808\n",
      "1428\n",
      "249\n",
      "2184\n",
      "1997\n",
      "3040\n",
      "513\n",
      "3384\n",
      "3209\n",
      "3141\n",
      "962\n",
      "2279\n",
      "3354\n",
      "1637\n",
      "517\n",
      "1587\n",
      "3379\n",
      "2183\n",
      "2308\n",
      "2073\n",
      "2994\n",
      "2589\n",
      "1356\n",
      "1462\n",
      "1305\n",
      "1108\n",
      "1361\n",
      "798\n",
      "1774\n",
      "883\n",
      "231\n",
      "526\n",
      "3146\n",
      "1\n",
      "775\n",
      "875\n",
      "819\n",
      "273\n",
      "2357\n",
      "2153\n",
      "2349\n",
      "1851\n",
      "380\n",
      "759\n",
      "1994\n",
      "2779\n",
      "2418\n",
      "881\n",
      "2933\n",
      "96\n",
      "3306\n",
      "2413\n",
      "2010\n",
      "2672\n",
      "2700\n",
      "229\n",
      "3150\n",
      "3213\n",
      "459\n",
      "3068\n",
      "955\n",
      "201\n",
      "952\n",
      "1217\n",
      "3392\n",
      "491\n",
      "1463\n",
      "1199\n",
      "363\n",
      "2476\n",
      "3196\n",
      "2966\n",
      "555\n",
      "920\n",
      "3096\n",
      "2339\n",
      "2186\n",
      "2597\n",
      "1383\n",
      "175\n",
      "1509\n",
      "220\n",
      "1512\n",
      "3043\n",
      "407\n",
      "2807\n",
      "34\n",
      "2642\n",
      "496\n",
      "29\n",
      "627\n",
      "1828\n",
      "2129\n",
      "582\n",
      "2878\n",
      "2588\n",
      "65\n",
      "297\n",
      "2178\n",
      "629\n",
      "2043\n",
      "2398\n",
      "1064\n",
      "2041\n",
      "3427\n",
      "2635\n",
      "1235\n",
      "2392\n",
      "1119\n",
      "2197\n",
      "398\n",
      "1541\n",
      "1353\n",
      "2667\n",
      "1489\n",
      "300\n",
      "164\n",
      "3259\n",
      "2844\n",
      "2736\n",
      "1974\n",
      "1499\n",
      "2314\n",
      "865\n",
      "2417\n",
      "710\n",
      "2840\n",
      "1218\n",
      "435\n",
      "1523\n",
      "1000\n",
      "1592\n",
      "1471\n",
      "2874\n",
      "2095\n",
      "2038\n",
      "2504\n",
      "314\n",
      "1436\n",
      "1970\n",
      "2631\n",
      "218\n",
      "3175\n",
      "1504\n",
      "3118\n",
      "2379\n",
      "3125\n",
      "1548\n",
      "61\n",
      "971\n",
      "2660\n",
      "1318\n",
      "2064\n",
      "2100\n",
      "204\n",
      "3174\n",
      "2430\n",
      "729\n",
      "553\n",
      "1996\n",
      "1011\n",
      "3348\n",
      "3038\n",
      "2555\n",
      "717\n",
      "646\n",
      "3070\n",
      "1566\n",
      "22\n",
      "3060\n",
      "2025\n",
      "1071\n",
      "3200\n",
      "990\n",
      "817\n",
      "3377\n",
      "1032\n",
      "2686\n",
      "423\n",
      "948\n",
      "684\n",
      "383\n",
      "3029\n",
      "1786\n",
      "2821\n",
      "1279\n",
      "490\n",
      "3082\n",
      "1088\n",
      "2440\n",
      "2739\n",
      "2344\n",
      "1322\n",
      "3238\n",
      "3055\n",
      "2324\n",
      "612\n",
      "615\n",
      "45\n",
      "3105\n",
      "3139\n",
      "1832\n",
      "991\n",
      "1717\n",
      "228\n",
      "3059\n",
      "1015\n",
      "2678\n",
      "276\n",
      "2393\n",
      "1277\n",
      "1314\n",
      "2554\n",
      "2958\n",
      "537\n",
      "2569\n",
      "1617\n",
      "241\n",
      "2334\n",
      "1848\n",
      "203\n",
      "1109\n",
      "1517\n",
      "3436\n",
      "2566\n",
      "3312\n",
      "307\n",
      "780\n",
      "1263\n",
      "3000\n",
      "1357\n",
      "2252\n",
      "2078\n",
      "1663\n",
      "1726\n",
      "546\n",
      "1648\n",
      "807\n",
      "263\n",
      "1580\n",
      "3022\n",
      "275\n",
      "1247\n",
      "2086\n",
      "1783\n",
      "2716\n",
      "778\n",
      "748\n",
      "3281\n",
      "57\n",
      "1308\n",
      "216\n",
      "2242\n",
      "3034\n",
      "1121\n",
      "877\n",
      "1770\n",
      "381\n",
      "2768\n",
      "1643\n",
      "2940\n",
      "1935\n",
      "2848\n",
      "1251\n",
      "1814\n",
      "1610\n",
      "2524\n",
      "136\n",
      "814\n",
      "2976\n",
      "908\n",
      "2247\n",
      "1379\n",
      "788\n",
      "2125\n",
      "1621\n",
      "1459\n",
      "995\n",
      "1384\n",
      "2892\n",
      "936\n",
      "486\n",
      "852\n",
      "2679\n",
      "801\n",
      "2076\n",
      "2675\n",
      "1542\n",
      "2832\n",
      "1098\n",
      "429\n",
      "932\n",
      "2886\n",
      "2385\n",
      "444\n",
      "1415\n",
      "907\n",
      "502\n",
      "70\n",
      "1479\n",
      "1244\n",
      "3338\n",
      "2386\n",
      "436\n",
      "631\n",
      "1579\n",
      "23\n",
      "52\n",
      "979\n",
      "1853\n",
      "3420\n",
      "72\n",
      "1153\n",
      "5\n",
      "335\n",
      "358\n",
      "2156\n",
      "1644\n",
      "3148\n",
      "1737\n",
      "2720\n",
      "75\n",
      "535\n",
      "431\n",
      "191\n",
      "432\n",
      "806\n",
      "2851\n",
      "1526\n",
      "551\n",
      "626\n",
      "384\n",
      "2914\n",
      "1465\n",
      "1797\n",
      "3236\n",
      "1177\n",
      "2465\n",
      "3165\n",
      "1943\n",
      "230\n",
      "1238\n",
      "3152\n",
      "2553\n",
      "550\n",
      "288\n",
      "1304\n",
      "1507\n",
      "1552\n",
      "2814\n",
      "3072\n",
      "3123\n",
      "2472\n",
      "2420\n",
      "2947\n",
      "1141\n",
      "3119\n",
      "199\n",
      "42\n",
      "2245\n",
      "889\n",
      "712\n",
      "1165\n",
      "1781\n",
      "1777\n",
      "2397\n",
      "3318\n",
      "3260\n",
      "474\n",
      "1510\n",
      "3231\n",
      "2486\n",
      "2582\n",
      "1435\n",
      "587\n",
      "365\n",
      "1926\n",
      "3149\n",
      "3329\n",
      "520\n",
      "1740\n",
      "2750\n",
      "465\n",
      "1419\n",
      "222\n",
      "2135\n",
      "1933\n",
      "608\n",
      "1572\n",
      "2051\n",
      "256\n",
      "2866\n",
      "3443\n",
      "835\n",
      "1550\n",
      "2259\n",
      "815\n",
      "225\n",
      "2714\n",
      "2918\n",
      "1188\n",
      "1934\n",
      "1468\n",
      "1583\n",
      "1598\n",
      "379\n",
      "3117\n",
      "3369\n",
      "2098\n",
      "1968\n",
      "2608\n",
      "446\n",
      "737\n",
      "2932\n",
      "270\n",
      "1928\n",
      "2475\n",
      "1405\n",
      "2312\n",
      "259\n",
      "150\n",
      "600\n",
      "640\n",
      "1278\n",
      "2463\n",
      "3421\n",
      "334\n",
      "2451\n",
      "127\n",
      "1849\n",
      "194\n",
      "2130\n",
      "1168\n",
      "632\n",
      "2868\n",
      "1896\n",
      "1720\n",
      "154\n",
      "139\n",
      "2718\n",
      "2233\n",
      "1911\n",
      "2689\n",
      "845\n",
      "1567\n",
      "585\n",
      "1547\n",
      "340\n",
      "522\n",
      "3219\n",
      "82\n",
      "3191\n",
      "453\n",
      "2144\n",
      "308\n",
      "906\n",
      "3292\n",
      "404\n",
      "2993\n",
      "0\n",
      "2696\n",
      "2824\n",
      "941\n",
      "120\n",
      "1137\n",
      "3343\n",
      "2996\n",
      "1432\n",
      "728\n",
      "2364\n",
      "2105\n",
      "2330\n",
      "3402\n",
      "2924\n",
      "3216\n",
      "193\n",
      "782\n",
      "1365\n",
      "1731\n",
      "2089\n",
      "1041\n",
      "2419\n",
      "939\n",
      "3172\n",
      "2006\n",
      "3183\n",
      "3120\n",
      "1192\n",
      "2229\n",
      "3065\n",
      "1918\n",
      "1779\n",
      "1924\n",
      "2879\n",
      "3042\n",
      "1759\n",
      "1370\n",
      "2244\n",
      "2974\n",
      "1037\n",
      "864\n",
      "2621\n",
      "1534\n",
      "605\n",
      "2938\n",
      "1735\n",
      "350\n",
      "2670\n",
      "2913\n",
      "657\n",
      "3394\n",
      "1942\n",
      "2912\n",
      "574\n",
      "1766\n",
      "1316\n",
      "3137\n",
      "2804\n",
      "485\n",
      "207\n",
      "982\n",
      "1877\n",
      "238\n",
      "887\n",
      "1443\n",
      "1078\n",
      "1025\n",
      "2375\n",
      "1481\n",
      "1721\n",
      "2021\n",
      "1821\n",
      "86\n",
      "1404\n",
      "3437\n",
      "1729\n",
      "3138\n",
      "2571\n",
      "900\n",
      "3257\n",
      "3140\n",
      "695\n",
      "487\n",
      "347\n",
      "558\n",
      "533\n",
      "967\n",
      "155\n",
      "857\n",
      "2550\n",
      "1237\n",
      "1730\n",
      "1409\n",
      "2508\n",
      "1352\n",
      "1113\n",
      "2796\n",
      "3415\n",
      "3277\n",
      "1212\n",
      "3327\n",
      "2333\n",
      "1101\n",
      "3185\n",
      "2433\n",
      "258\n",
      "323\n",
      "330\n",
      "1181\n",
      "2175\n",
      "2959\n",
      "306\n",
      "3109\n",
      "2281\n",
      "2511\n",
      "2841\n",
      "1009\n",
      "329\n",
      "1272\n",
      "2306\n",
      "2185\n",
      "2336\n",
      "1761\n",
      "3324\n",
      "1265\n",
      "389\n",
      "103\n",
      "1389\n",
      "3399\n",
      "2008\n",
      "2817\n",
      "3154\n",
      "1879\n",
      "2448\n",
      "2541\n",
      "987\n",
      "79\n",
      "2347\n",
      "309\n",
      "2285\n",
      "3425\n",
      "3116\n",
      "1057\n",
      "774\n",
      "3061\n",
      "2916\n",
      "998\n",
      "2378\n",
      "1038\n",
      "901\n",
      "2806\n",
      "99\n",
      "1952\n",
      "1807\n",
      "2857\n",
      "784\n",
      "1642\n",
      "2342\n",
      "3396\n",
      "497\n",
      "616\n",
      "76\n",
      "3245\n",
      "190\n",
      "1210\n",
      "573\n",
      "984\n",
      "3157\n",
      "1530\n",
      "1040\n",
      "1191\n",
      "1219\n",
      "1302\n",
      "549\n",
      "3208\n",
      "3301\n",
      "291\n",
      "1307\n",
      "2973\n",
      "2823\n",
      "2676\n",
      "1335\n",
      "2282\n",
      "2110\n",
      "1264\n",
      "2356\n",
      "1449\n",
      "1615\n",
      "2004\n",
      "2484\n",
      "3305\n",
      "2221\n",
      "813\n",
      "1408\n",
      "794\n",
      "2023\n",
      "688\n",
      "911\n",
      "2883\n",
      "931\n",
      "2934\n",
      "773\n",
      "2923\n",
      "2538\n",
      "2296\n",
      "3382\n",
      "691\n",
      "1255\n",
      "1039\n",
      "2888\n",
      "1007\n",
      "2755\n",
      "1490\n",
      "1784\n",
      "1260\n",
      "2946\n",
      "1588\n",
      "1348\n",
      "173\n",
      "1201\n",
      "2763\n",
      "1170\n",
      "1311\n",
      "661\n",
      "818\n",
      "3347\n",
      "2627\n",
      "2441\n",
      "1904\n",
      "3079\n",
      "1236\n",
      "1306\n",
      "2599\n",
      "3353\n",
      "3349\n",
      "3229\n",
      "772\n",
      "71\n",
      "3325\n",
      "1117\n",
      "2469\n",
      "2223\n",
      "2943\n",
      "669\n",
      "1227\n",
      "1866\n",
      "3218\n",
      "1092\n",
      "2536\n",
      "1812\n",
      "905\n",
      "240\n",
      "2323\n",
      "336\n",
      "2124\n",
      "2272\n",
      "2803\n",
      "2987\n",
      "839\n",
      "1050\n",
      "1252\n",
      "1939\n",
      "2318\n",
      "2756\n",
      "1288\n",
      "195\n",
      "2467\n",
      "2258\n",
      "656\n",
      "1679\n",
      "1535\n",
      "1073\n",
      "2338\n",
      "2710\n",
      "442\n",
      "2559\n",
      "2119\n",
      "2563\n",
      "1367\n",
      "2455\n",
      "1224\n",
      "360\n",
      "73\n",
      "1824\n",
      "2733\n",
      "833\n",
      "19\n",
      "3355\n",
      "1868\n",
      "2651\n",
      "2505\n",
      "1243\n",
      "1291\n",
      "716\n",
      "1573\n",
      "3397\n",
      "2509\n",
      "1239\n",
      "2818\n",
      "556\n",
      "1586\n",
      "1927\n",
      "87\n",
      "458\n",
      "1209\n",
      "658\n",
      "159\n",
      "69\n",
      "1920\n",
      "895\n",
      "1441\n",
      "2967\n",
      "3181\n",
      "2096\n",
      "425\n",
      "324\n",
      "2587\n",
      "475\n",
      "1095\n",
      "697\n",
      "1905\n",
      "2706\n",
      "1640\n",
      "583\n",
      "624\n",
      "3121\n",
      "2056\n",
      "1430\n",
      "1589\n",
      "1328\n",
      "2123\n",
      "3405\n",
      "1270\n",
      "1554\n",
      "1081\n",
      "2895\n",
      "142\n",
      "3434\n",
      "1317\n",
      "2018\n",
      "1372\n",
      "1562\n",
      "2436\n",
      "1245\n",
      "1273\n",
      "102\n",
      "2945\n",
      "1176\n",
      "1225\n",
      "2548\n",
      "494\n",
      "2901\n",
      "506\n",
      "1068\n",
      "3128\n",
      "3214\n",
      "3411\n",
      "1162\n",
      "1626\n",
      "338\n",
      "2979\n",
      "2054\n",
      "1889\n",
      "2294\n",
      "313\n",
      "3129\n",
      "2552\n",
      "2766\n",
      "1850\n",
      "2970\n",
      "47\n",
      "569\n",
      "2371\n",
      "855\n",
      "244\n",
      "945\n",
      "1732\n",
      "985\n",
      "420\n",
      "1636\n",
      "885\n",
      "743\n",
      "2094\n",
      "3361\n",
      "2981\n",
      "2039\n",
      "1222\n",
      "331\n",
      "95\n",
      "2040\n",
      "989\n",
      "642\n",
      "1106\n",
      "2906\n",
      "2115\n",
      "1984\n",
      "1377\n",
      "1519\n",
      "1869\n",
      "3100\n",
      "1453\n",
      "2547\n",
      "1375\n",
      "983\n",
      "456\n",
      "2146\n",
      "170\n",
      "1502\n",
      "224\n",
      "2416\n",
      "1941\n",
      "1019\n",
      "1699\n",
      "2179\n",
      "1559\n",
      "1332\n",
      "2520\n",
      "1656\n",
      "1897\n",
      "1650\n",
      "2450\n",
      "292\n",
      "400\n",
      "1042\n",
      "1667\n",
      "2434\n",
      "1345\n",
      "3026\n",
      "1240\n",
      "1166\n",
      "221\n",
      "2443\n",
      "13\n",
      "3094\n",
      "2593\n",
      "3145\n",
      "2477\n",
      "1150\n",
      "2466\n",
      "3322\n",
      "1414\n",
      "859\n",
      "74\n",
      "1842\n",
      "247\n",
      "197\n",
      "2060\n",
      "1451\n",
      "2470\n",
      "3084\n",
      "223\n",
      "1159\n",
      "843\n",
      "2044\n",
      "1319\n",
      "750\n",
      "790\n",
      "3240\n",
      "682\n",
      "2956\n",
      "2063\n",
      "2784\n",
      "564\n",
      "3310\n",
      "3233\n",
      "1775\n",
      "2374\n",
      "320\n",
      "3264\n",
      "1044\n",
      "2059\n",
      "2915\n",
      "2896\n",
      "3393\n",
      "3328\n",
      "2765\n",
      "2070\n",
      "1226\n",
      "2842\n",
      "3177\n",
      "2922\n",
      "1100\n",
      "2637\n",
      "576\n",
      "3011\n",
      "1959\n",
      "362\n",
      "1840\n",
      "411\n",
      "410\n",
      "719\n",
      "3254\n",
      "2061\n",
      "1503\n",
      "3251\n",
      "2951\n",
      "2600\n",
      "621\n",
      "333\n",
      "2741\n",
      "2726\n",
      "1515\n",
      "3372\n",
      "2941\n",
      "1253\n",
      "148\n",
      "2011\n",
      "40\n",
      "3193\n",
      "2899\n",
      "1539\n",
      "2919\n",
      "2833\n",
      "269\n",
      "873\n",
      "856\n",
      "606\n",
      "2503\n",
      "1282\n",
      "3215\n",
      "108\n",
      "3435\n",
      "2468\n",
      "2780\n",
      "842\n",
      "723\n",
      "1157\n",
      "757\n",
      "2680\n",
      "2549\n",
      "2707\n",
      "3179\n",
      "613\n",
      "3241\n",
      "1595\n",
      "3364\n",
      "3227\n",
      "1931\n",
      "3290\n",
      "3080\n",
      "169\n",
      "2495\n",
      "2066\n",
      "1330\n",
      "112\n",
      "3014\n",
      "2963\n",
      "1875\n",
      "2069\n",
      "1096\n",
      "2080\n",
      "3101\n",
      "997\n",
      "591\n",
      "722\n",
      "1894\n",
      "185\n",
      "1385\n",
      "1482\n",
      "1287\n",
      "147\n",
      "1967\n",
      "3036\n",
      "137\n",
      "294\n",
      "1776\n",
      "2449\n",
      "1855\n",
      "874\n",
      "1538\n",
      "2162\n",
      "1122\n",
      "1725\n",
      "2822\n",
      "650\n",
      "3423\n",
      "699\n",
      "3207\n",
      "2176\n",
      "428\n",
      "578\n",
      "2075\n",
      "567\n",
      "355\n",
      "3408\n",
      "3156\n",
      "2310\n",
      "1484\n",
      "1140\n",
      "3442\n",
      "738\n",
      "914\n",
      "3300\n",
      "2521\n",
      "2351\n",
      "3332\n",
      "531\n",
      "1629\n",
      "1182\n",
      "3387\n",
      "986\n",
      "2108\n",
      "1712\n",
      "1908\n",
      "851\n",
      "2820\n",
      "1274\n",
      "1949\n",
      "192\n",
      "3008\n",
      "1639\n",
      "1600\n",
      "1079\n",
      "1558\n",
      "2648\n",
      "1334\n",
      "3076\n",
      "2767\n",
      "3378\n",
      "568\n",
      "48\n",
      "2885\n",
      "2298\n",
      "2000\n",
      "3346\n",
      "2496\n",
      "2454\n",
      "1452\n",
      "283\n",
      "393\n",
      "570\n",
      "992\n",
      "3153\n",
      "1031\n",
      "2643\n",
      "2955\n",
      "915\n",
      "1292\n",
      "1916\n",
      "3135\n",
      "649\n",
      "3115\n",
      "2173\n",
      "27\n",
      "55\n",
      "1483\n",
      "2299\n",
      "702\n",
      "509\n",
      "1080\n",
      "2207\n",
      "3127\n",
      "235\n",
      "1655\n",
      "528\n",
      "1649\n",
      "2849\n",
      "870\n",
      "278\n",
      "370\n",
      "2581\n",
      "1413\n",
      "1293\n",
      "2159\n",
      "1902\n",
      "1581\n",
      "2813\n",
      "1973\n",
      "1393\n",
      "2634\n",
      "3012\n",
      "1298\n",
      "683\n",
      "1753\n",
      "3071\n",
      "415\n",
      "1284\n",
      "739\n",
      "1543\n",
      "3023\n",
      "595\n",
      "3263\n",
      "2216\n",
      "1021\n",
      "1831\n",
      "2957\n",
      "1594\n",
      "2782\n",
      "681\n",
      "762\n",
      "2786\n",
      "2944\n",
      "1179\n",
      "1913\n",
      "745\n",
      "3211\n",
      "2997\n",
      "1746\n",
      "1280\n",
      "1052\n",
      "202\n",
      "2939\n",
      "2029\n",
      "2622\n",
      "1056\n",
      "1682\n",
      "744\n",
      "3134\n",
      "lables\n",
      "[1023, 151, 2801, 1864, 1183, 1687, 260, 1922, 1745, 2045, 2411, 3274, 83, 1912, 1024, 1024, 2653, 2653, 2954, 3187, 2717, 938, 296, 296, 1131, 3063, 3403, 1525, 1458, 804, 516, 450, 1246, 2758, 3282, 2423, 3316, 1722, 1930, 2835, 2263, 2431, 2602, 433, 1790, 482, 482, 3250, 2910, 2354, 2325, 1094, 1094, 1094, 2446, 3027, 2128, 2128, 3016, 412, 1882, 1874, 2604, 709, 4, 2903, 2903, 3389, 3389, 1985, 3339, 345, 140, 2439, 1940, 277, 3424, 673, 1358, 1962, 3400, 3032, 2657, 2028, 1433, 1213, 1213, 1771, 2387, 2929, 2929, 2540, 3057, 2157, 2748, 844, 834, 2789, 2867, 2168, 2168, 2249, 541, 2067, 2067, 53, 53, 2137, 2526, 2526, 1574, 880, 2995, 1396, 2236, 2412, 1816, 2460, 704, 1953, 840, 3112, 953, 1756, 26, 328, 3308, 2269, 2545, 2545, 2545, 1806, 214, 663, 3370, 43, 43, 1400, 666, 2480, 2170, 1852, 2169, 1505, 2456, 2456, 2352, 364, 1929, 3053, 1144, 2880, 2880, 2290, 2516, 2984, 3017, 538, 676, 1516, 1724, 1439, 3056, 3056, 2724, 2724, 394, 1450, 3206, 1758, 1758, 1758, 680, 2251, 1744, 1827, 2590, 3252, 1028, 2727, 2609, 2030, 977, 3167, 1329, 838, 2646, 1014, 3058, 1964, 2049, 1123, 2227, 1061, 802, 3182, 796, 100, 100, 2406, 2406, 3285, 337, 1888, 254, 3205, 1969, 2109, 643, 659, 2691, 1609, 951, 705, 1906, 1906, 2264, 609, 274, 2875, 2875, 1611, 1846, 1846, 803, 1022, 2435, 413, 1455, 2287, 2053, 2191, 2191, 2253, 1631, 2615, 3099, 687, 1149, 670, 897, 1685, 2661, 2661, 130, 385, 385, 1207, 3365, 1670, 105, 1051, 1677, 1010, 2241, 1407, 418, 418, 783, 81, 81, 81, 671, 3158, 3158, 1283, 470, 2614, 1618, 2126, 652, 460, 396, 3143, 1564, 2935, 237, 1657, 1026, 1343, 1665, 926, 926, 1151, 2065, 2065, 3171, 1524, 59, 3278, 1271, 2704, 827, 827, 954, 954, 1733, 1858, 1858, 1858, 2649, 3108, 3330, 317, 644, 824, 1169, 3359, 1053, 3048, 30, 443, 443, 437, 1355, 3169, 1514, 2034, 3003, 1114, 1871, 1871, 1202, 1625, 1625, 149, 2596, 2355, 2847, 2705, 3267, 1411, 1142, 1152, 980, 980, 2542, 2542, 2583, 2777, 2250, 2893, 755, 755, 1461, 3111, 1016, 1765, 3391, 367, 1472, 1049, 2606, 2606, 1728, 686, 749, 1604, 1086, 2574, 957, 1597, 1789, 1825, 1825, 2340, 2058, 910, 2317, 589, 447, 2083, 2444, 1981, 863, 2749, 2402, 2633, 198, 2568, 2530, 2530, 2530, 2530, 886, 90, 792, 2369, 2369, 2369, 25, 295, 1366, 3133, 2042, 2042, 1739, 1546, 1546, 15, 847, 2122, 135, 1346, 3131, 2711, 2493, 2493, 1487, 1487, 1487, 2424, 2077, 925, 2732, 339, 339, 590, 590, 590, 1173, 1070, 2410, 994, 1549, 399, 399, 399, 399, 1310, 1310, 964, 1711, 949, 332, 2663, 893, 893, 1707, 1707, 2421, 2421, 2382, 3345, 3345, 1847, 854, 862, 268, 161, 1743, 610, 1354, 1354, 2703, 2703, 2703, 603, 603, 685, 2794, 2794, 1132, 1132, 1857, 2494, 1205, 402, 2860, 2860, 2860, 1719, 2237, 152, 113, 3246, 693, 693, 1992, 1537, 1309, 1085, 200, 2395, 3356, 3262, 2802, 2219, 2783, 1576, 2082, 2082, 1204, 2998, 2409, 2409, 167, 179, 2489, 2489, 1469, 3284, 1710, 1710, 1529, 930, 942, 2335, 1180, 1960, 1299, 1496, 2292, 1403, 2662, 1570, 1570, 104, 104, 1420, 1634, 832, 2745, 2200, 1561, 2161, 2161, 310, 2846, 2210, 6, 1077, 466, 3114, 3114, 561, 561, 1315, 1785, 89, 1815, 1815, 1815, 581, 1999, 2713, 484, 234, 1127, 1127, 758, 758, 3268, 3268, 1003, 3224, 3224, 3224, 3224, 3224, 884, 469, 1475, 1475, 1475, 3001, 2273, 2284, 118, 3253, 3253, 1020, 1020, 785, 2377, 77, 287, 539, 808, 305, 2999, 2683, 1364, 730, 188, 1256, 2628, 1983, 1983, 2514, 850, 1801, 2898, 3243, 3243, 924, 593, 369, 349, 1378, 32, 1196, 1196, 1961, 741, 741, 1046, 1046, 3326, 2525, 2525, 1540, 1540, 1540, 2872, 2872, 3180, 232, 3110, 3039, 1399, 1399, 597, 597, 1988, 1681, 2368, 312, 141, 552, 301, 391, 3390, 2427, 672, 1395, 3081, 2688, 1755, 1755, 734, 734, 3309, 937, 694, 3357, 64, 2429, 60, 3087, 1296, 1296, 1296, 3228, 1958, 1958, 1885, 1885, 3122, 2488, 357, 2329, 2949, 2551, 1178, 311, 311, 2891, 2009, 3321, 3005, 3005, 1154, 3430, 2367, 2367, 2367, 1104, 3414, 3414, 1520, 1520, 2171, 189, 2930, 3007, 483, 965, 3404, 1748, 946, 946, 1963, 879, 2928, 3210, 2971, 1688, 1915, 386, 1862, 828, 641, 280, 3220, 3002, 3088, 2985, 2501, 2149, 2091, 265, 265, 2471, 80, 2638, 2638, 575, 2788, 1607, 299, 3151, 2320, 2320, 2320, 3166, 3103, 3103, 3223, 3223, 882, 2350, 2238, 2311, 3188, 2391, 2391, 2391, 2391, 501, 2858, 488, 1782, 2980, 795, 1214, 78, 1606, 1606, 1233, 3383, 3383, 789, 789, 2572, 1006, 1062, 547, 17, 1232, 2953, 1805, 3334, 3028, 1431, 579, 2616, 1584, 3019, 1440, 1808, 1808, 1418, 1937, 1423, 1129, 2199, 2097, 2747, 1380, 2630, 1654, 2665, 1394, 3192, 1494, 2843, 1799, 2839, 2474, 2474, 799, 1466, 1466, 1466, 1018, 1982, 1982, 1982, 611, 1500, 830, 298, 2268, 2373, 1704, 2027, 1339, 2838, 2838, 409, 3385, 3439, 700, 963, 1803, 1678, 1120, 1120, 1147, 1147, 2900, 88, 2632, 2632, 1686, 2522, 2522, 2267, 1406, 1406, 771, 2363, 940, 2805, 2831, 1872, 2902, 2902, 2925, 3304, 970, 210, 210, 2326, 2326, 1792, 1792, 1792, 878, 1146, 3045, 918, 1261, 1261, 226, 1488, 2882, 2882, 2882, 973, 1269, 2968, 2327, 14, 972, 837, 837, 655, 950, 1713, 1074, 1624, 1624, 1155, 1155, 2414, 2133, 628, 1794, 439, 2020, 1387, 2659, 2482, 2482, 2151, 2348, 2348, 2734, 2702, 1063, 2136, 1909, 3142, 3142, 3142, 1478, 271, 3030, 2917, 2810, 2586, 2624, 2624, 633, 633, 1769, 1544, 1605, 725, 3212, 3212, 2591, 3098, 101, 3232, 325, 1612, 1612, 205, 205, 1630, 1630, 2081, 1090, 124, 614, 178, 3271, 489, 1793, 1793, 2461, 2775, 714, 479, 479, 2188, 2982, 448, 448, 1780, 2166, 2613, 2613, 853, 929, 196, 196, 1331, 1045, 2160, 707, 1341, 2277, 2407, 2652, 1381, 2507, 3194, 2712, 2712, 3044, 770, 770, 1684, 1979, 1980, 3091, 960, 960, 2361, 2629, 3433, 1837, 735, 2762, 3269, 1658, 1658, 507, 1186, 504, 2102, 2228, 620, 2498, 1336, 3074, 3074, 718, 530, 449, 449, 85, 944, 1200, 1787, 2322, 2322, 3033, 3303, 3438, 3438, 1467, 898, 3203, 251, 1641, 227, 2812, 1593, 3363, 3363, 2864, 3367, 3367, 3367, 3297, 2093, 2033, 2362, 3159, 377, 377, 2792, 2792, 1772, 452, 1289, 781, 2729, 2729, 2729, 2798, 2798, 2239, 2965, 184, 1521, 3248, 3302, 1948, 1666, 3062, 662, 2519, 1585, 1036, 2265, 2265, 2265, 956, 416, 927, 1843, 1843, 2491, 2491, 1506, 2576, 1860, 2534, 3270, 145, 1084, 521, 1513, 1513, 2743, 1457, 1577, 1998, 1445, 3422, 2681, 2681, 1865, 1865, 3406, 2107, 1671, 511, 2776, 2057, 2057, 2057, 1422, 1422, 1947, 1947, 290, 290, 3024, 1991, 753, 1321, 3289, 3289, 3289, 2532, 2026, 390, 848, 959, 2255, 860, 2222, 2578, 2658, 3217, 3217, 2790, 1881, 1881, 1881, 1881, 1881, 1881, 1845, 2271, 2598, 2598, 110, 110, 2746, 2256, 457, 98, 98, 98, 1103, 1350, 2962, 2001, 1697, 441, 899, 2845, 3035, 3418, 2856, 2856, 2319, 2319, 2612, 1669, 2654, 1870, 571, 1172, 1172, 1802, 2154, 1425, 1474, 1474, 1474, 1473, 2481, 2147, 1778, 293, 904, 2850, 2224, 1295, 524, 3320, 3320, 424, 503, 1599, 2641, 2855, 2855, 667, 667, 667, 667, 3279, 3279, 1919, 18, 1486, 1486, 272, 2206, 451, 1861, 1861, 698, 698, 2492, 2492, 346, 1241, 2862, 677, 187, 187, 180, 3189, 2190, 1373, 58, 943, 1646, 1646, 3095, 1161, 2869, 3160, 33, 1819, 1884, 1884, 580, 826, 826, 826, 1553, 1553, 1091, 1198, 2770, 653, 1938, 3429, 3429, 3429, 1211, 2640, 2640, 2908, 2908, 1189, 2487, 708, 1527, 975, 2346, 3234, 2201, 1532, 3147, 751, 751, 342, 2671, 2671, 2671, 622, 1386, 2005, 3358, 1518, 2403, 3298, 3298, 3298, 2601, 2438, 3417, 3258, 3258, 2116, 3046, 3046, 2645, 2926, 1972, 1017, 1248, 3190, 2211, 2211, 1898, 1823, 1823, 1810, 166, 2307, 1434, 2483, 2483, 1754, 1754, 3004, 1917, 1672, 746, 3021, 732, 1613, 1187, 253, 805, 2209, 2773, 2772, 3242, 2664, 2760, 2760, 2837, 1266, 1703, 849, 625, 3344, 1702, 2694, 2694, 39, 1863, 162, 1160, 2799, 91, 91, 3006, 505, 2243, 2243, 2884, 1901, 1901, 421, 2376, 3077, 478, 1603, 1603, 284, 35, 419, 1221, 1221, 267, 267, 2134, 1290, 2231, 3351, 2610, 3050, 1327, 1327, 3239, 1727, 1501, 1501, 557, 1259, 438, 2141, 2090, 3440, 2687, 654, 2215, 2163, 1216, 1841, 1698, 2819, 1661, 2072, 2072, 1701, 468, 933, 2024, 2024, 1033, 2715, 3360, 366, 2257, 2150, 1575, 2360, 996, 133, 523, 1464, 2564, 422, 2270, 2270, 2744, 2535, 1891, 2617, 1651, 2143, 2396, 2035, 519, 352, 1281, 1921, 3168, 2544, 2195, 1124, 1124, 1058, 1058, 215, 1371, 1130, 3275, 3078, 245, 674, 2722, 3064, 2302, 2889, 3069, 2071, 902, 1878, 2017, 1887, 1627, 1627, 3202, 1867, 2623, 392, 2605, 1087, 31, 31, 2952, 2518, 2112, 2937, 923, 3178, 577, 2401, 2401, 106, 2735, 2735, 793, 793, 351, 2816, 2816, 2225, 1946, 177, 572, 572, 498, 3428, 1623, 1623, 1856, 1856, 1401, 1976, 1229, 3176, 3162, 134, 981, 3441, 1320, 2742, 279, 3426, 2695, 2515, 2873, 16, 1097, 1633, 2881, 1223, 2297, 440, 1950, 1715, 756, 756, 2226, 2226, 111, 1030, 3299, 3307, 163, 163, 1338, 1338, 1568, 2283, 1716, 1945, 3265, 1714, 1004, 2669, 405, 326, 2960, 2990, 869, 2865, 1951, 1054, 696, 696, 696, 2510, 2510, 2218, 2699, 3395, 2120, 2120, 2138, 2138, 2138, 1145, 1990, 1652, 97, 3106, 703, 2585, 1143, 1143, 2447, 1708, 2240, 1820, 1809, 731, 492, 2370, 2370, 2370, 3362, 1511, 2558, 414, 809, 1230, 1230, 3413, 2592, 2328, 2328, 2725, 2964, 1089, 1986, 165, 2584, 2969, 2528, 1368, 635, 219, 1676, 841, 1628, 1495, 1485, 3416, 2920, 2180, 1013, 3124, 594, 1268, 68, 2103, 2639, 1390, 1966, 1082, 2, 836, 2452, 675, 2769, 619, 935, 645, 3, 3, 665, 1492, 1995, 1995, 559, 969, 969, 1773, 820, 2155, 126, 3198, 3198, 3256, 1347, 28, 28, 1907, 2321, 1034, 2087, 2036, 2036, 2158, 2158, 508, 1762, 1125, 2948, 2506, 2506, 3432, 733, 2666, 1555, 1545, 1545, 2118, 2118, 372, 373, 1427, 2479, 2343, 1250, 1250, 1250, 791, 791, 791, 791, 1791, 2853, 2177, 2358, 1977, 388, 1194, 172, 1695, 2808, 2808, 1428, 249, 2184, 1997, 3040, 513, 3384, 3209, 3141, 962, 2279, 3354, 3354, 1637, 517, 1587, 3379, 2183, 2308, 2073, 2994, 2994, 2589, 2589, 1356, 1462, 1305, 1305, 1108, 1108, 1361, 798, 798, 1774, 1774, 883, 231, 526, 3146, 1, 775, 875, 819, 273, 2357, 2153, 2153, 2153, 2153, 2349, 2349, 1851, 380, 759, 1994, 2779, 2418, 881, 2933, 2933, 2933, 96, 3306, 2413, 2010, 2672, 2700, 229, 3150, 3213, 3213, 459, 3068, 955, 201, 952, 1217, 1217, 3392, 491, 1463, 1463, 1199, 363, 363, 2476, 2476, 2966, 555, 920, 3096, 2339, 2186, 2597, 1383, 1383, 175, 1509, 220, 1512, 3043, 407, 2807, 34, 2642, 29, 627, 627, 1828, 2129, 2129, 582, 2878, 2588, 2588, 65, 297, 297, 2178, 2043, 2398, 2041, 2635, 1235, 2392, 1119, 1119, 2197, 398, 1541, 1353, 1489, 300, 3259, 3259, 2844, 2736, 1974, 1499, 2314, 865, 865, 2417, 2417, 710, 1218, 435, 1523, 1000, 1000, 1592, 1471, 2874, 2095, 2038, 2504, 314, 1436, 1970, 2631, 218, 3175, 1504, 3118, 3118, 3125, 1548, 61, 971, 971, 2660, 1318, 2064, 2100, 204, 3174, 2430, 729, 553, 1011, 1011, 3348, 3038, 646, 3070, 1566, 22, 22, 3060, 2025, 1071, 3200, 990, 817, 3377, 1032, 2686, 948, 684, 383, 3029, 3029, 1786, 2821, 1279, 490, 3082, 1088, 2440, 2739, 2344, 1322, 3238, 3055, 2324, 612, 615, 45, 3105, 3105, 3139, 1832, 991, 1717, 228, 3059, 3059, 1015, 2678, 276, 2393, 1277, 2554, 2958, 2569, 2569, 2569, 1617, 2334, 1848, 203, 1109, 1517, 1517, 3436, 2566, 3312, 307, 780, 780, 1263, 3000, 3000, 1357, 2252, 2252, 2078, 1663, 1726, 546, 1648, 807, 807, 263, 263, 1580, 1580, 3022, 275, 1247, 2086, 1783, 2716, 778, 748, 3281, 57, 57, 1308, 216, 216, 2242, 3034, 1121, 877, 381, 2768, 1643, 2940, 1935, 2848, 1251, 1814, 1610, 1610, 2524, 136, 814, 2976, 908, 2247, 1379, 788, 2125, 2125, 1621, 1459, 995, 995, 1384, 1384, 936, 486, 852, 2679, 801, 2076, 2675, 2832, 1098, 429, 932, 932, 2886, 2385, 444, 1415, 907, 502, 70, 1479, 1244, 3338, 2386, 436, 631, 1579, 23, 23, 52, 1853, 3420, 3420, 72, 72, 1153, 1153, 5, 335, 358, 2156, 1644, 3148, 1737, 2720, 75, 75, 535, 431, 191, 432, 806, 2851, 1526, 551, 551, 551, 551, 626, 384, 2914, 1465, 1797, 3236, 1177, 2465, 3165, 1943, 230, 230, 1238, 3152, 2553, 550, 288, 1507, 1552, 2814, 3072, 3123, 2472, 2420, 2420, 2947, 2947, 1141, 3119, 199, 42, 2245, 889, 889, 712, 712, 1165, 1781, 1777, 1777, 2397, 3318, 3260, 3260, 474, 1510, 3231, 2486, 2582, 2582, 1435, 587, 365, 365, 1926, 3149, 3329, 3329, 520, 1740, 2750, 465, 1419, 222, 2135, 1933, 608, 1572, 2051, 256, 2866, 2866, 3443, 835, 1550, 2259, 815, 815, 815, 225, 2714, 2714, 2918, 1188, 1934, 1468, 1583, 1598, 379, 3117, 3369, 2098, 1968, 1968, 2608, 446, 446, 737, 2932, 270, 1928, 2475, 1405, 2312, 2312, 2312, 259, 150, 600, 640, 2463, 2463, 2463, 334, 2451, 127, 1849, 194, 2130, 1168, 1168, 632, 632, 632, 2868, 1896, 1720, 154, 139, 2718, 2233, 1911, 1911, 2689, 845, 1567, 585, 1547, 340, 522, 3219, 3219, 82, 3191, 453, 2144, 308, 3292, 3292, 404, 2993, 0, 0, 2824, 941, 120, 1137, 3343, 2996, 2996, 1432, 728, 2364, 2105, 2105, 2330, 2924, 3216, 782, 782, 1365, 2089, 1041, 2419, 939, 939, 3172, 2006, 3183, 3120, 1192, 2229, 3065, 3065, 1918, 1779, 1924, 1924, 2879, 3042, 1759, 1370, 2244, 2974, 1037, 864, 2621, 1534, 1534, 605, 2938, 1735, 1735, 1735, 350, 2670, 2913, 657, 3394, 1942, 2912, 574, 1766, 1316, 3137, 2804, 2804, 485, 207, 982, 1877, 238, 887, 1443, 1443, 1078, 1025, 2375, 1481, 1721, 1821, 86, 1404, 3437, 3437, 1729, 1729, 3138, 900, 900, 3257, 3140, 695, 487, 558, 533, 967, 155, 155, 857, 2550, 2550, 1237, 1237, 1730, 1409, 2508, 1352, 1113, 1113, 2796, 3277, 1212, 3327, 2333, 2333, 2333, 1101, 3185, 2433, 258, 323, 330, 330, 330, 1181, 2959, 2959, 2959, 306, 3109, 2281, 2511, 2841, 1009, 329, 1272, 1272, 2306, 2336, 2336, 1761, 3324, 3324, 1265, 389, 1389, 3399, 2008, 2817, 3154, 1879, 2448, 2448, 2448, 2541, 2541, 987, 79, 2347, 2347, 309, 309, 2285, 3425, 3425, 3116, 1057, 3061, 2916, 998, 2378, 1038, 901, 2806, 99, 1952, 1807, 2857, 784, 1642, 3396, 3396, 497, 616, 76, 3245, 190, 1210, 573, 984, 984, 984, 3157, 1530, 1040, 1191, 1219, 1302, 549, 3208, 3301, 3301, 3301, 291, 1307, 1307, 2823, 2676, 1335, 2282, 1264, 1264, 2356, 1449, 1449, 1449, 1615, 2004, 2484, 3305, 2221, 813, 813, 1408, 794, 2023, 688, 911, 911, 931, 2934, 773, 2923, 2538, 2296, 3382, 691, 1255, 1255, 1039, 2888, 1007, 2755, 1490, 1784, 1260, 1260, 1588, 1348, 173, 173, 173, 1201, 1201, 1201, 2763, 1311, 661, 818, 3347, 2627, 2441, 1904, 1236, 1306, 2599, 3353, 3349, 3229, 3229, 772, 71, 1117, 2469, 2223, 2943, 1227, 1866, 3218, 3218, 1092, 2536, 1812, 905, 905, 240, 2323, 336, 2124, 2272, 2272, 2803, 2987, 839, 839, 1050, 1252, 1939, 2756, 2756, 1288, 2467, 2258, 2258, 656, 1679, 1535, 1073, 2338, 2710, 2559, 2119, 2563, 1367, 2455, 1224, 360, 73, 1824, 2733, 833, 3355, 1868, 2651, 2505, 1243, 1291, 716, 1573, 2509, 1239, 2818, 1927, 1927, 87, 458, 1209, 658, 159, 159, 1920, 895, 1441, 1441, 2967, 3181, 2096, 425, 324, 324, 2587, 475, 1095, 697, 697, 1905, 1640, 583, 624, 3121, 2056, 1430, 1589, 1328, 2123, 2123, 3405, 1270, 1554, 1554, 1081, 2895, 142, 3434, 3434, 1317, 2018, 1372, 1562, 2436, 1245, 1273, 102, 2945, 1176, 1225, 2548, 494, 506, 3128, 3214, 3411, 3411, 1162, 1626, 338, 338, 2979, 2054, 1889, 1889, 2294, 313, 3129, 3129, 2766, 1850, 2970, 2970, 47, 569, 2371, 855, 244, 945, 985, 420, 1636, 885, 743, 2094, 3361, 2981, 2039, 1222, 331, 331, 95, 2040, 642, 1106, 2906, 2115, 2115, 1984, 1377, 1519, 1869, 3100, 3100, 1453, 2547, 1375, 1375, 983, 983, 456, 2146, 170, 1502, 224, 2416, 1941, 1019, 1699, 2179, 1559, 1332, 2520, 1656, 1656, 1897, 1650, 2450, 2450, 292, 400, 1042, 1042, 1667, 2434, 1345, 3026, 1240, 1166, 2443, 2443, 13, 3094, 2593, 3145, 2477, 1150, 2466, 3322, 1414, 859, 1842, 1842, 197, 2060, 1451, 2470, 3084, 3084, 223, 1159, 843, 2044, 1319, 750, 790, 3240, 682, 2956, 2784, 564, 564, 3233, 1775, 1775, 2374, 320, 3264, 3264, 1044, 2059, 2915, 2896, 3393, 3328, 2765, 2070, 1226, 1226, 2842, 3177, 2922, 1100, 2637, 576, 576, 3011, 1959, 362, 1840, 411, 410, 719, 719, 3254, 2061, 1503, 3251, 2951, 2951, 2600, 621, 333, 2741, 2726, 1515, 3372, 3372, 2941, 2941, 1253, 148, 2011, 40, 3193, 2899, 1539, 2919, 2833, 269, 873, 856, 606, 2503, 1282, 3215, 108, 3435, 2468, 2468, 2780, 842, 723, 723, 723, 723, 1157, 757, 2549, 2707, 2707, 3179, 613, 613, 3241, 1595, 3364, 3227, 1931, 3290, 3080, 169, 2495, 2066, 1330, 3014, 2963, 1875, 2069, 1096, 2080, 3101, 997, 591, 722, 1894, 185, 1385, 1482, 1287, 147, 1967, 3036, 294, 1776, 1776, 2449, 2449, 1855, 1538, 2162, 1122, 1725, 2822, 650, 3423, 699, 3207, 2176, 428, 578, 2075, 567, 567, 355, 3408, 3156, 2310, 2310, 2310, 2310, 1484, 1140, 3442, 738, 914, 914, 2521, 2351, 2351, 3332, 531, 1629, 1629, 1182, 3387, 986, 2108, 1712, 1908, 851, 2820, 2820, 1274, 1949, 192, 3008, 1639, 1600, 1079, 1558, 1558, 2648, 2648, 1334, 3076, 2767, 3378, 568, 48, 2885, 2298, 2000, 3346, 2496, 2454, 1452, 1452, 1452, 283, 393, 570, 992, 3153, 1031, 2643, 2643, 2643, 2643, 2955, 915, 1292, 1916, 1916, 3135, 649, 3115, 2173, 27, 55, 1483, 2299, 702, 509, 1080, 2207, 3127, 235, 1655, 528, 1649, 2849, 870, 278, 370, 2581, 1413, 1413, 1293, 2159, 1902, 1581, 1581, 1581, 2813, 1973, 2634, 3012, 3012, 1298, 683, 1753, 3071, 415, 1284, 739, 739, 1543, 1543, 1543, 3023, 595, 3263, 3263, 2216, 1021, 1831, 2957, 1594, 2782, 681, 762, 2786, 2944, 1179, 1913, 745, 745, 3211, 2997, 1746, 1280, 1052, 202, 2939, 2029, 2622, 1056, 1682, 744, 744, 3134]\n",
      "Age_current\n",
      "[0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "images, labels, ageList = get_images_and_labels('Label_Images_Age')\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#res_images = []\n",
    "res_age = []\n",
    "\n",
    "for age in ageList:\n",
    "    res_age.append(np.array(age))\n",
    "                   \n",
    "res_age = np.array(res_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2969,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_age.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model parameters as external flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic model parameters as external flags.\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer('hidden1', 1500, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 1000, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_integer('hidden3', 500, 'Number of units in hidden layer 3.')\n",
    "flags.DEFINE_integer('batch_size', 100, 'Batch size.  '\n",
    "                     'Must divide evenly into the dataset sizes.')\n",
    "flags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')\n",
    "flags.DEFINE_boolean('fake_data', False, 'If true, uses fake data '\n",
    "                     'for unit testing.')\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 100\n",
    "#CHANNELS = 3\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_nodes = [IMAGE_PIXELS, 1500, 1000, 500, NUM_CLASSES]\n",
    "n_epochs = 10\n",
    "#NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def inference(images, hidden1_units, hidden2_units):\n",
    "#     # Hidden 1\n",
    "#     with tf.name_scope('hidden1'):\n",
    "#         weights = tf.Variable(\n",
    "#             tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "#                                 stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "#             name='weights')\n",
    "#         biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "#                              name='biases')\n",
    "#         hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "#     # Hidden 2\n",
    "#     with tf.name_scope('hidden2'):\n",
    "#         weights = tf.Variable(\n",
    "#             tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "#                                 stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "#             name='weights')\n",
    "#         biases = tf00,.Variable(tf.zeros([hidden2_units]),\n",
    "#                              name='biases')\n",
    "#         hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "#     # Linear\n",
    "#     with tf.name_scope('softmax_linear'):\n",
    "#         weights = tf.Variable(\n",
    "#             tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "#                                 stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "#             name='weights')\n",
    "#         biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "#                              name='biases')\n",
    "#         logits = tf.matmul(hidden2, weights) + biases\n",
    "    \n",
    "#     return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    n_hidden_layers = 3\n",
    "    # define the layers\n",
    "    layers = [] \n",
    "    for i in range(n_hidden_layers + 1):\n",
    "        layers.append( {'weights':tf.Variable(tf.random_normal([n_nodes[i], n_nodes[i+1]])), \n",
    "                        'biases':tf.Variable(tf.random_normal([n_nodes[i+1]]))} )\n",
    "    \n",
    "    # calculate the nodal values for each layer\n",
    "    calcs = [data]\n",
    "    for i in range(n_hidden_layers):\n",
    "        calcs.append( tf.nn.relu(tf.matmul(calcs[i], layers[i]['weights']) + layers[i]['biases']) )\n",
    "\n",
    "    #  return the last layer of nodes\n",
    "    return tf.matmul(calcs[-1], layers[-1]['weights']) + layers[-1]['biases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def cal_loss(logits, labels):\n",
    "#     #labels = tf.to_int64(labels)\n",
    "#     cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#         logits, labels, name='xentropy')\n",
    "#     loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "  \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def training(loss, learning_rate):\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#     global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "#     train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "  \n",
    "#     return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "  \n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs():\n",
    "    images_placeholder = tf.placeholder(tf.float32, [None,IMAGE_PIXELS])\n",
    "    labels_placeholder = tf.placeholder(tf.float32, [None,NUM_CLASSES])\n",
    "    \n",
    "    return images_placeholder, labels_placeholder\n",
    "\n",
    "\n",
    "def fill_feed_dict(images_feed,labels_feed, images_pl, labels_pl):\n",
    "    feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "    }\n",
    "  \n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def do_eval(sess, eval_correct, images_placeholder, labels_placeholder, data_set):\n",
    "#     # And run one epoch of eval.\n",
    "#     true_count = 0  # Counts the number of correct predictions.\n",
    "#     steps_per_epoch = 47 // FLAGS.batch_size\n",
    "#     num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "#     for step in xrange(steps_per_epoch):\n",
    "#         feed_dict = fill_feed_dict(train_images,train_labels,\n",
    "#                                images_placeholder,\n",
    "#                                labels_placeholder)\n",
    "#         true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "#     precision = true_count / num_examples\n",
    "#     print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "#         (num_examples, true_count, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    \n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    with tf.Graph().as_default():\n",
    "        # Generate placeholders for the images and labels.\n",
    "        images_placeholder, labels_placeholder = placeholder_inputs()\n",
    "        \n",
    "        \n",
    "        logits = neural_network_model(images_placeholder)\n",
    "        \n",
    "        \n",
    "        cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits,labels_placeholder) )\n",
    "        training_acc = []\n",
    "        testing_acc = []\n",
    "       \n",
    "        #print cost\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "            subset_size = 128\n",
    "            for step in xrange(1000):\n",
    "                start_time = time.time()\n",
    "                total_loss = 0\n",
    "                for i in range(int(train_images.shape[0] / subset_size) ):\n",
    "                    \n",
    "                    epoch_x = train_images[i * subset_size:][:subset_size]\n",
    "                    epoch_y = train_labels[i * subset_size:][:subset_size]\n",
    "                    \n",
    "                    feed_dict = fill_feed_dict(epoch_x, epoch_y, images_placeholder, labels_placeholder)\n",
    "                    \n",
    "                    _, loss_value = sess.run([optimizer, cost],\n",
    "                                           feed_dict=feed_dict)\n",
    "                    \n",
    "                    total_loss+=loss_value\n",
    "                    \n",
    "                duration = time.time() - start_time\n",
    "                #if step % 10 == 0:\n",
    "                    #Print status to stdout.\n",
    "                correct = tf.equal(tf.argmax(logits,1), tf.argmax(labels_placeholder,1))\n",
    "                #print correct\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "                \n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, total_loss, duration)),\n",
    "                \n",
    "                current_train_acc = accuracy.eval({images_placeholder: train_images, labels_placeholder: train_labels})\n",
    "                current_test_acc = accuracy.eval({images_placeholder: test_images, labels_placeholder: test_labels})\n",
    "                \n",
    "                training_acc.append(current_train_acc)\n",
    "                testing_acc.append(current_test_acc)\n",
    "                \n",
    "                \n",
    "                print('Training Accuracy:', current_train_acc),\n",
    "                print('Testing Accuracy:', current_test_acc)\n",
    "    \n",
    "    return training_acc, testing_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the sets of images and labels for training, validation, and\n",
    "\n",
    "images = np.array(images)\n",
    "images = images.reshape(images.shape[0],IMAGE_PIXELS)\n",
    "\n",
    "#label = res_gender\n",
    "labels = dense_to_one_hot(res_age,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = images[:-300]\n",
    "train_labels = labels[:-300]\n",
    "test_images = images[-300:]\n",
    "test_labels = labels[-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669, 10000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 395528670.00 (20.735 sec) ('Training Accuracy:', 0.58523792) ('Testing Accuracy:', 0.55666667)\n",
      "Step 1: loss = 220396099.00 (18.116 sec) ('Training Accuracy:', 0.54589736) ('Testing Accuracy:', 0.54000002)\n",
      "Step 2: loss = 179621873.00 (18.783 sec) ('Training Accuracy:', 0.57474709) ('Testing Accuracy:', 0.57333332)\n",
      "Step 3: loss = 155101924.50 (18.809 sec) ('Training Accuracy:', 0.58673662) ('Testing Accuracy:', 0.56666666)\n",
      "Step 4: loss = 134338255.50 (18.091 sec) ('Training Accuracy:', 0.6140877) ('Testing Accuracy:', 0.55666667)\n",
      "Step 5: loss = 119382057.00 (18.524 sec) ('Training Accuracy:', 0.62233049) ('Testing Accuracy:', 0.56666666)\n",
      "Step 6: loss = 109150841.00 (18.371 sec) ('Training Accuracy:', 0.6140877) ('Testing Accuracy:', 0.52999997)\n",
      "Step 7: loss = 100355448.50 (18.288 sec) ('Training Accuracy:', 0.64368677) ('Testing Accuracy:', 0.55000001)\n",
      "Step 8: loss = 90316874.00 (18.259 sec) ('Training Accuracy:', 0.65979767) ('Testing Accuracy:', 0.56333333)\n",
      "Step 9: loss = 83114883.75 (19.799 sec) ('Training Accuracy:', 0.683402) ('Testing Accuracy:', 0.58333331)\n",
      "Step 10: loss = 80883845.25 (19.754 sec) ('Training Accuracy:', 0.63057327) ('Testing Accuracy:', 0.54333335)\n",
      "Step 11: loss = 76445174.25 (23.017 sec) ('Training Accuracy:', 0.6301986) ('Testing Accuracy:', 0.52666664)\n",
      "Step 12: loss = 70804721.50 (18.260 sec) ('Training Accuracy:', 0.65567631) ('Testing Accuracy:', 0.55000001)\n",
      "Step 13: loss = 65426590.75 (19.133 sec) ('Training Accuracy:', 0.71599853) ('Testing Accuracy:', 0.57999998)\n",
      "Step 14: loss = 58496118.25 (22.298 sec) ('Training Accuracy:', 0.72461593) ('Testing Accuracy:', 0.56666666)\n",
      "Step 15: loss = 56644456.00 (19.309 sec) ('Training Accuracy:', 0.71749717) ('Testing Accuracy:', 0.56333333)\n",
      "Step 16: loss = 58432280.50 (20.469 sec) ('Training Accuracy:', 0.64668417) ('Testing Accuracy:', 0.51999998)\n",
      "Step 17: loss = 55783294.38 (22.985 sec) ('Training Accuracy:', 0.68415135) ('Testing Accuracy:', 0.56)\n",
      "Step 18: loss = 48997586.62 (22.081 sec) ('Training Accuracy:', 0.73510677) ('Testing Accuracy:', 0.57333332)\n",
      "Step 19: loss = 42381009.88 (20.846 sec) ('Training Accuracy:', 0.7602098) ('Testing Accuracy:', 0.56999999)\n",
      "Step 20: loss = 39973975.25 (18.261 sec) ('Training Accuracy:', 0.76807791) ('Testing Accuracy:', 0.57666665)\n",
      "Step 21: loss = 39958214.75 (20.391 sec) ('Training Accuracy:', 0.76058447) ('Testing Accuracy:', 0.57666665)\n",
      "Step 22: loss = 46518738.62 (20.704 sec) ('Training Accuracy:', 0.65642565) ('Testing Accuracy:', 0.52999997)\n",
      "Step 23: loss = 47077415.62 (17.999 sec) ('Training Accuracy:', 0.71674782) ('Testing Accuracy:', 0.58333331)\n",
      "Step 24: loss = 38812797.19 (18.560 sec) ('Training Accuracy:', 0.78643686) ('Testing Accuracy:', 0.57666665)\n",
      "Step 25: loss = 30932883.81 (18.507 sec) ('Training Accuracy:', 0.79617834) ('Testing Accuracy:', 0.58666664)\n",
      "Step 26: loss = 29650082.25 (17.934 sec) ('Training Accuracy:', 0.79243159) ('Testing Accuracy:', 0.57333332)\n",
      "Step 27: loss = 34410482.25 (18.509 sec) ('Training Accuracy:', 0.73023605) ('Testing Accuracy:', 0.56999999)\n",
      "Step 28: loss = 32995481.94 (19.420 sec) ('Training Accuracy:', 0.6878981) ('Testing Accuracy:', 0.54666668)\n",
      "Step 29: loss = 33173641.38 (21.079 sec) ('Training Accuracy:', 0.8017984) ('Testing Accuracy:', 0.56333333)\n",
      "Step 30: loss = 30599068.00 (26.529 sec) ('Training Accuracy:', 0.80816787) ('Testing Accuracy:', 0.5933333)\n",
      "Step 31: loss = 26812000.25 (24.931 sec) ('Training Accuracy:', 0.81528664) ('Testing Accuracy:', 0.58666664)\n",
      "Step 32: loss = 29528569.69 (24.281 sec) ('Training Accuracy:', 0.72686398) ('Testing Accuracy:', 0.55333334)\n",
      "Step 33: loss = 29149880.19 (25.218 sec) ('Training Accuracy:', 0.77969277) ('Testing Accuracy:', 0.58333331)\n",
      "Step 34: loss = 22667415.34 (25.822 sec) ('Training Accuracy:', 0.83626825) ('Testing Accuracy:', 0.56)\n",
      "Step 35: loss = 18673262.44 (27.346 sec) ('Training Accuracy:', 0.83626825) ('Testing Accuracy:', 0.56)\n",
      "Step 36: loss = 20855827.88 (25.101 sec) ('Training Accuracy:', 0.83139753) ('Testing Accuracy:', 0.56999999)\n",
      "Step 37: loss = 22282441.66 (23.905 sec) ('Training Accuracy:', 0.78418881) ('Testing Accuracy:', 0.56)\n",
      "Step 38: loss = 22829820.62 (26.225 sec) ('Training Accuracy:', 0.71375048) ('Testing Accuracy:', 0.55333334)\n",
      "Step 39: loss = 22245457.81 (26.954 sec) ('Training Accuracy:', 0.83814162) ('Testing Accuracy:', 0.56)\n",
      "Step 40: loss = 19963534.31 (24.358 sec) ('Training Accuracy:', 0.84713376) ('Testing Accuracy:', 0.57666665)\n",
      "Step 41: loss = 19722497.25 (26.249 sec) ('Training Accuracy:', 0.79505432) ('Testing Accuracy:', 0.62)\n",
      "Step 42: loss = 23510871.66 (24.984 sec) ('Training Accuracy:', 0.81978267) ('Testing Accuracy:', 0.56333333)\n",
      "Step 43: loss = 19166439.72 (26.057 sec) ('Training Accuracy:', 0.77856874) ('Testing Accuracy:', 0.56666666)\n",
      "Step 44: loss = 15818942.69 (25.816 sec) ('Training Accuracy:', 0.85650057) ('Testing Accuracy:', 0.55666667)\n",
      "Step 45: loss = 13190035.11 (24.819 sec) ('Training Accuracy:', 0.85387784) ('Testing Accuracy:', 0.57999998)\n",
      "Step 46: loss = 13806201.25 (24.695 sec) ('Training Accuracy:', 0.863994) ('Testing Accuracy:', 0.56333333)\n",
      "Step 47: loss = 14714919.06 (25.895 sec) ('Training Accuracy:', 0.80591983) ('Testing Accuracy:', 0.57333332)\n",
      "Step 48: loss = 14885947.03 (25.921 sec) ('Training Accuracy:', 0.84001499) ('Testing Accuracy:', 0.56999999)\n",
      "Step 49: loss = 16717305.72 (25.257 sec) ('Training Accuracy:', 0.8077932) ('Testing Accuracy:', 0.62)\n",
      "Step 50: loss = 18009347.41 (25.201 sec) ('Training Accuracy:', 0.88047957) ('Testing Accuracy:', 0.57666665)\n",
      "Step 51: loss = 11516599.02 (25.491 sec) ('Training Accuracy:', 0.85500187) ('Testing Accuracy:', 0.57999998)\n",
      "Step 52: loss = 9653285.82 (22.732 sec) ('Training Accuracy:', 0.85949796) ('Testing Accuracy:', 0.57333332)\n",
      "Step 53: loss = 9678017.12 (20.977 sec) ('Training Accuracy:', 0.8786062) ('Testing Accuracy:', 0.57333332)\n",
      "Step 54: loss = 10106953.42 (24.663 sec) ('Training Accuracy:', 0.83476955) ('Testing Accuracy:', 0.61000001)\n",
      "Step 55: loss = 12767426.06 (20.331 sec) ('Training Accuracy:', 0.88909703) ('Testing Accuracy:', 0.56999999)\n",
      "Step 56: loss = 12256964.94 (18.805 sec) ('Training Accuracy:', 0.85275382) ('Testing Accuracy:', 0.57666665)\n",
      "Step 57: loss = 13983117.12 (18.510 sec) ('Training Accuracy:', 0.87223679) ('Testing Accuracy:', 0.57666665)\n",
      "Step 58: loss = 10849780.32 (18.433 sec) ('Training Accuracy:', 0.85462719) ('Testing Accuracy:', 0.61000001)\n",
      "Step 59: loss = 11475276.03 (18.343 sec) ('Training Accuracy:', 0.88047957) ('Testing Accuracy:', 0.60000002)\n",
      "Step 60: loss = 10135785.66 (18.627 sec) ('Training Accuracy:', 0.89958787) ('Testing Accuracy:', 0.57333332)\n",
      "Step 61: loss = 13481515.30 (19.093 sec) ('Training Accuracy:', 0.88235295) ('Testing Accuracy:', 0.57333332)\n",
      "Step 62: loss = 8455843.21 (18.362 sec) ('Training Accuracy:', 0.86137128) ('Testing Accuracy:', 0.60333335)\n",
      "Step 63: loss = 8857971.64 (17.993 sec) ('Training Accuracy:', 0.89921319) ('Testing Accuracy:', 0.57666665)\n",
      "Step 64: loss = 7981046.94 (18.455 sec) ('Training Accuracy:', 0.90071189) ('Testing Accuracy:', 0.57666665)\n",
      "Step 65: loss = 8506238.92 (18.654 sec) ('Training Accuracy:', 0.89921319) ('Testing Accuracy:', 0.56999999)\n",
      "Step 66: loss = 7148400.58 (18.652 sec) ('Training Accuracy:', 0.83739227) ('Testing Accuracy:', 0.61333334)\n",
      "Step 67: loss = 7399944.48 (18.475 sec) ('Training Accuracy:', 0.8801049) ('Testing Accuracy:', 0.5933333)\n",
      "Step 68: loss = 7592826.45 (18.734 sec) ('Training Accuracy:', 0.90146124) ('Testing Accuracy:', 0.58333331)\n",
      "Step 69: loss = 8530550.36 (18.594 sec) ('Training Accuracy:', 0.90333456) ('Testing Accuracy:', 0.57999998)\n",
      "Step 70: loss = 8917605.17 (18.609 sec) ('Training Accuracy:', 0.88759834) ('Testing Accuracy:', 0.5933333)\n",
      "Step 71: loss = 9181566.79 (19.142 sec) ('Training Accuracy:', 0.76395655) ('Testing Accuracy:', 0.63)\n",
      "Step 72: loss = 11113199.07 (18.253 sec) ('Training Accuracy:', 0.89171976) ('Testing Accuracy:', 0.5933333)\n",
      "Step 73: loss = 10964444.16 (18.636 sec) ('Training Accuracy:', 0.89958787) ('Testing Accuracy:', 0.57999998)\n",
      "Step 74: loss = 10920355.54 (18.682 sec) ('Training Accuracy:', 0.86624205) ('Testing Accuracy:', 0.60000002)\n",
      "Step 75: loss = 9520502.06 (19.101 sec) ('Training Accuracy:', 0.75271636) ('Testing Accuracy:', 0.63999999)\n",
      "Step 76: loss = 12823183.77 (19.034 sec) ('Training Accuracy:', 0.89359313) ('Testing Accuracy:', 0.5933333)\n",
      "Step 77: loss = 14376796.55 (18.359 sec) ('Training Accuracy:', 0.85799927) ('Testing Accuracy:', 0.60333335)\n",
      "Step 78: loss = 10358715.42 (18.704 sec) ('Training Accuracy:', 0.75683779) ('Testing Accuracy:', 0.63)\n",
      "Step 79: loss = 9613111.28 (18.739 sec) ('Training Accuracy:', 0.81491196) ('Testing Accuracy:', 0.63999999)\n",
      "Step 80: loss = 13734371.75 (18.540 sec) ('Training Accuracy:', 0.87448484) ('Testing Accuracy:', 0.61666667)\n",
      "Step 81: loss = 16758030.50 (18.828 sec) ('Training Accuracy:', 0.81266391) ('Testing Accuracy:', 0.61666667)\n",
      "Step 82: loss = 13494503.95 (18.513 sec) ('Training Accuracy:', 0.69426751) ('Testing Accuracy:', 0.64333332)\n",
      "Step 83: loss = 19960144.81 (18.549 sec) ('Training Accuracy:', 0.81303859) ('Testing Accuracy:', 0.63333333)\n",
      "Step 84: loss = 29600476.73 (18.369 sec) ('Training Accuracy:', 0.7010116) ('Testing Accuracy:', 0.64999998)\n",
      "Step 85: loss = 29474308.66 (18.691 sec) ('Training Accuracy:', 0.70063692) ('Testing Accuracy:', 0.65333331)\n",
      "Step 86: loss = 54626339.88 (18.969 sec) ('Training Accuracy:', 0.83889097) ('Testing Accuracy:', 0.62333333)\n",
      "Step 87: loss = 79810417.00 (18.407 sec) ('Training Accuracy:', 0.55638814) ('Testing Accuracy:', 0.47999999)\n",
      "Step 88: loss = 51759860.06 (18.392 sec) ('Training Accuracy:', 0.72386664) ('Testing Accuracy:', 0.52666664)\n",
      "Step 89: loss = 19125332.47 (17.981 sec) ('Training Accuracy:', 0.80966651) ('Testing Accuracy:', 0.51999998)\n",
      "Step 90: loss = 10198429.27 (17.612 sec) ('Training Accuracy:', 0.8254028) ('Testing Accuracy:', 0.53333336)\n",
      "Step 91: loss = 5774459.58 (18.848 sec) ('Training Accuracy:', 0.88385165) ('Testing Accuracy:', 0.53333336)\n",
      "Step 92: loss = 3832364.97 (18.248 sec) ('Training Accuracy:', 0.89996254) ('Testing Accuracy:', 0.55333334)\n",
      "Step 93: loss = 3600217.03 (18.402 sec) ('Training Accuracy:', 0.84825778) ('Testing Accuracy:', 0.54333335)\n",
      "Step 94: loss = 2834735.17 (18.749 sec) ('Training Accuracy:', 0.91270137) ('Testing Accuracy:', 0.56333333)\n",
      "Step 95: loss = 2483986.63 (21.205 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.5933333)\n",
      "Step 96: loss = 2488870.22 (18.592 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.58999997)\n",
      "Step 97: loss = 6099974.95 (24.486 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.58666664)\n",
      "Step 98: loss = 10518841.50 (18.998 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.58666664)\n",
      "Step 99: loss = 19011843.37 (18.609 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.57999998)\n",
      "Step 100: loss = 26509186.40 (18.661 sec) ('Training Accuracy:', 0.88797301) ('Testing Accuracy:', 0.56333333)\n",
      "Step 101: loss = 34970640.25 (18.755 sec) ('Training Accuracy:', 0.70513302) ('Testing Accuracy:', 0.50999999)\n",
      "Step 102: loss = 33019193.95 (20.832 sec) ('Training Accuracy:', 0.61034095) ('Testing Accuracy:', 0.51333332)\n",
      "Step 103: loss = 33624793.92 (22.864 sec) ('Training Accuracy:', 0.59722745) ('Testing Accuracy:', 0.49666667)\n",
      "Step 104: loss = 29889762.48 (26.473 sec) ('Training Accuracy:', 0.63694268) ('Testing Accuracy:', 0.52333331)\n",
      "Step 105: loss = 25318065.02 (20.331 sec) ('Training Accuracy:', 0.78493816) ('Testing Accuracy:', 0.52999997)\n",
      "Step 106: loss = 16578960.23 (18.543 sec) ('Training Accuracy:', 0.93518174) ('Testing Accuracy:', 0.57333332)\n",
      "Step 107: loss = 8810454.80 (18.595 sec) ('Training Accuracy:', 0.91532409) ('Testing Accuracy:', 0.58999997)\n",
      "Step 108: loss = 4014625.62 (18.779 sec) ('Training Accuracy:', 0.9437992) ('Testing Accuracy:', 0.55333334)\n",
      "Step 109: loss = 2028400.20 (19.126 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.57666665)\n",
      "Step 110: loss = 1291151.69 (20.425 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.55666667)\n",
      "Step 111: loss = 1005738.67 (22.640 sec) ('Training Accuracy:', 0.94267517) ('Testing Accuracy:', 0.56)\n",
      "Step 112: loss = 881171.88 (22.072 sec) ('Training Accuracy:', 0.96028477) ('Testing Accuracy:', 0.58666664)\n",
      "Step 113: loss = 726453.29 (18.441 sec) ('Training Accuracy:', 0.95653802) ('Testing Accuracy:', 0.56)\n",
      "Step 114: loss = 808755.73 (18.473 sec) ('Training Accuracy:', 0.95653802) ('Testing Accuracy:', 0.56)\n",
      "Step 115: loss = 818544.90 (18.560 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.55666667)\n",
      "Step 116: loss = 742247.03 (18.486 sec) ('Training Accuracy:', 0.94792056) ('Testing Accuracy:', 0.57333332)\n",
      "Step 117: loss = 546955.98 (18.538 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.56)\n",
      "Step 118: loss = 500832.01 (18.696 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.56)\n",
      "Step 119: loss = 477957.42 (18.407 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.56999999)\n",
      "Step 120: loss = 530397.55 (18.263 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.54666668)\n",
      "Step 121: loss = 520169.84 (18.710 sec) ('Training Accuracy:', 0.90970403) ('Testing Accuracy:', 0.54666668)\n",
      "Step 122: loss = 720789.46 (18.322 sec) ('Training Accuracy:', 0.95616335) ('Testing Accuracy:', 0.58666664)\n",
      "Step 123: loss = 476493.73 (20.140 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.56)\n",
      "Step 124: loss = 434232.17 (20.813 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.54666668)\n",
      "Step 125: loss = 386569.96 (19.303 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.55333334)\n",
      "Step 126: loss = 505985.57 (18.582 sec) ('Training Accuracy:', 0.91644812) ('Testing Accuracy:', 0.56333333)\n",
      "Step 127: loss = 452760.37 (19.209 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.55000001)\n",
      "Step 128: loss = 667435.76 (19.690 sec) ('Training Accuracy:', 0.9393031) ('Testing Accuracy:', 0.56)\n",
      "Step 129: loss = 737116.00 (19.419 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.57666665)\n",
      "Step 130: loss = 791851.41 (18.720 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.56333333)\n",
      "Step 131: loss = 1062663.68 (19.156 sec) ('Training Accuracy:', 0.93330836) ('Testing Accuracy:', 0.55000001)\n",
      "Step 132: loss = 1549395.81 (18.852 sec) ('Training Accuracy:', 0.89696515) ('Testing Accuracy:', 0.55666667)\n",
      "Step 133: loss = 948608.04 (19.588 sec) ('Training Accuracy:', 0.91120267) ('Testing Accuracy:', 0.56333333)\n",
      "Step 134: loss = 905151.49 (18.816 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.57999998)\n",
      "Step 135: loss = 449450.18 (19.002 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.56)\n",
      "Step 136: loss = 411582.90 (18.578 sec) ('Training Accuracy:', 0.93180966) ('Testing Accuracy:', 0.55666667)\n",
      "Step 137: loss = 487857.95 (19.103 sec) ('Training Accuracy:', 0.92881227) ('Testing Accuracy:', 0.56)\n",
      "Step 138: loss = 485285.46 (19.355 sec) ('Training Accuracy:', 0.95466465) ('Testing Accuracy:', 0.57333332)\n",
      "Step 139: loss = 512884.69 (20.375 sec) ('Training Accuracy:', 0.91082805) ('Testing Accuracy:', 0.55666667)\n",
      "Step 140: loss = 428029.55 (19.056 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.55000001)\n",
      "Step 141: loss = 524102.02 (18.614 sec) ('Training Accuracy:', 0.90595728) ('Testing Accuracy:', 0.56333333)\n",
      "Step 142: loss = 526997.01 (19.216 sec) ('Training Accuracy:', 0.95578867) ('Testing Accuracy:', 0.56999999)\n",
      "Step 143: loss = 346189.91 (19.781 sec) ('Training Accuracy:', 0.94042712) ('Testing Accuracy:', 0.56666666)\n",
      "Step 144: loss = 407018.55 (20.084 sec) ('Training Accuracy:', 0.9246909) ('Testing Accuracy:', 0.56666666)\n",
      "Step 145: loss = 583977.98 (20.186 sec) ('Training Accuracy:', 0.96065944) ('Testing Accuracy:', 0.57333332)\n",
      "Step 146: loss = 641364.68 (20.337 sec) ('Training Accuracy:', 0.93443239) ('Testing Accuracy:', 0.56)\n",
      "Step 147: loss = 418056.84 (20.494 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.56)\n",
      "Step 148: loss = 484374.75 (20.290 sec) ('Training Accuracy:', 0.91607344) ('Testing Accuracy:', 0.56333333)\n",
      "Step 149: loss = 450067.21 (20.135 sec) ('Training Accuracy:', 0.94792056) ('Testing Accuracy:', 0.56)\n",
      "Step 150: loss = 472774.74 (20.517 sec) ('Training Accuracy:', 0.91494942) ('Testing Accuracy:', 0.55333334)\n",
      "Step 151: loss = 497126.85 (20.328 sec) ('Training Accuracy:', 0.95578867) ('Testing Accuracy:', 0.56666666)\n",
      "Step 152: loss = 490250.77 (20.691 sec) ('Training Accuracy:', 0.90370923) ('Testing Accuracy:', 0.55666667)\n",
      "Step 153: loss = 368840.56 (20.495 sec) ('Training Accuracy:', 0.92956161) ('Testing Accuracy:', 0.55000001)\n",
      "Step 154: loss = 535823.58 (19.908 sec) ('Training Accuracy:', 0.92806292) ('Testing Accuracy:', 0.55666667)\n",
      "Step 155: loss = 482863.35 (19.886 sec) ('Training Accuracy:', 0.92731363) ('Testing Accuracy:', 0.56333333)\n",
      "Step 156: loss = 347522.20 (19.304 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.56666666)\n",
      "Step 157: loss = 695685.58 (20.076 sec) ('Training Accuracy:', 0.92806292) ('Testing Accuracy:', 0.56666666)\n",
      "Step 158: loss = 380563.73 (19.875 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.56333333)\n",
      "Step 159: loss = 530029.11 (19.948 sec) ('Training Accuracy:', 0.88797301) ('Testing Accuracy:', 0.55333334)\n",
      "Step 160: loss = 708251.66 (20.051 sec) ('Training Accuracy:', 0.89808917) ('Testing Accuracy:', 0.56333333)\n",
      "Step 161: loss = 634554.94 (19.921 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.56999999)\n",
      "Step 162: loss = 457547.76 (19.983 sec) ('Training Accuracy:', 0.94192582) ('Testing Accuracy:', 0.58333331)\n",
      "Step 163: loss = 412397.88 (19.437 sec) ('Training Accuracy:', 0.92881227) ('Testing Accuracy:', 0.56666666)\n",
      "Step 164: loss = 475227.25 (19.914 sec) ('Training Accuracy:', 0.91157734) ('Testing Accuracy:', 0.55666667)\n",
      "Step 165: loss = 325684.73 (19.952 sec) ('Training Accuracy:', 0.92056948) ('Testing Accuracy:', 0.55666667)\n",
      "Step 166: loss = 543957.46 (20.221 sec) ('Training Accuracy:', 0.91982013) ('Testing Accuracy:', 0.56666666)\n",
      "Step 167: loss = 628894.66 (19.794 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.56333333)\n",
      "Step 168: loss = 348035.80 (19.634 sec) ('Training Accuracy:', 0.90445858) ('Testing Accuracy:', 0.55000001)\n",
      "Step 169: loss = 495778.09 (19.877 sec) ('Training Accuracy:', 0.95616335) ('Testing Accuracy:', 0.57333332)\n",
      "Step 170: loss = 386648.65 (19.973 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.56333333)\n",
      "Step 171: loss = 427082.45 (20.438 sec) ('Training Accuracy:', 0.9261896) ('Testing Accuracy:', 0.55333334)\n",
      "Step 172: loss = 332340.49 (19.964 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.56999999)\n",
      "Step 173: loss = 554492.00 (20.348 sec) ('Training Accuracy:', 0.94042712) ('Testing Accuracy:', 0.57333332)\n",
      "Step 174: loss = 669353.57 (19.676 sec) ('Training Accuracy:', 0.93368304) ('Testing Accuracy:', 0.56999999)\n",
      "Step 175: loss = 722732.74 (19.810 sec) ('Training Accuracy:', 0.90108657) ('Testing Accuracy:', 0.56)\n",
      "Step 176: loss = 569300.93 (20.043 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.57333332)\n",
      "Step 177: loss = 702314.55 (19.776 sec) ('Training Accuracy:', 0.92544025) ('Testing Accuracy:', 0.57333332)\n",
      "Step 178: loss = 434440.93 (19.939 sec) ('Training Accuracy:', 0.90783066) ('Testing Accuracy:', 0.55666667)\n",
      "Step 179: loss = 661501.71 (19.651 sec) ('Training Accuracy:', 0.87148744) ('Testing Accuracy:', 0.55666667)\n",
      "Step 180: loss = 753493.39 (20.084 sec) ('Training Accuracy:', 0.89059573) ('Testing Accuracy:', 0.56)\n",
      "Step 181: loss = 2278272.88 (19.722 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.56999999)\n",
      "Step 182: loss = 965835.36 (19.282 sec) ('Training Accuracy:', 0.91569877) ('Testing Accuracy:', 0.56333333)\n",
      "Step 183: loss = 453197.38 (19.867 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.57999998)\n",
      "Step 184: loss = 447842.86 (19.383 sec) ('Training Accuracy:', 0.9408018) ('Testing Accuracy:', 0.56333333)\n",
      "Step 185: loss = 247947.00 (20.147 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.58333331)\n",
      "Step 186: loss = 413876.28 (19.523 sec) ('Training Accuracy:', 0.90745598) ('Testing Accuracy:', 0.56666666)\n",
      "Step 187: loss = 552991.07 (19.597 sec) ('Training Accuracy:', 0.92881227) ('Testing Accuracy:', 0.56999999)\n",
      "Step 188: loss = 765477.68 (19.745 sec) ('Training Accuracy:', 0.96140879) ('Testing Accuracy:', 0.56999999)\n",
      "Step 189: loss = 673428.16 (19.387 sec) ('Training Accuracy:', 0.94979393) ('Testing Accuracy:', 0.56666666)\n",
      "Step 190: loss = 571760.30 (19.633 sec) ('Training Accuracy:', 0.94979393) ('Testing Accuracy:', 0.55666667)\n",
      "Step 191: loss = 314596.90 (19.286 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.56999999)\n",
      "Step 192: loss = 497101.23 (19.703 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.55333334)\n",
      "Step 193: loss = 484139.30 (19.459 sec) ('Training Accuracy:', 0.91345072) ('Testing Accuracy:', 0.56)\n",
      "Step 194: loss = 416408.98 (19.741 sec) ('Training Accuracy:', 0.95466465) ('Testing Accuracy:', 0.56)\n",
      "Step 195: loss = 589691.84 (19.978 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.55666667)\n",
      "Step 196: loss = 346290.78 (19.859 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.56333333)\n",
      "Step 197: loss = 285855.92 (19.648 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.56666666)\n",
      "Step 198: loss = 357525.41 (19.589 sec) ('Training Accuracy:', 0.93180966) ('Testing Accuracy:', 0.56)\n",
      "Step 199: loss = 264433.79 (19.996 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.56999999)\n",
      "Step 200: loss = 305804.99 (19.117 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.56)\n",
      "Step 201: loss = 372789.33 (19.643 sec) ('Training Accuracy:', 0.93817908) ('Testing Accuracy:', 0.56666666)\n",
      "Step 202: loss = 224685.29 (19.587 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.57666665)\n",
      "Step 203: loss = 308731.07 (19.678 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.56)\n",
      "Step 204: loss = 255179.55 (20.490 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.56666666)\n",
      "Step 205: loss = 414352.77 (19.977 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.56666666)\n",
      "Step 206: loss = 422845.05 (19.650 sec) ('Training Accuracy:', 0.96740353) ('Testing Accuracy:', 0.56333333)\n",
      "Step 207: loss = 404293.19 (20.274 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.56666666)\n",
      "Step 208: loss = 495015.66 (20.134 sec) ('Training Accuracy:', 0.96140879) ('Testing Accuracy:', 0.56999999)\n",
      "Step 209: loss = 253650.08 (20.036 sec) ('Training Accuracy:', 0.9216935) ('Testing Accuracy:', 0.56666666)\n",
      "Step 210: loss = 360968.85 (19.897 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.56999999)\n",
      "Step 211: loss = 448394.76 (19.948 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.55666667)\n",
      "Step 212: loss = 280270.40 (19.744 sec) ('Training Accuracy:', 0.95279133) ('Testing Accuracy:', 0.56333333)\n",
      "Step 213: loss = 334648.75 (20.788 sec) ('Training Accuracy:', 0.94042712) ('Testing Accuracy:', 0.56333333)\n",
      "Step 214: loss = 265454.50 (20.290 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.56)\n",
      "Step 215: loss = 453869.40 (20.064 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.57666665)\n",
      "Step 216: loss = 328018.92 (20.195 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.56666666)\n",
      "Step 217: loss = 225655.80 (20.313 sec) ('Training Accuracy:', 0.92206818) ('Testing Accuracy:', 0.56333333)\n",
      "Step 218: loss = 437707.95 (20.332 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.55666667)\n",
      "Step 219: loss = 259201.14 (20.384 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.55666667)\n",
      "Step 220: loss = 428738.93 (19.990 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.56666666)\n",
      "Step 221: loss = 570143.33 (19.912 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.56)\n",
      "Step 222: loss = 840541.49 (25.522 sec) ('Training Accuracy:', 0.88535035) ('Testing Accuracy:', 0.55333334)\n",
      "Step 223: loss = 1469882.89 (24.638 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.56999999)\n",
      "Step 224: loss = 525639.05 (24.045 sec) ('Training Accuracy:', 0.95578867) ('Testing Accuracy:', 0.57333332)\n",
      "Step 225: loss = 410434.89 (24.219 sec) ('Training Accuracy:', 0.92956161) ('Testing Accuracy:', 0.56333333)\n",
      "Step 226: loss = 628714.96 (20.389 sec) ('Training Accuracy:', 0.93817908) ('Testing Accuracy:', 0.56333333)\n",
      "Step 227: loss = 543289.63 (24.148 sec) ('Training Accuracy:', 0.97152489) ('Testing Accuracy:', 0.57999998)\n",
      "Step 228: loss = 608845.15 (24.604 sec) ('Training Accuracy:', 0.91232669) ('Testing Accuracy:', 0.56333333)\n",
      "Step 229: loss = 672304.75 (22.251 sec) ('Training Accuracy:', 0.91345072) ('Testing Accuracy:', 0.56333333)\n",
      "Step 230: loss = 996096.17 (20.501 sec) ('Training Accuracy:', 0.93330836) ('Testing Accuracy:', 0.55000001)\n",
      "Step 231: loss = 1526849.56 (19.566 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.5933333)\n",
      "Step 232: loss = 3809939.59 (19.218 sec) ('Training Accuracy:', 0.94267517) ('Testing Accuracy:', 0.56333333)\n",
      "Step 233: loss = 3389452.82 (19.979 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.55333334)\n",
      "Step 234: loss = 2183025.43 (19.541 sec) ('Training Accuracy:', 0.8624953) ('Testing Accuracy:', 0.64333332)\n",
      "Step 235: loss = 5238474.12 (19.236 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.57333332)\n",
      "Step 236: loss = 9339312.41 (19.450 sec) ('Training Accuracy:', 0.86736608) ('Testing Accuracy:', 0.63666666)\n",
      "Step 237: loss = 4565297.18 (19.274 sec) ('Training Accuracy:', 0.87448484) ('Testing Accuracy:', 0.61666667)\n",
      "Step 238: loss = 8421358.66 (19.032 sec) ('Training Accuracy:', 0.89921319) ('Testing Accuracy:', 0.62)\n",
      "Step 239: loss = 23878209.06 (19.395 sec) ('Training Accuracy:', 0.88422632) ('Testing Accuracy:', 0.64333332)\n",
      "Step 240: loss = 46003665.50 (19.003 sec) ('Training Accuracy:', 0.59010863) ('Testing Accuracy:', 0.47666666)\n",
      "Step 241: loss = 55187746.98 (18.967 sec) ('Training Accuracy:', 0.89996254) ('Testing Accuracy:', 0.55666667)\n",
      "Step 242: loss = 11303700.06 (18.753 sec) ('Training Accuracy:', 0.91832149) ('Testing Accuracy:', 0.62333333)\n",
      "Step 243: loss = 2699350.59 (18.698 sec) ('Training Accuracy:', 0.96553016) ('Testing Accuracy:', 0.57666665)\n",
      "Step 244: loss = 1277992.41 (18.838 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.57333332)\n",
      "Step 245: loss = 1158161.91 (19.072 sec) ('Training Accuracy:', 0.87748218) ('Testing Accuracy:', 0.54666668)\n",
      "Step 246: loss = 2104946.16 (18.744 sec) ('Training Accuracy:', 0.90333456) ('Testing Accuracy:', 0.56)\n",
      "Step 247: loss = 1591197.25 (18.706 sec) ('Training Accuracy:', 0.86624205) ('Testing Accuracy:', 0.55000001)\n",
      "Step 248: loss = 1774371.47 (18.658 sec) ('Training Accuracy:', 0.90820533) ('Testing Accuracy:', 0.56333333)\n",
      "Step 249: loss = 1897967.26 (18.809 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.56)\n",
      "Step 250: loss = 2538284.75 (18.856 sec) ('Training Accuracy:', 0.9393031) ('Testing Accuracy:', 0.56333333)\n",
      "Step 251: loss = 3152986.30 (19.001 sec) ('Training Accuracy:', 0.96627951) ('Testing Accuracy:', 0.56999999)\n",
      "Step 252: loss = 2021620.47 (18.756 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.57333332)\n",
      "Step 253: loss = 1925774.48 (23.611 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.60666668)\n",
      "Step 254: loss = 1530740.54 (23.576 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.57999998)\n",
      "Step 255: loss = 667232.01 (19.331 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.56)\n",
      "Step 256: loss = 445244.97 (23.482 sec) ('Training Accuracy:', 0.95279133) ('Testing Accuracy:', 0.56333333)\n",
      "Step 257: loss = 382514.83 (21.249 sec) ('Training Accuracy:', 0.95054328) ('Testing Accuracy:', 0.56)\n",
      "Step 258: loss = 396881.96 (23.714 sec) ('Training Accuracy:', 0.96590483) ('Testing Accuracy:', 0.56)\n",
      "Step 259: loss = 244298.68 (19.724 sec) ('Training Accuracy:', 0.94679654) ('Testing Accuracy:', 0.55000001)\n",
      "Step 260: loss = 449878.29 (18.746 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.55000001)\n",
      "Step 261: loss = 585870.39 (18.579 sec) ('Training Accuracy:', 0.95616335) ('Testing Accuracy:', 0.56666666)\n",
      "Step 262: loss = 236986.59 (18.984 sec) ('Training Accuracy:', 0.9569127) ('Testing Accuracy:', 0.56333333)\n",
      "Step 263: loss = 358134.38 (19.301 sec) ('Training Accuracy:', 0.94042712) ('Testing Accuracy:', 0.54666668)\n",
      "Step 264: loss = 504308.16 (19.020 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.56)\n",
      "Step 265: loss = 601731.66 (19.249 sec) ('Training Accuracy:', 0.96103412) ('Testing Accuracy:', 0.55000001)\n",
      "Step 266: loss = 392540.63 (18.656 sec) ('Training Accuracy:', 0.92356688) ('Testing Accuracy:', 0.56)\n",
      "Step 267: loss = 727373.18 (19.079 sec) ('Training Accuracy:', 0.96103412) ('Testing Accuracy:', 0.57999998)\n",
      "Step 268: loss = 303537.76 (18.491 sec) ('Training Accuracy:', 0.94866991) ('Testing Accuracy:', 0.56)\n",
      "Step 269: loss = 285368.65 (19.031 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.56)\n",
      "Step 270: loss = 264224.48 (18.792 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.56333333)\n",
      "Step 271: loss = 288954.95 (18.890 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.57333332)\n",
      "Step 272: loss = 179199.31 (18.494 sec) ('Training Accuracy:', 0.95953542) ('Testing Accuracy:', 0.57333332)\n",
      "Step 273: loss = 207059.68 (18.567 sec) ('Training Accuracy:', 0.96590483) ('Testing Accuracy:', 0.57666665)\n",
      "Step 274: loss = 265274.43 (18.797 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.56999999)\n",
      "Step 275: loss = 412165.29 (18.534 sec) ('Training Accuracy:', 0.94904459) ('Testing Accuracy:', 0.55333334)\n",
      "Step 276: loss = 270334.72 (18.751 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.56)\n",
      "Step 277: loss = 209313.73 (18.654 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.56333333)\n",
      "Step 278: loss = 294275.04 (19.170 sec) ('Training Accuracy:', 0.94642186) ('Testing Accuracy:', 0.55333334)\n",
      "Step 279: loss = 386953.01 (19.307 sec) ('Training Accuracy:', 0.93031096) ('Testing Accuracy:', 0.55333334)\n",
      "Step 280: loss = 428894.59 (19.611 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.56)\n",
      "Step 281: loss = 319422.20 (19.954 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.56)\n",
      "Step 282: loss = 345292.67 (19.692 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.56)\n",
      "Step 283: loss = 361160.38 (20.102 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.56333333)\n",
      "Step 284: loss = 357877.72 (20.073 sec) ('Training Accuracy:', 0.94117647) ('Testing Accuracy:', 0.55666667)\n",
      "Step 285: loss = 332382.53 (20.017 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.56333333)\n",
      "Step 286: loss = 246122.60 (20.099 sec) ('Training Accuracy:', 0.95653802) ('Testing Accuracy:', 0.56333333)\n",
      "Step 287: loss = 262981.84 (20.658 sec) ('Training Accuracy:', 0.96253276) ('Testing Accuracy:', 0.57999998)\n",
      "Step 288: loss = 262501.74 (20.169 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.56333333)\n",
      "Step 289: loss = 277193.47 (20.705 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.56)\n",
      "Step 290: loss = 205147.18 (20.359 sec) ('Training Accuracy:', 0.94866991) ('Testing Accuracy:', 0.55666667)\n",
      "Step 291: loss = 366031.94 (20.238 sec) ('Training Accuracy:', 0.95728737) ('Testing Accuracy:', 0.56666666)\n",
      "Step 292: loss = 510765.25 (21.488 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.55333334)\n",
      "Step 293: loss = 339358.57 (21.577 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.5933333)\n",
      "Step 294: loss = 393983.11 (20.947 sec) ('Training Accuracy:', 0.94454855) ('Testing Accuracy:', 0.56333333)\n",
      "Step 295: loss = 455986.42 (21.576 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.57999998)\n",
      "Step 296: loss = 492631.43 (20.852 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.56999999)\n",
      "Step 297: loss = 330349.36 (20.680 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.56999999)\n",
      "Step 298: loss = 418616.94 (20.569 sec) ('Training Accuracy:', 0.97452229) ('Testing Accuracy:', 0.57999998)\n",
      "Step 299: loss = 418981.60 (20.743 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.56999999)\n",
      "Step 300: loss = 321820.07 (21.246 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.57666665)\n",
      "Step 301: loss = 590561.46 (20.901 sec) ('Training Accuracy:', 0.94567251) ('Testing Accuracy:', 0.55333334)\n",
      "Step 302: loss = 915065.43 (20.220 sec) ('Training Accuracy:', 0.91644812) ('Testing Accuracy:', 0.56)\n",
      "Step 303: loss = 739542.97 (20.692 sec) ('Training Accuracy:', 0.88535035) ('Testing Accuracy:', 0.56)\n",
      "Step 304: loss = 1182975.73 (20.356 sec) ('Training Accuracy:', 0.95653802) ('Testing Accuracy:', 0.56999999)\n",
      "Step 305: loss = 277350.03 (20.569 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.56666666)\n",
      "Step 306: loss = 2299128.64 (20.046 sec) ('Training Accuracy:', 0.96927691) ('Testing Accuracy:', 0.58333331)\n",
      "Step 307: loss = 492118.19 (20.576 sec) ('Training Accuracy:', 0.96403146) ('Testing Accuracy:', 0.5933333)\n",
      "Step 308: loss = 767958.64 (19.996 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.54000002)\n",
      "Step 309: loss = 716800.89 (20.211 sec) ('Training Accuracy:', 0.91270137) ('Testing Accuracy:', 0.55000001)\n",
      "Step 310: loss = 1289987.46 (20.053 sec) ('Training Accuracy:', 0.87298614) ('Testing Accuracy:', 0.54666668)\n",
      "Step 311: loss = 687521.51 (19.648 sec) ('Training Accuracy:', 0.88122892) ('Testing Accuracy:', 0.56333333)\n",
      "Step 312: loss = 1376906.65 (21.062 sec) ('Training Accuracy:', 0.93405771) ('Testing Accuracy:', 0.56666666)\n",
      "Step 313: loss = 1476966.94 (19.497 sec) ('Training Accuracy:', 0.91307604) ('Testing Accuracy:', 0.54000002)\n",
      "Step 314: loss = 377234.00 (19.766 sec) ('Training Accuracy:', 0.91195202) ('Testing Accuracy:', 0.55000001)\n",
      "Step 315: loss = 1771370.27 (19.883 sec) ('Training Accuracy:', 0.83889097) ('Testing Accuracy:', 0.56333333)\n",
      "Step 316: loss = 1546759.52 (19.775 sec) ('Training Accuracy:', 0.90932935) ('Testing Accuracy:', 0.55333334)\n",
      "Step 317: loss = 1247847.20 (19.207 sec) ('Training Accuracy:', 0.95953542) ('Testing Accuracy:', 0.57999998)\n",
      "Step 318: loss = 350348.94 (19.654 sec) ('Training Accuracy:', 0.89846385) ('Testing Accuracy:', 0.55666667)\n",
      "Step 319: loss = 951479.44 (19.350 sec) ('Training Accuracy:', 0.8801049) ('Testing Accuracy:', 0.56)\n",
      "Step 320: loss = 2382463.16 (19.314 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.58666664)\n",
      "Step 321: loss = 1883305.55 (19.486 sec) ('Training Accuracy:', 0.91494942) ('Testing Accuracy:', 0.54333335)\n",
      "Step 322: loss = 2174300.59 (20.473 sec) ('Training Accuracy:', 0.86024725) ('Testing Accuracy:', 0.54000002)\n",
      "Step 323: loss = 2349490.51 (21.101 sec) ('Training Accuracy:', 0.93443239) ('Testing Accuracy:', 0.57333332)\n",
      "Step 324: loss = 1631848.65 (23.972 sec) ('Training Accuracy:', 0.91719747) ('Testing Accuracy:', 0.57666665)\n",
      "Step 325: loss = 826275.38 (24.338 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.56333333)\n",
      "Step 326: loss = 934518.31 (21.978 sec) ('Training Accuracy:', 0.86511803) ('Testing Accuracy:', 0.53666669)\n",
      "Step 327: loss = 771802.32 (21.900 sec) ('Training Accuracy:', 0.88047957) ('Testing Accuracy:', 0.52999997)\n",
      "Step 328: loss = 1678407.27 (19.368 sec) ('Training Accuracy:', 0.88759834) ('Testing Accuracy:', 0.56666666)\n",
      "Step 329: loss = 953371.66 (19.452 sec) ('Training Accuracy:', 0.92881227) ('Testing Accuracy:', 0.53666669)\n",
      "Step 330: loss = 509733.47 (19.291 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.57666665)\n",
      "Step 331: loss = 559819.01 (19.406 sec) ('Training Accuracy:', 0.9569127) ('Testing Accuracy:', 0.57333332)\n",
      "Step 332: loss = 404575.08 (19.268 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.57333332)\n",
      "Step 333: loss = 371172.84 (19.516 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.57999998)\n",
      "Step 334: loss = 365972.12 (19.094 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.58333331)\n",
      "Step 335: loss = 183500.40 (19.383 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.59666669)\n",
      "Step 336: loss = 279910.67 (19.178 sec) ('Training Accuracy:', 0.97564632) ('Testing Accuracy:', 0.60666668)\n",
      "Step 337: loss = 1240769.55 (19.587 sec) ('Training Accuracy:', 0.88572496) ('Testing Accuracy:', 0.55333334)\n",
      "Step 338: loss = 989629.76 (19.086 sec) ('Training Accuracy:', 0.93106031) ('Testing Accuracy:', 0.58333331)\n",
      "Step 339: loss = 1280432.30 (19.263 sec) ('Training Accuracy:', 0.90370923) ('Testing Accuracy:', 0.55666667)\n",
      "Step 340: loss = 1387839.50 (18.996 sec) ('Training Accuracy:', 0.90071189) ('Testing Accuracy:', 0.54333335)\n",
      "Step 341: loss = 959423.27 (19.474 sec) ('Training Accuracy:', 0.91869617) ('Testing Accuracy:', 0.56666666)\n",
      "Step 342: loss = 1519716.87 (19.637 sec) ('Training Accuracy:', 0.84526038) ('Testing Accuracy:', 0.53666669)\n",
      "Step 343: loss = 3547225.60 (18.827 sec) ('Training Accuracy:', 0.91982013) ('Testing Accuracy:', 0.56)\n",
      "Step 344: loss = 1385736.52 (19.333 sec) ('Training Accuracy:', 0.89321846) ('Testing Accuracy:', 0.55333334)\n",
      "Step 345: loss = 1317245.96 (19.108 sec) ('Training Accuracy:', 0.96065944) ('Testing Accuracy:', 0.5933333)\n",
      "Step 346: loss = 875414.17 (19.316 sec) ('Training Accuracy:', 0.96590483) ('Testing Accuracy:', 0.57666665)\n",
      "Step 347: loss = 653999.89 (19.715 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.54000002)\n",
      "Step 348: loss = 868965.49 (19.463 sec) ('Training Accuracy:', 0.90670663) ('Testing Accuracy:', 0.55333334)\n",
      "Step 349: loss = 1456880.22 (19.644 sec) ('Training Accuracy:', 0.95466465) ('Testing Accuracy:', 0.57999998)\n",
      "Step 350: loss = 656078.70 (18.695 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.58333331)\n",
      "Step 351: loss = 556967.87 (19.757 sec) ('Training Accuracy:', 0.96927691) ('Testing Accuracy:', 0.60666668)\n",
      "Step 352: loss = 844270.74 (19.391 sec) ('Training Accuracy:', 0.97489697) ('Testing Accuracy:', 0.58333331)\n",
      "Step 353: loss = 1381874.44 (19.163 sec) ('Training Accuracy:', 0.91719747) ('Testing Accuracy:', 0.54666668)\n",
      "Step 354: loss = 677886.80 (19.556 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.58666664)\n",
      "Step 355: loss = 529450.37 (19.139 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.57666665)\n",
      "Step 356: loss = 565118.02 (19.317 sec) ('Training Accuracy:', 0.97077554) ('Testing Accuracy:', 0.58666664)\n",
      "Step 357: loss = 708147.56 (19.624 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.59666669)\n",
      "Step 358: loss = 993169.12 (19.727 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.57333332)\n",
      "Step 359: loss = 359483.45 (19.692 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.58333331)\n",
      "Step 360: loss = 343091.67 (18.985 sec) ('Training Accuracy:', 0.94304985) ('Testing Accuracy:', 0.55000001)\n",
      "Step 361: loss = 406277.48 (19.424 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.58333331)\n",
      "Step 362: loss = 393048.93 (19.553 sec) ('Training Accuracy:', 0.96927691) ('Testing Accuracy:', 0.60000002)\n",
      "Step 363: loss = 296825.50 (19.632 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.55666667)\n",
      "Step 364: loss = 297763.74 (19.305 sec) ('Training Accuracy:', 0.94979393) ('Testing Accuracy:', 0.55000001)\n",
      "Step 365: loss = 499308.50 (19.229 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.59666669)\n",
      "Step 366: loss = 280085.76 (19.489 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.55666667)\n",
      "Step 367: loss = 468691.65 (19.122 sec) ('Training Accuracy:', 0.90558261) ('Testing Accuracy:', 0.53666669)\n",
      "Step 368: loss = 666324.14 (19.885 sec) ('Training Accuracy:', 0.91232669) ('Testing Accuracy:', 0.56)\n",
      "Step 369: loss = 1222028.23 (19.482 sec) ('Training Accuracy:', 0.93068564) ('Testing Accuracy:', 0.55333334)\n",
      "Step 370: loss = 648291.21 (19.313 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.55333334)\n",
      "Step 371: loss = 520332.42 (19.748 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.60000002)\n",
      "Step 372: loss = 305955.07 (19.639 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.56666666)\n",
      "Step 373: loss = 861091.16 (24.042 sec) ('Training Accuracy:', 0.93668038) ('Testing Accuracy:', 0.54666668)\n",
      "Step 374: loss = 3730714.13 (23.526 sec) ('Training Accuracy:', 0.89659047) ('Testing Accuracy:', 0.55333334)\n",
      "Step 375: loss = 957610.64 (23.061 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.55666667)\n",
      "Step 376: loss = 655158.06 (24.187 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.56666666)\n",
      "Step 377: loss = 764899.61 (23.325 sec) ('Training Accuracy:', 0.93705505) ('Testing Accuracy:', 0.60000002)\n",
      "Step 378: loss = 471086.57 (23.507 sec) ('Training Accuracy:', 0.92843759) ('Testing Accuracy:', 0.55000001)\n",
      "Step 379: loss = 678653.32 (23.421 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.62333333)\n",
      "Step 380: loss = 713180.54 (25.603 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.57333332)\n",
      "Step 381: loss = 325509.68 (20.549 sec) ('Training Accuracy:', 0.90820533) ('Testing Accuracy:', 0.56)\n",
      "Step 382: loss = 640644.85 (22.145 sec) ('Training Accuracy:', 0.9569127) ('Testing Accuracy:', 0.60666668)\n",
      "Step 383: loss = 363779.48 (21.699 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.57333332)\n",
      "Step 384: loss = 489012.46 (24.064 sec) ('Training Accuracy:', 0.97377294) ('Testing Accuracy:', 0.60666668)\n",
      "Step 385: loss = 963574.02 (18.816 sec) ('Training Accuracy:', 0.94866991) ('Testing Accuracy:', 0.60333335)\n",
      "Step 386: loss = 1004067.59 (19.453 sec) ('Training Accuracy:', 0.90932935) ('Testing Accuracy:', 0.56)\n",
      "Step 387: loss = 628198.40 (18.912 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.59666669)\n",
      "Step 388: loss = 266313.38 (18.957 sec) ('Training Accuracy:', 0.97264892) ('Testing Accuracy:', 0.58999997)\n",
      "Step 389: loss = 460146.56 (19.007 sec) ('Training Accuracy:', 0.96028477) ('Testing Accuracy:', 0.60000002)\n",
      "Step 390: loss = 530747.27 (19.141 sec) ('Training Accuracy:', 0.97677034) ('Testing Accuracy:', 0.56999999)\n",
      "Step 391: loss = 899848.55 (19.062 sec) ('Training Accuracy:', 0.97564632) ('Testing Accuracy:', 0.57999998)\n",
      "Step 392: loss = 963993.59 (19.000 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.57999998)\n",
      "Step 393: loss = 789662.77 (19.517 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.56333333)\n",
      "Step 394: loss = 858612.05 (19.278 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.57999998)\n",
      "Step 395: loss = 1295348.84 (18.886 sec) ('Training Accuracy:', 0.88797301) ('Testing Accuracy:', 0.55000001)\n",
      "Step 396: loss = 1624568.53 (18.704 sec) ('Training Accuracy:', 0.95054328) ('Testing Accuracy:', 0.57666665)\n",
      "Step 397: loss = 3438507.09 (19.355 sec) ('Training Accuracy:', 0.93518174) ('Testing Accuracy:', 0.57999998)\n",
      "Step 398: loss = 1092920.72 (18.850 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.55666667)\n",
      "Step 399: loss = 1040252.20 (18.965 sec) ('Training Accuracy:', 0.96965158) ('Testing Accuracy:', 0.57666665)\n",
      "Step 400: loss = 873784.67 (19.265 sec) ('Training Accuracy:', 0.93031096) ('Testing Accuracy:', 0.55000001)\n",
      "Step 401: loss = 601359.71 (19.245 sec) ('Training Accuracy:', 0.96852756) ('Testing Accuracy:', 0.58333331)\n",
      "Step 402: loss = 514614.92 (19.037 sec) ('Training Accuracy:', 0.91382539) ('Testing Accuracy:', 0.54666668)\n",
      "Step 403: loss = 780267.72 (19.220 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.58666664)\n",
      "Step 404: loss = 467166.28 (18.852 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.55333334)\n",
      "Step 405: loss = 911971.64 (19.347 sec) ('Training Accuracy:', 0.97751969) ('Testing Accuracy:', 0.58666664)\n",
      "Step 406: loss = 741138.77 (19.225 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.57999998)\n",
      "Step 407: loss = 951487.12 (19.058 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.60000002)\n",
      "Step 408: loss = 583725.32 (19.441 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.56666666)\n",
      "Step 409: loss = 438476.23 (19.013 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.59666669)\n",
      "Step 410: loss = 495430.66 (19.341 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.56)\n",
      "Step 411: loss = 449787.61 (19.191 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.58999997)\n",
      "Step 412: loss = 311049.72 (19.270 sec) ('Training Accuracy:', 0.91832149) ('Testing Accuracy:', 0.54666668)\n",
      "Step 413: loss = 721353.71 (18.983 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.57666665)\n",
      "Step 414: loss = 320671.58 (19.367 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.57666665)\n",
      "Step 415: loss = 501334.40 (19.128 sec) ('Training Accuracy:', 0.96253276) ('Testing Accuracy:', 0.58666664)\n",
      "Step 416: loss = 441209.00 (19.559 sec) ('Training Accuracy:', 0.96777821) ('Testing Accuracy:', 0.58999997)\n",
      "Step 417: loss = 576242.66 (20.233 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.61333334)\n",
      "Step 418: loss = 8111827.07 (19.083 sec) ('Training Accuracy:', 0.72461593) ('Testing Accuracy:', 0.50999999)\n",
      "Step 419: loss = 36788859.62 (18.995 sec) ('Training Accuracy:', 0.90146124) ('Testing Accuracy:', 0.61666667)\n",
      "Step 420: loss = 5711719.10 (19.147 sec) ('Training Accuracy:', 0.96103412) ('Testing Accuracy:', 0.57333332)\n",
      "Step 421: loss = 3226278.73 (18.819 sec) ('Training Accuracy:', 0.93518174) ('Testing Accuracy:', 0.57666665)\n",
      "Step 422: loss = 1268828.81 (18.954 sec) ('Training Accuracy:', 0.97152489) ('Testing Accuracy:', 0.57666665)\n",
      "Step 423: loss = 444991.80 (18.785 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.56333333)\n",
      "Step 424: loss = 494005.33 (18.954 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.55333334)\n",
      "Step 425: loss = 866440.68 (18.697 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.56666666)\n",
      "Step 426: loss = 535066.41 (18.667 sec) ('Training Accuracy:', 0.97489697) ('Testing Accuracy:', 0.59666669)\n",
      "Step 427: loss = 160649.31 (18.894 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.58333331)\n",
      "Step 428: loss = 401642.68 (19.005 sec) ('Training Accuracy:', 0.96627951) ('Testing Accuracy:', 0.56666666)\n",
      "Step 429: loss = 452435.31 (18.796 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.56333333)\n",
      "Step 430: loss = 582845.63 (19.005 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.58999997)\n",
      "Step 431: loss = 537027.00 (18.732 sec) ('Training Accuracy:', 0.96403146) ('Testing Accuracy:', 0.57666665)\n",
      "Step 432: loss = 402189.07 (19.262 sec) ('Training Accuracy:', 0.95279133) ('Testing Accuracy:', 0.56333333)\n",
      "Step 433: loss = 642207.19 (19.011 sec) ('Training Accuracy:', 0.96065944) ('Testing Accuracy:', 0.56333333)\n",
      "Step 434: loss = 235278.59 (19.143 sec) ('Training Accuracy:', 0.96665418) ('Testing Accuracy:', 0.57999998)\n",
      "Step 435: loss = 167250.43 (18.835 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.57333332)\n",
      "Step 436: loss = 365816.99 (18.893 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.56666666)\n",
      "Step 437: loss = 333363.16 (19.020 sec) ('Training Accuracy:', 0.97264892) ('Testing Accuracy:', 0.57999998)\n",
      "Step 438: loss = 131672.05 (18.906 sec) ('Training Accuracy:', 0.97564632) ('Testing Accuracy:', 0.57999998)\n",
      "Step 439: loss = 108510.65 (19.167 sec) ('Training Accuracy:', 0.97602099) ('Testing Accuracy:', 0.57333332)\n",
      "Step 440: loss = 285900.48 (19.373 sec) ('Training Accuracy:', 0.95578867) ('Testing Accuracy:', 0.56666666)\n",
      "Step 441: loss = 423199.09 (18.634 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.56666666)\n",
      "Step 442: loss = 308870.00 (19.156 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.56999999)\n",
      "Step 443: loss = 321266.15 (19.133 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.55666667)\n",
      "Step 444: loss = 518270.08 (19.225 sec) ('Training Accuracy:', 0.97152489) ('Testing Accuracy:', 0.56999999)\n",
      "Step 445: loss = 129279.09 (18.439 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.61000001)\n",
      "Step 446: loss = 160710.94 (19.227 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.56666666)\n",
      "Step 447: loss = 524659.44 (19.104 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.58666664)\n",
      "Step 448: loss = 199309.45 (19.101 sec) ('Training Accuracy:', 0.97414762) ('Testing Accuracy:', 0.60333335)\n",
      "Step 449: loss = 285235.94 (18.731 sec) ('Training Accuracy:', 0.97489697) ('Testing Accuracy:', 0.57999998)\n",
      "Step 450: loss = 578254.50 (18.786 sec) ('Training Accuracy:', 0.97527164) ('Testing Accuracy:', 0.5933333)\n",
      "Step 451: loss = 559414.16 (19.206 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.56)\n",
      "Step 452: loss = 1093991.04 (18.754 sec) ('Training Accuracy:', 0.87073809) ('Testing Accuracy:', 0.52666664)\n",
      "Step 453: loss = 989997.44 (20.057 sec) ('Training Accuracy:', 0.92693895) ('Testing Accuracy:', 0.54333335)\n",
      "Step 454: loss = 1006533.07 (19.430 sec) ('Training Accuracy:', 0.93668038) ('Testing Accuracy:', 0.55666667)\n",
      "Step 455: loss = 956959.55 (18.931 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.55666667)\n",
      "Step 456: loss = 471455.00 (19.784 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.56666666)\n",
      "Step 457: loss = 260920.55 (19.644 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.58666664)\n",
      "Step 458: loss = 491269.73 (20.151 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.55666667)\n",
      "Step 459: loss = 252419.92 (20.210 sec) ('Training Accuracy:', 0.97002625) ('Testing Accuracy:', 0.58333331)\n",
      "Step 460: loss = 229969.13 (19.813 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.56999999)\n",
      "Step 461: loss = 203182.23 (20.559 sec) ('Training Accuracy:', 0.96103412) ('Testing Accuracy:', 0.56)\n",
      "Step 462: loss = 542953.42 (20.868 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.59666669)\n",
      "Step 463: loss = 212298.90 (20.785 sec) ('Training Accuracy:', 0.94604719) ('Testing Accuracy:', 0.56333333)\n",
      "Step 464: loss = 917311.89 (20.918 sec) ('Training Accuracy:', 0.89883852) ('Testing Accuracy:', 0.53333336)\n",
      "Step 465: loss = 850307.66 (21.390 sec) ('Training Accuracy:', 0.91420007) ('Testing Accuracy:', 0.55333334)\n",
      "Step 466: loss = 1023736.55 (20.634 sec) ('Training Accuracy:', 0.92056948) ('Testing Accuracy:', 0.55666667)\n",
      "Step 467: loss = 2692151.47 (20.332 sec) ('Training Accuracy:', 0.96253276) ('Testing Accuracy:', 0.56999999)\n",
      "Step 468: loss = 2562172.32 (20.042 sec) ('Training Accuracy:', 0.9216935) ('Testing Accuracy:', 0.56333333)\n",
      "Step 469: loss = 5213046.23 (20.120 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.5933333)\n",
      "Step 470: loss = 3553616.74 (19.727 sec) ('Training Accuracy:', 0.93293369) ('Testing Accuracy:', 0.54666668)\n",
      "Step 471: loss = 5209717.74 (19.221 sec) ('Training Accuracy:', 0.92056948) ('Testing Accuracy:', 0.55666667)\n",
      "Step 472: loss = 2984381.06 (19.130 sec) ('Training Accuracy:', 0.84526038) ('Testing Accuracy:', 0.53666669)\n",
      "Step 473: loss = 17806772.37 (19.539 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.61666667)\n",
      "Step 474: loss = 12608122.61 (19.368 sec) ('Training Accuracy:', 0.75533909) ('Testing Accuracy:', 0.51666665)\n",
      "Step 475: loss = 12246604.77 (19.572 sec) ('Training Accuracy:', 0.87223679) ('Testing Accuracy:', 0.56333333)\n",
      "Step 476: loss = 21480273.67 (19.536 sec) ('Training Accuracy:', 0.9231922) ('Testing Accuracy:', 0.62666667)\n",
      "Step 477: loss = 6581888.50 (19.569 sec) ('Training Accuracy:', 0.90408391) ('Testing Accuracy:', 0.56)\n",
      "Step 478: loss = 1674687.59 (19.160 sec) ('Training Accuracy:', 0.97002625) ('Testing Accuracy:', 0.56333333)\n",
      "Step 479: loss = 1202672.82 (19.061 sec) ('Training Accuracy:', 0.97527164) ('Testing Accuracy:', 0.55666667)\n",
      "Step 480: loss = 1109854.22 (19.209 sec) ('Training Accuracy:', 0.97002625) ('Testing Accuracy:', 0.56999999)\n",
      "Step 481: loss = 276510.97 (18.741 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.57333332)\n",
      "Step 482: loss = 355774.39 (19.380 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.56999999)\n",
      "Step 483: loss = 188909.28 (18.632 sec) ('Training Accuracy:', 0.96852756) ('Testing Accuracy:', 0.58666664)\n",
      "Step 484: loss = 167545.79 (19.392 sec) ('Training Accuracy:', 0.96403146) ('Testing Accuracy:', 0.57999998)\n",
      "Step 485: loss = 368611.26 (19.139 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.57666665)\n",
      "Step 486: loss = 477897.59 (18.811 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.54333335)\n",
      "Step 487: loss = 637223.26 (19.284 sec) ('Training Accuracy:', 0.94604719) ('Testing Accuracy:', 0.54666668)\n",
      "Step 488: loss = 894284.56 (19.041 sec) ('Training Accuracy:', 0.93668038) ('Testing Accuracy:', 0.54000002)\n",
      "Step 489: loss = 693407.72 (19.652 sec) ('Training Accuracy:', 0.9201948) ('Testing Accuracy:', 0.54000002)\n",
      "Step 490: loss = 810623.46 (18.930 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.56999999)\n",
      "Step 491: loss = 463959.88 (18.999 sec) ('Training Accuracy:', 0.95129263) ('Testing Accuracy:', 0.55333334)\n",
      "Step 492: loss = 854348.97 (19.008 sec) ('Training Accuracy:', 0.90708131) ('Testing Accuracy:', 0.55666667)\n",
      "Step 493: loss = 1419203.46 (19.083 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.56)\n",
      "Step 494: loss = 877289.20 (19.477 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.56333333)\n",
      "Step 495: loss = 457857.19 (19.283 sec) ('Training Accuracy:', 0.95728737) ('Testing Accuracy:', 0.57999998)\n",
      "Step 496: loss = 313340.03 (18.926 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.57666665)\n",
      "Step 497: loss = 414592.71 (19.163 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.57333332)\n",
      "Step 498: loss = 643465.54 (19.043 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.56999999)\n",
      "Step 499: loss = 733003.79 (19.234 sec) ('Training Accuracy:', 0.93705505) ('Testing Accuracy:', 0.56999999)\n",
      "Step 500: loss = 1248046.27 (18.877 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.56)\n",
      "Step 501: loss = 529425.42 (19.129 sec) ('Training Accuracy:', 0.93068564) ('Testing Accuracy:', 0.55666667)\n",
      "Step 502: loss = 506171.52 (19.418 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.56666666)\n",
      "Step 503: loss = 581678.24 (19.125 sec) ('Training Accuracy:', 0.90633196) ('Testing Accuracy:', 0.55333334)\n",
      "Step 504: loss = 724136.99 (19.352 sec) ('Training Accuracy:', 0.89958787) ('Testing Accuracy:', 0.54333335)\n",
      "Step 505: loss = 1116952.96 (18.898 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.56)\n",
      "Step 506: loss = 1077193.89 (19.593 sec) ('Training Accuracy:', 0.91757214) ('Testing Accuracy:', 0.56)\n",
      "Step 507: loss = 931108.88 (18.815 sec) ('Training Accuracy:', 0.90445858) ('Testing Accuracy:', 0.56)\n",
      "Step 508: loss = 783305.91 (19.338 sec) ('Training Accuracy:', 0.9276883) ('Testing Accuracy:', 0.56999999)\n",
      "Step 509: loss = 1287758.25 (18.919 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.57333332)\n",
      "Step 510: loss = 439491.52 (19.044 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.56)\n",
      "Step 511: loss = 890519.02 (19.126 sec) ('Training Accuracy:', 0.91157734) ('Testing Accuracy:', 0.55333334)\n",
      "Step 512: loss = 1156475.36 (19.142 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.55666667)\n",
      "Step 513: loss = 464505.78 (19.835 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.56)\n",
      "Step 514: loss = 648137.28 (20.001 sec) ('Training Accuracy:', 0.91982013) ('Testing Accuracy:', 0.56333333)\n",
      "Step 515: loss = 1198233.61 (20.096 sec) ('Training Accuracy:', 0.96965158) ('Testing Accuracy:', 0.56999999)\n",
      "Step 516: loss = 1030794.61 (20.501 sec) ('Training Accuracy:', 0.97714502) ('Testing Accuracy:', 0.5933333)\n",
      "Step 517: loss = 1316629.21 (20.443 sec) ('Training Accuracy:', 0.95728737) ('Testing Accuracy:', 0.54000002)\n",
      "Step 518: loss = 677498.73 (20.824 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.55333334)\n",
      "Step 519: loss = 712937.71 (20.583 sec) ('Training Accuracy:', 0.96965158) ('Testing Accuracy:', 0.56666666)\n",
      "Step 520: loss = 542259.32 (20.575 sec) ('Training Accuracy:', 0.97377294) ('Testing Accuracy:', 0.56999999)\n",
      "Step 521: loss = 457864.32 (19.951 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.54666668)\n",
      "Step 522: loss = 558957.34 (20.509 sec) ('Training Accuracy:', 0.93668038) ('Testing Accuracy:', 0.55000001)\n",
      "Step 523: loss = 787281.96 (25.483 sec) ('Training Accuracy:', 0.90970403) ('Testing Accuracy:', 0.53666669)\n",
      "Step 524: loss = 665727.26 (24.681 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.55666667)\n",
      "Step 525: loss = 549171.57 (24.879 sec) ('Training Accuracy:', 0.87710756) ('Testing Accuracy:', 0.54666668)\n",
      "Step 526: loss = 926213.81 (22.501 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.60333335)\n",
      "Step 527: loss = 901695.34 (21.986 sec) ('Training Accuracy:', 0.97264892) ('Testing Accuracy:', 0.55666667)\n",
      "Step 528: loss = 767663.63 (24.713 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.55000001)\n",
      "Step 529: loss = 611848.54 (22.650 sec) ('Training Accuracy:', 0.97639567) ('Testing Accuracy:', 0.58333331)\n",
      "Step 530: loss = 990816.85 (23.342 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.55000001)\n",
      "Step 531: loss = 1122284.87 (21.725 sec) ('Training Accuracy:', 0.92544025) ('Testing Accuracy:', 0.55333334)\n",
      "Step 532: loss = 652422.82 (21.806 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.55000001)\n",
      "Step 533: loss = 626402.92 (23.407 sec) ('Training Accuracy:', 0.94717121) ('Testing Accuracy:', 0.55666667)\n",
      "Step 534: loss = 661281.13 (23.041 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.54000002)\n",
      "Step 535: loss = 492912.13 (24.455 sec) ('Training Accuracy:', 0.92843759) ('Testing Accuracy:', 0.55333334)\n",
      "Step 536: loss = 573986.76 (18.459 sec) ('Training Accuracy:', 0.93405771) ('Testing Accuracy:', 0.54333335)\n",
      "Step 537: loss = 759771.28 (19.713 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.54333335)\n",
      "Step 538: loss = 416368.12 (19.623 sec) ('Training Accuracy:', 0.92131883) ('Testing Accuracy:', 0.53333336)\n",
      "Step 539: loss = 654735.53 (20.508 sec) ('Training Accuracy:', 0.93705505) ('Testing Accuracy:', 0.53333336)\n",
      "Step 540: loss = 671546.53 (19.138 sec) ('Training Accuracy:', 0.91045338) ('Testing Accuracy:', 0.53333336)\n",
      "Step 541: loss = 695624.89 (19.334 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.57333332)\n",
      "Step 542: loss = 860072.22 (19.116 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.55666667)\n",
      "Step 543: loss = 1098139.81 (19.293 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.54666668)\n",
      "Step 544: loss = 539632.23 (19.499 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.57333332)\n",
      "Step 545: loss = 436194.15 (18.991 sec) ('Training Accuracy:', 0.94304985) ('Testing Accuracy:', 0.57999998)\n",
      "Step 546: loss = 355845.07 (19.204 sec) ('Training Accuracy:', 0.94866991) ('Testing Accuracy:', 0.55666667)\n",
      "Step 547: loss = 514512.09 (19.692 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.61333334)\n",
      "Step 548: loss = 199163.35 (19.394 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.58333331)\n",
      "Step 549: loss = 548881.95 (19.336 sec) ('Training Accuracy:', 0.94642186) ('Testing Accuracy:', 0.55666667)\n",
      "Step 550: loss = 317810.74 (18.986 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.56666666)\n",
      "Step 551: loss = 354835.32 (19.311 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.57666665)\n",
      "Step 552: loss = 316754.93 (19.697 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.55666667)\n",
      "Step 553: loss = 474043.45 (19.784 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.56333333)\n",
      "Step 554: loss = 448501.90 (19.267 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.55666667)\n",
      "Step 555: loss = 350076.86 (19.020 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.55333334)\n",
      "Step 556: loss = 486492.92 (19.334 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.58666664)\n",
      "Step 557: loss = 272609.24 (19.043 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.55000001)\n",
      "Step 558: loss = 576911.19 (19.624 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.58333331)\n",
      "Step 559: loss = 400540.48 (19.389 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.55333334)\n",
      "Step 560: loss = 261010.19 (19.579 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.57999998)\n",
      "Step 561: loss = 502106.48 (19.176 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.57333332)\n",
      "Step 562: loss = 604618.64 (19.667 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.56999999)\n",
      "Step 563: loss = 962802.25 (19.571 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.56666666)\n",
      "Step 564: loss = 550002.04 (19.260 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.56999999)\n",
      "Step 565: loss = 407361.03 (19.256 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.57333332)\n",
      "Step 566: loss = 520512.54 (19.553 sec) ('Training Accuracy:', 0.91907084) ('Testing Accuracy:', 0.54333335)\n",
      "Step 567: loss = 690529.88 (19.358 sec) ('Training Accuracy:', 0.9261896) ('Testing Accuracy:', 0.55333334)\n",
      "Step 568: loss = 904621.29 (19.826 sec) ('Training Accuracy:', 0.87148744) ('Testing Accuracy:', 0.55000001)\n",
      "Step 569: loss = 785262.43 (19.517 sec) ('Training Accuracy:', 0.93668038) ('Testing Accuracy:', 0.56666666)\n",
      "Step 570: loss = 706672.84 (19.191 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.57666665)\n",
      "Step 571: loss = 951369.16 (20.187 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.56999999)\n",
      "Step 572: loss = 439585.05 (19.309 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.55666667)\n",
      "Step 573: loss = 248097.98 (19.889 sec) ('Training Accuracy:', 0.9261896) ('Testing Accuracy:', 0.56333333)\n",
      "Step 574: loss = 1040029.95 (19.267 sec) ('Training Accuracy:', 0.92731363) ('Testing Accuracy:', 0.56999999)\n",
      "Step 575: loss = 874416.51 (19.569 sec) ('Training Accuracy:', 0.91382539) ('Testing Accuracy:', 0.54666668)\n",
      "Step 576: loss = 755725.91 (19.349 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.56333333)\n",
      "Step 577: loss = 333058.73 (19.309 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.56)\n",
      "Step 578: loss = 601937.80 (19.024 sec) ('Training Accuracy:', 0.94454855) ('Testing Accuracy:', 0.57666665)\n",
      "Step 579: loss = 722746.05 (18.810 sec) ('Training Accuracy:', 0.93180966) ('Testing Accuracy:', 0.56666666)\n",
      "Step 580: loss = 649476.14 (19.501 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.56)\n",
      "Step 581: loss = 524050.54 (19.352 sec) ('Training Accuracy:', 0.88722366) ('Testing Accuracy:', 0.57666665)\n",
      "Step 582: loss = 648687.48 (19.771 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.58999997)\n",
      "Step 583: loss = 516938.87 (19.319 sec) ('Training Accuracy:', 0.93405771) ('Testing Accuracy:', 0.56999999)\n",
      "Step 584: loss = 1303795.26 (19.311 sec) ('Training Accuracy:', 0.92544025) ('Testing Accuracy:', 0.56333333)\n",
      "Step 585: loss = 782971.81 (19.231 sec) ('Training Accuracy:', 0.93293369) ('Testing Accuracy:', 0.56333333)\n",
      "Step 586: loss = 1008747.66 (19.080 sec) ('Training Accuracy:', 0.8816036) ('Testing Accuracy:', 0.53333336)\n",
      "Step 587: loss = 3216989.13 (19.404 sec) ('Training Accuracy:', 0.83626825) ('Testing Accuracy:', 0.55666667)\n",
      "Step 588: loss = 1662080.92 (19.254 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.55000001)\n",
      "Step 589: loss = 1413261.02 (18.802 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.56333333)\n",
      "Step 590: loss = 724865.50 (18.844 sec) ('Training Accuracy:', 0.96777821) ('Testing Accuracy:', 0.57666665)\n",
      "Step 591: loss = 448772.30 (19.320 sec) ('Training Accuracy:', 0.93443239) ('Testing Accuracy:', 0.58666664)\n",
      "Step 592: loss = 797700.02 (19.214 sec) ('Training Accuracy:', 0.90295988) ('Testing Accuracy:', 0.55000001)\n",
      "Step 593: loss = 1529566.32 (19.232 sec) ('Training Accuracy:', 0.96740353) ('Testing Accuracy:', 0.60666668)\n",
      "Step 594: loss = 686185.04 (19.934 sec) ('Training Accuracy:', 0.92356688) ('Testing Accuracy:', 0.55000001)\n",
      "Step 595: loss = 656186.81 (19.492 sec) ('Training Accuracy:', 0.94005245) ('Testing Accuracy:', 0.56)\n",
      "Step 596: loss = 1046219.83 (19.035 sec) ('Training Accuracy:', 0.95878607) ('Testing Accuracy:', 0.56999999)\n",
      "Step 597: loss = 613938.43 (19.414 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.58333331)\n",
      "Step 598: loss = 508403.96 (19.121 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.57333332)\n",
      "Step 599: loss = 566591.49 (19.176 sec) ('Training Accuracy:', 0.95878607) ('Testing Accuracy:', 0.5933333)\n",
      "Step 600: loss = 448650.65 (18.547 sec) ('Training Accuracy:', 0.8947171) ('Testing Accuracy:', 0.55333334)\n",
      "Step 601: loss = 789707.64 (19.402 sec) ('Training Accuracy:', 0.94117647) ('Testing Accuracy:', 0.57333332)\n",
      "Step 602: loss = 624934.54 (19.223 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.61000001)\n",
      "Step 603: loss = 226029.17 (18.978 sec) ('Training Accuracy:', 0.96065944) ('Testing Accuracy:', 0.58333331)\n",
      "Step 604: loss = 433831.04 (19.422 sec) ('Training Accuracy:', 0.93106031) ('Testing Accuracy:', 0.56999999)\n",
      "Step 605: loss = 611305.67 (19.222 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.5933333)\n",
      "Step 606: loss = 1276424.60 (19.346 sec) ('Training Accuracy:', 0.9408018) ('Testing Accuracy:', 0.57666665)\n",
      "Step 607: loss = 565404.46 (18.095 sec) ('Training Accuracy:', 0.96890223) ('Testing Accuracy:', 0.56999999)\n",
      "Step 608: loss = 450392.04 (19.171 sec) ('Training Accuracy:', 0.89321846) ('Testing Accuracy:', 0.56333333)\n",
      "Step 609: loss = 961504.70 (19.062 sec) ('Training Accuracy:', 0.94192582) ('Testing Accuracy:', 0.57666665)\n",
      "Step 610: loss = 574392.20 (19.124 sec) ('Training Accuracy:', 0.9216935) ('Testing Accuracy:', 0.57333332)\n",
      "Step 611: loss = 886699.46 (19.299 sec) ('Training Accuracy:', 0.91082805) ('Testing Accuracy:', 0.55666667)\n",
      "Step 612: loss = 1192621.12 (19.474 sec) ('Training Accuracy:', 0.9231922) ('Testing Accuracy:', 0.58333331)\n",
      "Step 613: loss = 814387.89 (19.475 sec) ('Training Accuracy:', 0.84750843) ('Testing Accuracy:', 0.52999997)\n",
      "Step 614: loss = 1193902.43 (18.963 sec) ('Training Accuracy:', 0.95054328) ('Testing Accuracy:', 0.57666665)\n",
      "Step 615: loss = 469523.73 (19.002 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.57666665)\n",
      "Step 616: loss = 460720.16 (18.650 sec) ('Training Accuracy:', 0.92544025) ('Testing Accuracy:', 0.56333333)\n",
      "Step 617: loss = 754228.31 (19.055 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.57666665)\n",
      "Step 618: loss = 472556.86 (19.269 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.56666666)\n",
      "Step 619: loss = 702023.73 (19.279 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.56)\n",
      "Step 620: loss = 335883.58 (19.312 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.58333331)\n",
      "Step 621: loss = 739894.54 (19.051 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.56999999)\n",
      "Step 622: loss = 1122739.12 (19.073 sec) ('Training Accuracy:', 0.91607344) ('Testing Accuracy:', 0.60000002)\n",
      "Step 623: loss = 1030411.06 (19.072 sec) ('Training Accuracy:', 0.92918694) ('Testing Accuracy:', 0.56999999)\n",
      "Step 624: loss = 1061627.49 (19.037 sec) ('Training Accuracy:', 0.90633196) ('Testing Accuracy:', 0.57666665)\n",
      "Step 625: loss = 2734303.37 (18.851 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.60333335)\n",
      "Step 626: loss = 1596084.38 (19.489 sec) ('Training Accuracy:', 0.94042712) ('Testing Accuracy:', 0.64666665)\n",
      "Step 627: loss = 2522245.45 (18.816 sec) ('Training Accuracy:', 0.96403146) ('Testing Accuracy:', 0.58666664)\n",
      "Step 628: loss = 2063735.57 (18.680 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.58333331)\n",
      "Step 629: loss = 3059568.50 (18.570 sec) ('Training Accuracy:', 0.96627951) ('Testing Accuracy:', 0.60333335)\n",
      "Step 630: loss = 9611176.91 (19.102 sec) ('Training Accuracy:', 0.86998874) ('Testing Accuracy:', 0.56666666)\n",
      "Step 631: loss = 4127130.77 (19.436 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.62333333)\n",
      "Step 632: loss = 1334706.35 (18.871 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.58666664)\n",
      "Step 633: loss = 802493.63 (18.710 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.56999999)\n",
      "Step 634: loss = 602553.92 (18.943 sec) ('Training Accuracy:', 0.8947171) ('Testing Accuracy:', 0.54666668)\n",
      "Step 635: loss = 693891.10 (18.816 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.58666664)\n",
      "Step 636: loss = 523260.58 (18.989 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.58666664)\n",
      "Step 637: loss = 466842.89 (18.034 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.56)\n",
      "Step 638: loss = 471540.96 (18.709 sec) ('Training Accuracy:', 0.96065944) ('Testing Accuracy:', 0.60000002)\n",
      "Step 639: loss = 348624.44 (19.035 sec) ('Training Accuracy:', 0.95016861) ('Testing Accuracy:', 0.5933333)\n",
      "Step 640: loss = 540497.87 (19.063 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.5933333)\n",
      "Step 641: loss = 215136.16 (18.163 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.60000002)\n",
      "Step 642: loss = 281170.47 (18.956 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.58999997)\n",
      "Step 643: loss = 263084.12 (18.980 sec) ('Training Accuracy:', 0.97527164) ('Testing Accuracy:', 0.60333335)\n",
      "Step 644: loss = 91631.38 (18.594 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.60666668)\n",
      "Step 645: loss = 593035.17 (19.035 sec) ('Training Accuracy:', 0.90558261) ('Testing Accuracy:', 0.56666666)\n",
      "Step 646: loss = 953088.45 (18.960 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.57666665)\n",
      "Step 647: loss = 833800.94 (18.674 sec) ('Training Accuracy:', 0.97639567) ('Testing Accuracy:', 0.61000001)\n",
      "Step 648: loss = 628186.12 (19.053 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.58999997)\n",
      "Step 649: loss = 437868.32 (18.930 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.57666665)\n",
      "Step 650: loss = 420058.08 (19.136 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.57999998)\n",
      "Step 651: loss = 3107110.56 (18.464 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.60000002)\n",
      "Step 652: loss = 2377390.75 (18.764 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.56333333)\n",
      "Step 653: loss = 1569104.18 (18.677 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.58333331)\n",
      "Step 654: loss = 539655.21 (18.502 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.57666665)\n",
      "Step 655: loss = 498429.18 (19.188 sec) ('Training Accuracy:', 0.96665418) ('Testing Accuracy:', 0.59666669)\n",
      "Step 656: loss = 612586.67 (18.874 sec) ('Training Accuracy:', 0.96965158) ('Testing Accuracy:', 0.62666667)\n",
      "Step 657: loss = 1841519.77 (19.292 sec) ('Training Accuracy:', 0.92956161) ('Testing Accuracy:', 0.57333332)\n",
      "Step 658: loss = 350425.77 (19.126 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.58666664)\n",
      "Step 659: loss = 285051.71 (18.819 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.57333332)\n",
      "Step 660: loss = 1514821.93 (19.270 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.60333335)\n",
      "Step 661: loss = 336514.31 (18.730 sec) ('Training Accuracy:', 0.96140879) ('Testing Accuracy:', 0.55666667)\n",
      "Step 662: loss = 399644.12 (18.994 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.58333331)\n",
      "Step 663: loss = 981689.68 (18.645 sec) ('Training Accuracy:', 0.90783066) ('Testing Accuracy:', 0.56666666)\n",
      "Step 664: loss = 667019.94 (18.582 sec) ('Training Accuracy:', 0.96140879) ('Testing Accuracy:', 0.5933333)\n",
      "Step 665: loss = 247754.20 (19.273 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.5933333)\n",
      "Step 666: loss = 251479.40 (19.213 sec) ('Training Accuracy:', 0.97602099) ('Testing Accuracy:', 0.58999997)\n",
      "Step 667: loss = 387519.28 (19.264 sec) ('Training Accuracy:', 0.95616335) ('Testing Accuracy:', 0.57999998)\n",
      "Step 668: loss = 412643.29 (18.722 sec) ('Training Accuracy:', 0.96065944) ('Testing Accuracy:', 0.58666664)\n",
      "Step 669: loss = 493661.70 (19.374 sec) ('Training Accuracy:', 0.94492316) ('Testing Accuracy:', 0.57999998)\n",
      "Step 670: loss = 715654.78 (19.640 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.57999998)\n",
      "Step 671: loss = 515671.13 (19.159 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.58999997)\n",
      "Step 672: loss = 262682.72 (19.105 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.57999998)\n",
      "Step 673: loss = 219926.78 (18.976 sec) ('Training Accuracy:', 0.96028477) ('Testing Accuracy:', 0.58666664)\n",
      "Step 674: loss = 442611.48 (19.098 sec) ('Training Accuracy:', 0.94304985) ('Testing Accuracy:', 0.57999998)\n",
      "Step 675: loss = 380220.77 (19.606 sec) ('Training Accuracy:', 0.97377294) ('Testing Accuracy:', 0.5933333)\n",
      "Step 676: loss = 269309.39 (19.268 sec) ('Training Accuracy:', 0.97789437) ('Testing Accuracy:', 0.58666664)\n",
      "Step 677: loss = 383575.25 (19.152 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.5933333)\n",
      "Step 678: loss = 166858.72 (18.645 sec) ('Training Accuracy:', 0.97677034) ('Testing Accuracy:', 0.57999998)\n",
      "Step 679: loss = 363467.29 (19.304 sec) ('Training Accuracy:', 0.96553016) ('Testing Accuracy:', 0.59666669)\n",
      "Step 680: loss = 130901.32 (19.378 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.58333331)\n",
      "Step 681: loss = 969032.61 (18.965 sec) ('Training Accuracy:', 0.90483326) ('Testing Accuracy:', 0.5933333)\n",
      "Step 682: loss = 1345345.28 (19.364 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.56333333)\n",
      "Step 683: loss = 488787.77 (19.143 sec) ('Training Accuracy:', 0.94492316) ('Testing Accuracy:', 0.57999998)\n",
      "Step 684: loss = 1109797.03 (19.660 sec) ('Training Accuracy:', 0.97751969) ('Testing Accuracy:', 0.61000001)\n",
      "Step 685: loss = 199939.59 (19.334 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.57333332)\n",
      "Step 686: loss = 544952.15 (19.031 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.56999999)\n",
      "Step 687: loss = 887155.49 (54.917 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.55666667)\n",
      "Step 688: loss = 578995.61 (62.659 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.5933333)\n",
      "Step 689: loss = 353138.99 (63.104 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.58999997)\n",
      "Step 690: loss = 395309.83 (76.293 sec) ('Training Accuracy:', 0.94492316) ('Testing Accuracy:', 0.57333332)\n",
      "Step 691: loss = 527019.50 (63.849 sec) ('Training Accuracy:', 0.92431623) ('Testing Accuracy:', 0.56999999)\n",
      "Step 692: loss = 702364.89 (64.087 sec) ('Training Accuracy:', 0.92356688) ('Testing Accuracy:', 0.57333332)\n",
      "Step 693: loss = 1235957.95 (20.415 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.5933333)\n",
      "Step 694: loss = 488613.97 (18.355 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.58666664)\n",
      "Step 695: loss = 438935.59 (20.070 sec) ('Training Accuracy:', 0.9408018) ('Testing Accuracy:', 0.57666665)\n",
      "Step 696: loss = 525060.23 (20.400 sec) ('Training Accuracy:', 0.9569127) ('Testing Accuracy:', 0.60000002)\n",
      "Step 697: loss = 446188.91 (20.789 sec) ('Training Accuracy:', 0.97377294) ('Testing Accuracy:', 0.60333335)\n",
      "Step 698: loss = 498349.39 (20.737 sec) ('Training Accuracy:', 0.91757214) ('Testing Accuracy:', 0.57666665)\n",
      "Step 699: loss = 882155.93 (20.753 sec) ('Training Accuracy:', 0.86062193) ('Testing Accuracy:', 0.59666669)\n",
      "Step 700: loss = 1407696.75 (21.602 sec) ('Training Accuracy:', 0.85987264) ('Testing Accuracy:', 0.57999998)\n",
      "Step 701: loss = 5561970.37 (20.276 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.61666667)\n",
      "Step 702: loss = 1863965.64 (20.170 sec) ('Training Accuracy:', 0.82053202) ('Testing Accuracy:', 0.52999997)\n",
      "Step 703: loss = 3238706.04 (19.562 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.58333331)\n",
      "Step 704: loss = 1701089.11 (20.130 sec) ('Training Accuracy:', 0.85237914) ('Testing Accuracy:', 0.55000001)\n",
      "Step 705: loss = 3093093.82 (24.072 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.56333333)\n",
      "Step 706: loss = 5965874.66 (24.147 sec) ('Training Accuracy:', 0.86324465) ('Testing Accuracy:', 0.57666665)\n",
      "Step 707: loss = 7630970.27 (24.033 sec) ('Training Accuracy:', 0.97527164) ('Testing Accuracy:', 0.60666668)\n",
      "Step 708: loss = 10297302.69 (23.773 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.59666669)\n",
      "Step 709: loss = 17502744.17 (23.332 sec) ('Training Accuracy:', 0.84863245) ('Testing Accuracy:', 0.65333331)\n",
      "Step 710: loss = 17072267.58 (23.615 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.58666664)\n",
      "Step 711: loss = 3462656.47 (23.681 sec) ('Training Accuracy:', 0.97527164) ('Testing Accuracy:', 0.62333333)\n",
      "Step 712: loss = 638935.39 (23.619 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.60666668)\n",
      "Step 713: loss = 358155.12 (23.668 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.61000001)\n",
      "Step 714: loss = 301704.38 (24.138 sec) ('Training Accuracy:', 0.97564632) ('Testing Accuracy:', 0.62333333)\n",
      "Step 715: loss = 92300.77 (23.600 sec) ('Training Accuracy:', 0.97414762) ('Testing Accuracy:', 0.63333333)\n",
      "Step 716: loss = 115064.45 (22.621 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.60333335)\n",
      "Step 717: loss = 242489.74 (23.556 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.5933333)\n",
      "Step 718: loss = 272572.92 (23.960 sec) ('Training Accuracy:', 0.9569127) ('Testing Accuracy:', 0.57666665)\n",
      "Step 719: loss = 482054.10 (24.005 sec) ('Training Accuracy:', 0.95466465) ('Testing Accuracy:', 0.57666665)\n",
      "Step 720: loss = 449090.19 (23.805 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.57333332)\n",
      "Step 721: loss = 691287.07 (24.092 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.58666664)\n",
      "Step 722: loss = 144324.34 (24.315 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.57333332)\n",
      "Step 723: loss = 364054.14 (24.325 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.56666666)\n",
      "Step 724: loss = 973844.66 (24.017 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.58999997)\n",
      "Step 725: loss = 186069.90 (23.950 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.59666669)\n",
      "Step 726: loss = 303176.52 (23.606 sec) ('Training Accuracy:', 0.93330836) ('Testing Accuracy:', 0.57999998)\n",
      "Step 727: loss = 808084.89 (24.072 sec) ('Training Accuracy:', 0.91120267) ('Testing Accuracy:', 0.56333333)\n",
      "Step 728: loss = 2613758.78 (23.826 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.56666666)\n",
      "Step 729: loss = 3219129.48 (24.220 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.5933333)\n",
      "Step 730: loss = 3381204.45 (23.770 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.57333332)\n",
      "Step 731: loss = 3457847.05 (23.876 sec) ('Training Accuracy:', 0.97751969) ('Testing Accuracy:', 0.62)\n",
      "Step 732: loss = 4140097.09 (23.954 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.57666665)\n",
      "Step 733: loss = 2521914.82 (24.321 sec) ('Training Accuracy:', 0.96103412) ('Testing Accuracy:', 0.62333333)\n",
      "Step 734: loss = 3321219.23 (23.802 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.66666669)\n",
      "Step 735: loss = 3534369.88 (23.884 sec) ('Training Accuracy:', 0.95728737) ('Testing Accuracy:', 0.63666666)\n",
      "Step 736: loss = 3525653.95 (23.583 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.62)\n",
      "Step 737: loss = 3536171.90 (25.005 sec) ('Training Accuracy:', 0.9569127) ('Testing Accuracy:', 0.60333335)\n",
      "Step 738: loss = 708073.27 (23.418 sec) ('Training Accuracy:', 0.9393031) ('Testing Accuracy:', 0.56666666)\n",
      "Step 739: loss = 972416.58 (24.367 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.57666665)\n",
      "Step 740: loss = 322847.94 (24.202 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.56333333)\n",
      "Step 741: loss = 1261775.95 (25.094 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.58999997)\n",
      "Step 742: loss = 724651.99 (23.804 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.57999998)\n",
      "Step 743: loss = 485120.25 (23.579 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.5933333)\n",
      "Step 744: loss = 257114.69 (24.005 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.58333331)\n",
      "Step 745: loss = 440375.84 (24.047 sec) ('Training Accuracy:', 0.93218434) ('Testing Accuracy:', 0.57666665)\n",
      "Step 746: loss = 980826.24 (23.835 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.56999999)\n",
      "Step 747: loss = 135766.56 (24.052 sec) ('Training Accuracy:', 0.95728737) ('Testing Accuracy:', 0.57333332)\n",
      "Step 748: loss = 444725.74 (24.019 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.56999999)\n",
      "Step 749: loss = 990744.68 (26.149 sec) ('Training Accuracy:', 0.90033722) ('Testing Accuracy:', 0.56333333)\n",
      "Step 750: loss = 1204066.68 (24.308 sec) ('Training Accuracy:', 0.93518174) ('Testing Accuracy:', 0.57333332)\n",
      "Step 751: loss = 583031.17 (24.200 sec) ('Training Accuracy:', 0.91082805) ('Testing Accuracy:', 0.57333332)\n",
      "Step 752: loss = 1321199.36 (24.705 sec) ('Training Accuracy:', 0.93218434) ('Testing Accuracy:', 0.56333333)\n",
      "Step 753: loss = 936746.95 (25.280 sec) ('Training Accuracy:', 0.90932935) ('Testing Accuracy:', 0.57666665)\n",
      "Step 754: loss = 688519.55 (25.176 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.62333333)\n",
      "Step 755: loss = 361860.35 (24.428 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.57666665)\n",
      "Step 756: loss = 1224356.14 (24.894 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.56333333)\n",
      "Step 757: loss = 703085.58 (23.442 sec) ('Training Accuracy:', 0.90708131) ('Testing Accuracy:', 0.57666665)\n",
      "Step 758: loss = 881599.19 (24.090 sec) ('Training Accuracy:', 0.93068564) ('Testing Accuracy:', 0.56)\n",
      "Step 759: loss = 839190.47 (24.191 sec) ('Training Accuracy:', 0.87710756) ('Testing Accuracy:', 0.55000001)\n",
      "Step 760: loss = 790963.62 (23.766 sec) ('Training Accuracy:', 0.9276883) ('Testing Accuracy:', 0.55666667)\n",
      "Step 761: loss = 982249.13 (11.253 sec) ('Training Accuracy:', 0.8684901) ('Testing Accuracy:', 0.55000001)\n",
      "Step 762: loss = 1070710.64 (11.531 sec) ('Training Accuracy:', 0.96890223) ('Testing Accuracy:', 0.62)\n",
      "Step 763: loss = 573341.98 (11.731 sec) ('Training Accuracy:', 0.93218434) ('Testing Accuracy:', 0.56)\n",
      "Step 764: loss = 491265.61 (12.055 sec) ('Training Accuracy:', 0.94267517) ('Testing Accuracy:', 0.57333332)\n",
      "Step 765: loss = 516379.69 (12.066 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.56666666)\n",
      "Step 766: loss = 1002670.57 (12.067 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.58333331)\n",
      "Step 767: loss = 192270.81 (12.537 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.56999999)\n",
      "Step 768: loss = 566057.66 (12.463 sec) ('Training Accuracy:', 0.92131883) ('Testing Accuracy:', 0.56)\n",
      "Step 769: loss = 1142202.66 (12.623 sec) ('Training Accuracy:', 0.96777821) ('Testing Accuracy:', 0.58666664)\n",
      "Step 770: loss = 224131.96 (12.905 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.58333331)\n",
      "Step 771: loss = 253125.00 (13.047 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.58666664)\n",
      "Step 772: loss = 384010.06 (13.164 sec) ('Training Accuracy:', 0.96927691) ('Testing Accuracy:', 0.60333335)\n",
      "Step 773: loss = 154694.59 (13.113 sec) ('Training Accuracy:', 0.96403146) ('Testing Accuracy:', 0.5933333)\n",
      "Step 774: loss = 243199.84 (13.283 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.58999997)\n",
      "Step 775: loss = 207659.62 (13.374 sec) ('Training Accuracy:', 0.94454855) ('Testing Accuracy:', 0.56333333)\n",
      "Step 776: loss = 1187078.86 (13.269 sec) ('Training Accuracy:', 0.91270137) ('Testing Accuracy:', 0.56666666)\n",
      "Step 777: loss = 642584.85 (13.245 sec) ('Training Accuracy:', 0.93106031) ('Testing Accuracy:', 0.56333333)\n",
      "Step 778: loss = 989564.79 (12.797 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.56333333)\n",
      "Step 779: loss = 1205817.72 (12.552 sec) ('Training Accuracy:', 0.90858001) ('Testing Accuracy:', 0.54666668)\n",
      "Step 780: loss = 2136651.13 (12.157 sec) ('Training Accuracy:', 0.92431623) ('Testing Accuracy:', 0.56)\n",
      "Step 781: loss = 252357.27 (11.969 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.58333331)\n",
      "Step 782: loss = 686796.59 (11.965 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.5933333)\n",
      "Step 783: loss = 719518.89 (11.751 sec) ('Training Accuracy:', 0.96590483) ('Testing Accuracy:', 0.62333333)\n",
      "Step 784: loss = 351703.70 (11.373 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.58333331)\n",
      "Step 785: loss = 453712.89 (11.300 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.57333332)\n",
      "Step 786: loss = 765039.24 (11.291 sec) ('Training Accuracy:', 0.90370923) ('Testing Accuracy:', 0.56)\n",
      "Step 787: loss = 1010468.91 (11.185 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.60666668)\n",
      "Step 788: loss = 310721.08 (11.319 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.55666667)\n",
      "Step 789: loss = 711201.17 (11.518 sec) ('Training Accuracy:', 0.94492316) ('Testing Accuracy:', 0.57999998)\n",
      "Step 790: loss = 761383.82 (11.262 sec) ('Training Accuracy:', 0.90146124) ('Testing Accuracy:', 0.56999999)\n",
      "Step 791: loss = 1013784.04 (11.139 sec) ('Training Accuracy:', 0.96140879) ('Testing Accuracy:', 0.60000002)\n",
      "Step 792: loss = 579297.27 (11.227 sec) ('Training Accuracy:', 0.91494942) ('Testing Accuracy:', 0.56999999)\n",
      "Step 793: loss = 849744.41 (11.151 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.58999997)\n",
      "Step 794: loss = 926108.82 (11.237 sec) ('Training Accuracy:', 0.88272762) ('Testing Accuracy:', 0.57333332)\n",
      "Step 795: loss = 896763.92 (11.166 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.59666669)\n",
      "Step 796: loss = 311202.98 (11.136 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.5933333)\n",
      "Step 797: loss = 339360.30 (11.343 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.60333335)\n",
      "Step 798: loss = 400262.28 (11.273 sec) ('Training Accuracy:', 0.93368304) ('Testing Accuracy:', 0.57333332)\n",
      "Step 799: loss = 752236.62 (11.197 sec) ('Training Accuracy:', 0.90071189) ('Testing Accuracy:', 0.57999998)\n",
      "Step 800: loss = 1056755.02 (11.189 sec) ('Training Accuracy:', 0.97077554) ('Testing Accuracy:', 0.58999997)\n",
      "Step 801: loss = 1044630.85 (11.187 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.59666669)\n",
      "Step 802: loss = 367625.48 (11.373 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.5933333)\n",
      "Step 803: loss = 529645.75 (11.325 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.60000002)\n",
      "Step 804: loss = 501218.23 (11.750 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.58666664)\n",
      "Step 805: loss = 675950.56 (11.674 sec) ('Training Accuracy:', 0.9246909) ('Testing Accuracy:', 0.56999999)\n",
      "Step 806: loss = 824946.28 (11.621 sec) ('Training Accuracy:', 0.86137128) ('Testing Accuracy:', 0.55666667)\n",
      "Step 807: loss = 796520.66 (11.637 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.59666669)\n",
      "Step 808: loss = 747665.85 (11.646 sec) ('Training Accuracy:', 0.98051703) ('Testing Accuracy:', 0.60000002)\n",
      "Step 809: loss = 446034.95 (11.760 sec) ('Training Accuracy:', 0.91420007) ('Testing Accuracy:', 0.57333332)\n",
      "Step 810: loss = 799903.98 (11.626 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.58333331)\n",
      "Step 811: loss = 912891.84 (11.591 sec) ('Training Accuracy:', 0.93668038) ('Testing Accuracy:', 0.55333334)\n",
      "Step 812: loss = 418907.48 (11.756 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.58333331)\n",
      "Step 813: loss = 233088.27 (11.713 sec) ('Training Accuracy:', 0.94492316) ('Testing Accuracy:', 0.57333332)\n",
      "Step 814: loss = 849716.66 (11.777 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.57999998)\n",
      "Step 815: loss = 216098.02 (11.529 sec) ('Training Accuracy:', 0.92881227) ('Testing Accuracy:', 0.56)\n",
      "Step 816: loss = 554258.33 (11.433 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.58999997)\n",
      "Step 817: loss = 372413.82 (11.518 sec) ('Training Accuracy:', 0.93817908) ('Testing Accuracy:', 0.56333333)\n",
      "Step 818: loss = 586194.16 (11.591 sec) ('Training Accuracy:', 0.93180966) ('Testing Accuracy:', 0.56666666)\n",
      "Step 819: loss = 842703.43 (12.059 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.57999998)\n",
      "Step 820: loss = 291698.70 (12.300 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.61333334)\n",
      "Step 821: loss = 286336.50 (12.357 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.57666665)\n",
      "Step 822: loss = 532794.25 (12.445 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.56666666)\n",
      "Step 823: loss = 368015.84 (12.618 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.57333332)\n",
      "Step 824: loss = 364087.22 (12.951 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.56333333)\n",
      "Step 825: loss = 384709.14 (13.113 sec) ('Training Accuracy:', 0.97414762) ('Testing Accuracy:', 0.59666669)\n",
      "Step 826: loss = 455919.12 (13.372 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.57999998)\n",
      "Step 827: loss = 480213.08 (13.350 sec) ('Training Accuracy:', 0.9408018) ('Testing Accuracy:', 0.57999998)\n",
      "Step 828: loss = 361685.36 (13.364 sec) ('Training Accuracy:', 0.93705505) ('Testing Accuracy:', 0.56666666)\n",
      "Step 829: loss = 625999.29 (13.244 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.57333332)\n",
      "Step 830: loss = 472917.45 (13.278 sec) ('Training Accuracy:', 0.96140879) ('Testing Accuracy:', 0.58666664)\n",
      "Step 831: loss = 331048.58 (13.266 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.61000001)\n",
      "Step 832: loss = 354988.39 (13.267 sec) ('Training Accuracy:', 0.91232669) ('Testing Accuracy:', 0.58333331)\n",
      "Step 833: loss = 1035122.88 (13.415 sec) ('Training Accuracy:', 0.93480706) ('Testing Accuracy:', 0.57666665)\n",
      "Step 834: loss = 713306.24 (13.482 sec) ('Training Accuracy:', 0.94642186) ('Testing Accuracy:', 0.58666664)\n",
      "Step 835: loss = 597733.67 (13.239 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.60333335)\n",
      "Step 836: loss = 403030.30 (12.960 sec) ('Training Accuracy:', 0.92544025) ('Testing Accuracy:', 0.57666665)\n",
      "Step 837: loss = 630649.94 (12.857 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.59666669)\n",
      "Step 838: loss = 712958.34 (12.742 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.60000002)\n",
      "Step 839: loss = 1120086.43 (12.836 sec) ('Training Accuracy:', 0.96627951) ('Testing Accuracy:', 0.64999998)\n",
      "Step 840: loss = 1082229.75 (12.274 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.5933333)\n",
      "Step 841: loss = 798092.55 (12.167 sec) ('Training Accuracy:', 0.96965158) ('Testing Accuracy:', 0.59666669)\n",
      "Step 842: loss = 864857.45 (11.625 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.58333331)\n",
      "Step 843: loss = 239969.07 (11.586 sec) ('Training Accuracy:', 0.94604719) ('Testing Accuracy:', 0.56666666)\n",
      "Step 844: loss = 874075.18 (11.333 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.62333333)\n",
      "Step 845: loss = 612324.71 (11.203 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.61666667)\n",
      "Step 846: loss = 834163.77 (11.202 sec) ('Training Accuracy:', 0.97377294) ('Testing Accuracy:', 0.60000002)\n",
      "Step 847: loss = 791717.70 (11.191 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.58333331)\n",
      "Step 848: loss = 953758.38 (11.137 sec) ('Training Accuracy:', 0.97452229) ('Testing Accuracy:', 0.60000002)\n",
      "Step 849: loss = 1184147.41 (11.216 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.60666668)\n",
      "Step 850: loss = 3957443.34 (11.122 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.63)\n",
      "Step 851: loss = 2006978.40 (11.131 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.58666664)\n",
      "Step 852: loss = 3319390.41 (10.928 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.58999997)\n",
      "Step 853: loss = 7904248.43 (10.901 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.67000002)\n",
      "Step 854: loss = 2820237.60 (10.967 sec) ('Training Accuracy:', 0.91457474) ('Testing Accuracy:', 0.56333333)\n",
      "Step 855: loss = 4465946.69 (10.871 sec) ('Training Accuracy:', 0.91682279) ('Testing Accuracy:', 0.57999998)\n",
      "Step 856: loss = 1616203.31 (11.148 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.57666665)\n",
      "Step 857: loss = 847953.19 (10.969 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.54666668)\n",
      "Step 858: loss = 1669757.42 (11.432 sec) ('Training Accuracy:', 0.82727611) ('Testing Accuracy:', 0.53666669)\n",
      "Step 859: loss = 15871869.81 (11.346 sec) ('Training Accuracy:', 0.97826904) ('Testing Accuracy:', 0.61000001)\n",
      "Step 860: loss = 10932904.21 (11.657 sec) ('Training Accuracy:', 0.92693895) ('Testing Accuracy:', 0.58666664)\n",
      "Step 861: loss = 2153564.64 (11.965 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.59666669)\n",
      "Step 862: loss = 1786166.23 (12.353 sec) ('Training Accuracy:', 0.97527164) ('Testing Accuracy:', 0.63333333)\n",
      "Step 863: loss = 743743.04 (12.499 sec) ('Training Accuracy:', 0.95953542) ('Testing Accuracy:', 0.5933333)\n",
      "Step 864: loss = 586921.01 (12.988 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.58333331)\n",
      "Step 865: loss = 558757.90 (12.822 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.60000002)\n",
      "Step 866: loss = 544317.71 (12.823 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.58666664)\n",
      "Step 867: loss = 1129469.18 (12.809 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.5933333)\n",
      "Step 868: loss = 289465.22 (12.443 sec) ('Training Accuracy:', 0.95279133) ('Testing Accuracy:', 0.58999997)\n",
      "Step 869: loss = 431382.58 (12.244 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.58666664)\n",
      "Step 870: loss = 698896.63 (12.134 sec) ('Training Accuracy:', 0.92056948) ('Testing Accuracy:', 0.57333332)\n",
      "Step 871: loss = 1306140.55 (11.631 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.5933333)\n",
      "Step 872: loss = 372310.25 (11.517 sec) ('Training Accuracy:', 0.97264892) ('Testing Accuracy:', 0.61333334)\n",
      "Step 873: loss = 365347.89 (11.892 sec) ('Training Accuracy:', 0.95054328) ('Testing Accuracy:', 0.58666664)\n",
      "Step 874: loss = 701791.09 (11.472 sec) ('Training Accuracy:', 0.91682279) ('Testing Accuracy:', 0.58333331)\n",
      "Step 875: loss = 1045956.04 (11.104 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.58333331)\n",
      "Step 876: loss = 1179461.16 (10.827 sec) ('Training Accuracy:', 0.8962158) ('Testing Accuracy:', 0.57333332)\n",
      "Step 877: loss = 1002432.44 (10.997 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.57666665)\n",
      "Step 878: loss = 1073795.03 (10.760 sec) ('Training Accuracy:', 0.8977145) ('Testing Accuracy:', 0.56999999)\n",
      "Step 879: loss = 891989.93 (10.918 sec) ('Training Accuracy:', 0.90108657) ('Testing Accuracy:', 0.58666664)\n",
      "Step 880: loss = 2185221.25 (10.781 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.58333331)\n",
      "Step 881: loss = 787858.70 (10.956 sec) ('Training Accuracy:', 0.94117647) ('Testing Accuracy:', 0.5933333)\n",
      "Step 882: loss = 768611.76 (11.131 sec) ('Training Accuracy:', 0.88722366) ('Testing Accuracy:', 0.56666666)\n",
      "Step 883: loss = 1599319.87 (11.498 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.60333335)\n",
      "Step 884: loss = 951472.23 (11.839 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.5933333)\n",
      "Step 885: loss = 183150.63 (12.272 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.5933333)\n",
      "Step 886: loss = 307360.93 (12.556 sec) ('Training Accuracy:', 0.96777821) ('Testing Accuracy:', 0.60333335)\n",
      "Step 887: loss = 264896.72 (12.924 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.61000001)\n",
      "Step 888: loss = 283339.18 (12.778 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.60333335)\n",
      "Step 889: loss = 292427.28 (13.021 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.59666669)\n",
      "Step 890: loss = 507084.72 (12.596 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.58999997)\n",
      "Step 891: loss = 1110972.79 (12.218 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.58666664)\n",
      "Step 892: loss = 604296.02 (13.876 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.60000002)\n",
      "Step 893: loss = 881166.25 (12.938 sec) ('Training Accuracy:', 0.89396781) ('Testing Accuracy:', 0.57666665)\n",
      "Step 894: loss = 1150711.96 (12.652 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.61000001)\n",
      "Step 895: loss = 822471.44 (12.494 sec) ('Training Accuracy:', 0.89134508) ('Testing Accuracy:', 0.57999998)\n",
      "Step 896: loss = 703315.38 (12.793 sec) ('Training Accuracy:', 0.93368304) ('Testing Accuracy:', 0.59666669)\n",
      "Step 897: loss = 999029.85 (12.734 sec) ('Training Accuracy:', 0.90970403) ('Testing Accuracy:', 0.5933333)\n",
      "Step 898: loss = 999253.79 (12.783 sec) ('Training Accuracy:', 0.88909703) ('Testing Accuracy:', 0.57333332)\n",
      "Step 899: loss = 1116532.05 (13.012 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.60000002)\n",
      "Step 900: loss = 456768.74 (13.398 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.57666665)\n",
      "Step 901: loss = 591314.30 (13.375 sec) ('Training Accuracy:', 0.91382539) ('Testing Accuracy:', 0.56666666)\n",
      "Step 902: loss = 1265156.50 (13.546 sec) ('Training Accuracy:', 0.92206818) ('Testing Accuracy:', 0.58666664)\n",
      "Step 903: loss = 503841.43 (13.380 sec) ('Training Accuracy:', 0.90408391) ('Testing Accuracy:', 0.58666664)\n",
      "Step 904: loss = 1084026.39 (13.438 sec) ('Training Accuracy:', 0.88535035) ('Testing Accuracy:', 0.56666666)\n",
      "Step 905: loss = 1056244.03 (13.382 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.58666664)\n",
      "Step 906: loss = 590577.75 (13.204 sec) ('Training Accuracy:', 0.94567251) ('Testing Accuracy:', 0.5933333)\n",
      "Step 907: loss = 891407.32 (13.801 sec) ('Training Accuracy:', 0.91794682) ('Testing Accuracy:', 0.56666666)\n",
      "Step 908: loss = 1053621.45 (11.858 sec) ('Training Accuracy:', 0.89022106) ('Testing Accuracy:', 0.56999999)\n",
      "Step 909: loss = 1212511.80 (11.666 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.62666667)\n",
      "Step 910: loss = 573538.61 (11.149 sec) ('Training Accuracy:', 0.92094415) ('Testing Accuracy:', 0.58333331)\n",
      "Step 911: loss = 585711.93 (11.220 sec) ('Training Accuracy:', 0.87635821) ('Testing Accuracy:', 0.57333332)\n",
      "Step 912: loss = 992812.83 (11.170 sec) ('Training Accuracy:', 0.95616335) ('Testing Accuracy:', 0.58666664)\n",
      "Step 913: loss = 740081.88 (11.092 sec) ('Training Accuracy:', 0.88984638) ('Testing Accuracy:', 0.56999999)\n",
      "Step 914: loss = 793573.42 (11.033 sec) ('Training Accuracy:', 0.95578867) ('Testing Accuracy:', 0.60666668)\n",
      "Step 915: loss = 541643.12 (11.204 sec) ('Training Accuracy:', 0.92544025) ('Testing Accuracy:', 0.57666665)\n",
      "Step 916: loss = 728866.03 (11.048 sec) ('Training Accuracy:', 0.9201948) ('Testing Accuracy:', 0.57333332)\n",
      "Step 917: loss = 956054.31 (11.088 sec) ('Training Accuracy:', 0.91307604) ('Testing Accuracy:', 0.57999998)\n",
      "Step 918: loss = 705640.52 (11.076 sec) ('Training Accuracy:', 0.88872236) ('Testing Accuracy:', 0.57333332)\n",
      "Step 919: loss = 2140193.05 (11.062 sec) ('Training Accuracy:', 0.89246911) ('Testing Accuracy:', 0.56)\n",
      "Step 920: loss = 1477797.57 (11.049 sec) ('Training Accuracy:', 0.94304985) ('Testing Accuracy:', 0.60000002)\n",
      "Step 921: loss = 525357.09 (11.021 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.5933333)\n",
      "Step 922: loss = 874092.49 (11.117 sec) ('Training Accuracy:', 0.9276883) ('Testing Accuracy:', 0.60333335)\n",
      "Step 923: loss = 417302.99 (13.527 sec) ('Training Accuracy:', 0.96777821) ('Testing Accuracy:', 0.60000002)\n",
      "Step 924: loss = 751873.00 (13.317 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.56333333)\n",
      "Step 925: loss = 274455.61 (13.110 sec) ('Training Accuracy:', 0.96740353) ('Testing Accuracy:', 0.61333334)\n",
      "Step 926: loss = 404899.99 (13.185 sec) ('Training Accuracy:', 0.91982013) ('Testing Accuracy:', 0.5933333)\n",
      "Step 927: loss = 959896.07 (13.064 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.58999997)\n",
      "Step 928: loss = 320031.28 (13.082 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.60333335)\n",
      "Step 929: loss = 636499.95 (13.078 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.60333335)\n",
      "Step 930: loss = 969610.42 (13.157 sec) ('Training Accuracy:', 0.97639567) ('Testing Accuracy:', 0.58666664)\n",
      "Step 931: loss = 301817.22 (12.991 sec) ('Training Accuracy:', 0.94642186) ('Testing Accuracy:', 0.57666665)\n",
      "Step 932: loss = 917463.52 (13.112 sec) ('Training Accuracy:', 0.89171976) ('Testing Accuracy:', 0.58999997)\n",
      "Step 933: loss = 1451408.43 (12.948 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.61000001)\n",
      "Step 934: loss = 453455.28 (13.100 sec) ('Training Accuracy:', 0.92581493) ('Testing Accuracy:', 0.58333331)\n",
      "Step 935: loss = 902871.98 (12.810 sec) ('Training Accuracy:', 0.90932935) ('Testing Accuracy:', 0.56333333)\n",
      "Step 936: loss = 693517.92 (12.619 sec) ('Training Accuracy:', 0.93480706) ('Testing Accuracy:', 0.56333333)\n",
      "Step 937: loss = 663374.99 (12.338 sec) ('Training Accuracy:', 0.94567251) ('Testing Accuracy:', 0.58666664)\n",
      "Step 938: loss = 711988.21 (12.406 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.58333331)\n",
      "Step 939: loss = 807003.00 (12.685 sec) ('Training Accuracy:', 0.90670663) ('Testing Accuracy:', 0.57666665)\n",
      "Step 940: loss = 589956.34 (12.400 sec) ('Training Accuracy:', 0.9408018) ('Testing Accuracy:', 0.55333334)\n",
      "Step 941: loss = 806389.66 (12.365 sec) ('Training Accuracy:', 0.91832149) ('Testing Accuracy:', 0.55666667)\n",
      "Step 942: loss = 684001.78 (12.127 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.57999998)\n",
      "Step 943: loss = 343493.10 (12.004 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.57999998)\n",
      "Step 944: loss = 258029.27 (11.827 sec) ('Training Accuracy:', 0.94492316) ('Testing Accuracy:', 0.58333331)\n",
      "Step 945: loss = 520096.56 (11.875 sec) ('Training Accuracy:', 0.86661673) ('Testing Accuracy:', 0.57333332)\n",
      "Step 946: loss = 835124.89 (11.774 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.57999998)\n",
      "Step 947: loss = 655629.37 (12.021 sec) ('Training Accuracy:', 0.89134508) ('Testing Accuracy:', 0.56666666)\n",
      "Step 948: loss = 967404.89 (11.772 sec) ('Training Accuracy:', 0.90745598) ('Testing Accuracy:', 0.56666666)\n",
      "Step 949: loss = 678117.81 (11.814 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.60333335)\n",
      "Step 950: loss = 364316.78 (11.649 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.57999998)\n",
      "Step 951: loss = 601778.16 (11.538 sec) ('Training Accuracy:', 0.97264892) ('Testing Accuracy:', 0.62)\n",
      "Step 952: loss = 573506.99 (11.642 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.61333334)\n",
      "Step 953: loss = 309869.80 (12.950 sec) ('Training Accuracy:', 0.94005245) ('Testing Accuracy:', 0.56999999)\n",
      "Step 954: loss = 548634.56 (12.777 sec) ('Training Accuracy:', 0.92581493) ('Testing Accuracy:', 0.54333335)\n",
      "Step 955: loss = 761629.91 (12.572 sec) ('Training Accuracy:', 0.96403146) ('Testing Accuracy:', 0.58666664)\n",
      "Step 956: loss = 575045.57 (12.635 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.58999997)\n",
      "Step 957: loss = 233247.51 (12.700 sec) ('Training Accuracy:', 0.96777821) ('Testing Accuracy:', 0.57666665)\n",
      "Step 958: loss = 1030901.35 (12.700 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.57666665)\n",
      "Step 959: loss = 436845.45 (12.699 sec) ('Training Accuracy:', 0.92956161) ('Testing Accuracy:', 0.56999999)\n",
      "Step 960: loss = 784764.55 (12.660 sec) ('Training Accuracy:', 0.89659047) ('Testing Accuracy:', 0.57333332)\n",
      "Step 961: loss = 931682.81 (12.723 sec) ('Training Accuracy:', 0.92956161) ('Testing Accuracy:', 0.57333332)\n",
      "Step 962: loss = 729839.30 (12.975 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.57666665)\n",
      "Step 963: loss = 743693.99 (12.928 sec) ('Training Accuracy:', 0.92281753) ('Testing Accuracy:', 0.57666665)\n",
      "Step 964: loss = 1055940.61 (13.198 sec) ('Training Accuracy:', 0.97377294) ('Testing Accuracy:', 0.61000001)\n",
      "Step 965: loss = 1117884.80 (13.163 sec) ('Training Accuracy:', 0.91195202) ('Testing Accuracy:', 0.57999998)\n",
      "Step 966: loss = 1491177.35 (12.994 sec) ('Training Accuracy:', 0.8801049) ('Testing Accuracy:', 0.55333334)\n",
      "Step 967: loss = 841525.38 (12.694 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.61333334)\n",
      "Step 968: loss = 415584.00 (12.706 sec) ('Training Accuracy:', 0.95279133) ('Testing Accuracy:', 0.59666669)\n",
      "Step 969: loss = 443077.20 (12.966 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.56666666)\n",
      "Step 970: loss = 504314.62 (12.569 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.58333331)\n",
      "Step 971: loss = 516254.59 (12.442 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.60333335)\n",
      "Step 972: loss = 478385.20 (12.690 sec) ('Training Accuracy:', 0.95279133) ('Testing Accuracy:', 0.60666668)\n",
      "Step 973: loss = 602866.78 (12.282 sec) ('Training Accuracy:', 0.93443239) ('Testing Accuracy:', 0.58999997)\n",
      "Step 974: loss = 686923.28 (12.183 sec) ('Training Accuracy:', 0.97264892) ('Testing Accuracy:', 0.60000002)\n",
      "Step 975: loss = 172173.94 (12.016 sec) ('Training Accuracy:', 0.95054328) ('Testing Accuracy:', 0.58333331)\n",
      "Step 976: loss = 517676.69 (12.033 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.59666669)\n",
      "Step 977: loss = 267033.06 (12.222 sec) ('Training Accuracy:', 0.97602099) ('Testing Accuracy:', 0.61333334)\n",
      "Step 978: loss = 4672687.09 (12.016 sec) ('Training Accuracy:', 0.90108657) ('Testing Accuracy:', 0.6566667)\n",
      "Step 979: loss = 4094497.29 (11.745 sec) ('Training Accuracy:', 0.90783066) ('Testing Accuracy:', 0.57666665)\n",
      "Step 980: loss = 5566997.66 (11.591 sec) ('Training Accuracy:', 0.95466465) ('Testing Accuracy:', 0.57666665)\n",
      "Step 981: loss = 4494088.17 (11.202 sec) ('Training Accuracy:', 0.89284378) ('Testing Accuracy:', 0.57333332)\n",
      "Step 982: loss = 3334961.63 (10.907 sec) ('Training Accuracy:', 0.93405771) ('Testing Accuracy:', 0.58333331)\n",
      "Step 983: loss = 1285707.84 (10.967 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.58666664)\n",
      "Step 984: loss = 706279.55 (13.476 sec) ('Training Accuracy:', 0.95878607) ('Testing Accuracy:', 0.5933333)\n",
      "Step 985: loss = 577954.70 (12.875 sec) ('Training Accuracy:', 0.97789437) ('Testing Accuracy:', 0.62)\n",
      "Step 986: loss = 1390533.20 (12.664 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.62333333)\n",
      "Step 987: loss = 260757.04 (12.390 sec) ('Training Accuracy:', 0.96890223) ('Testing Accuracy:', 0.60333335)\n",
      "Step 988: loss = 296113.90 (12.086 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.60000002)\n",
      "Step 989: loss = 417159.83 (11.833 sec) ('Training Accuracy:', 0.92244285) ('Testing Accuracy:', 0.5933333)\n",
      "Step 990: loss = 1083802.70 (11.448 sec) ('Training Accuracy:', 0.95728737) ('Testing Accuracy:', 0.62)\n",
      "Step 991: loss = 242461.63 (11.415 sec) ('Training Accuracy:', 0.96852756) ('Testing Accuracy:', 0.62)\n",
      "Step 992: loss = 126421.27 (11.352 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.61333334)\n",
      "Step 993: loss = 149449.24 (10.919 sec) ('Training Accuracy:', 0.96740353) ('Testing Accuracy:', 0.62666667)\n",
      "Step 994: loss = 283666.40 (10.769 sec) ('Training Accuracy:', 0.94005245) ('Testing Accuracy:', 0.61333334)\n",
      "Step 995: loss = 774035.92 (16.658 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.61000001)\n",
      "Step 996: loss = 359850.90 (15.574 sec) ('Training Accuracy:', 0.96028477) ('Testing Accuracy:', 0.60666668)\n",
      "Step 997: loss = 492872.47 (13.036 sec) ('Training Accuracy:', 0.94904459) ('Testing Accuracy:', 0.58999997)\n",
      "Step 998: loss = 1239235.01 (12.963 sec) ('Training Accuracy:', 0.97751969) ('Testing Accuracy:', 0.61000001)\n",
      "Step 999: loss = 533878.74 (13.145 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.58333331)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_acc, test_acc = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5517602,\n",
       " 0.57685608,\n",
       " 0.61275709,\n",
       " 0.61519694,\n",
       " 0.61903101,\n",
       " 0.62704778,\n",
       " 0.65597767,\n",
       " 0.64168698,\n",
       " 0.68455911,\n",
       " 0.68246776,\n",
       " 0.70303243,\n",
       " 0.70965493,\n",
       " 0.68978739,\n",
       " 0.70826072,\n",
       " 0.68978739,\n",
       " 0.67828512,\n",
       " 0.63715583,\n",
       " 0.68734753,\n",
       " 0.75182992,\n",
       " 0.77030325,\n",
       " 0.74695015,\n",
       " 0.72638547,\n",
       " 0.76158941,\n",
       " 0.69083303,\n",
       " 0.71627742,\n",
       " 0.66050887,\n",
       " 0.67096549,\n",
       " 0.78459394,\n",
       " 0.77866852,\n",
       " 0.80411291,\n",
       " 0.76158941,\n",
       " 0.7535727,\n",
       " 0.76298362,\n",
       " 0.830603,\n",
       " 0.81561518,\n",
       " 0.84454513,\n",
       " 0.69466716,\n",
       " 0.6521436,\n",
       " 0.79679328,\n",
       " 0.86197281,\n",
       " 0.86127573,\n",
       " 0.79365635,\n",
       " 0.66573721,\n",
       " 0.75845242,\n",
       " 0.85604739,\n",
       " 0.87905192,\n",
       " 0.82851166,\n",
       " 0.70094109,\n",
       " 0.74032766,\n",
       " 0.72359705,\n",
       " 0.84489369,\n",
       " 0.87173229,\n",
       " 0.88114327,\n",
       " 0.68316489,\n",
       " 0.62321365,\n",
       " 0.86301845,\n",
       " 0.85988152,\n",
       " 0.71314049,\n",
       " 0.78773093,\n",
       " 0.77065182,\n",
       " 0.67933077,\n",
       " 0.62669921,\n",
       " 0.66225165,\n",
       " 0.84593934,\n",
       " 0.8574416,\n",
       " 0.68211919,\n",
       " 0.61031717,\n",
       " 0.7570582,\n",
       " 0.81456953,\n",
       " 0.64830953,\n",
       " 0.59602648,\n",
       " 0.68246776,\n",
       " 0.7804113,\n",
       " 0.59637505,\n",
       " 0.58417565,\n",
       " 0.67096549,\n",
       " 0.61101431,\n",
       " 0.66922271,\n",
       " 0.7758801,\n",
       " 0.72533983,\n",
       " 0.76158941,\n",
       " 0.78773093,\n",
       " 0.82328337,\n",
       " 0.85779017,\n",
       " 0.88637155,\n",
       " 0.91634715,\n",
       " 0.93238062,\n",
       " 0.92575812,\n",
       " 0.91634715,\n",
       " 0.89857095,\n",
       " 0.90135938,\n",
       " 0.91251308,\n",
       " 0.93342626,\n",
       " 0.85918438,\n",
       " 0.76542348,\n",
       " 0.7647264,\n",
       " 0.85709304,\n",
       " 0.93203205,\n",
       " 0.91111887,\n",
       " 0.9041478,\n",
       " 0.95608228,\n",
       " 0.94283724,\n",
       " 0.95677936,\n",
       " 0.96619034,\n",
       " 0.96897876,\n",
       " 0.96967584,\n",
       " 0.96967584,\n",
       " 0.97246426,\n",
       " 0.96653885,\n",
       " 0.97490412,\n",
       " 0.9766469,\n",
       " 0.97838968,\n",
       " 0.97908682,\n",
       " 0.97838968,\n",
       " 0.97107005,\n",
       " 0.97072148,\n",
       " 0.97804111,\n",
       " 0.94632274,\n",
       " 0.9654932,\n",
       " 0.96235621,\n",
       " 0.96584177,\n",
       " 0.94318581,\n",
       " 0.95921922,\n",
       " 0.9072848,\n",
       " 0.9107703,\n",
       " 0.89055419,\n",
       " 0.90205646,\n",
       " 0.91878706,\n",
       " 0.91704428,\n",
       " 0.91355872,\n",
       " 0.94597423,\n",
       " 0.94458002,\n",
       " 0.96375042,\n",
       " 0.96967584,\n",
       " 0.96026492,\n",
       " 0.87835485,\n",
       " 0.87033808,\n",
       " 0.79539907,\n",
       " 0.76995468,\n",
       " 0.73126525,\n",
       " 0.82258624,\n",
       " 0.89787382,\n",
       " 0.84977347,\n",
       " 0.83443707,\n",
       " 0.90623909,\n",
       " 0.8874172,\n",
       " 0.85047054,\n",
       " 0.93516904,\n",
       " 0.93447196,\n",
       " 0.87905192,\n",
       " 0.74625307,\n",
       " 0.75392121,\n",
       " 0.87591493,\n",
       " 0.95643079,\n",
       " 0.92401534,\n",
       " 0.81944931,\n",
       " 0.9721157,\n",
       " 0.97072148,\n",
       " 0.97455561,\n",
       " 0.98082954,\n",
       " 0.97176719,\n",
       " 0.9776926,\n",
       " 0.9843151,\n",
       " 0.97978389,\n",
       " 0.98501217,\n",
       " 0.98048103,\n",
       " 0.9731614,\n",
       " 0.98396653,\n",
       " 0.98257232,\n",
       " 0.98536074,\n",
       " 0.97838968,\n",
       " 0.98501217,\n",
       " 0.98326945,\n",
       " 0.9843151,\n",
       " 0.98501217,\n",
       " 0.97804111,\n",
       " 0.9766469,\n",
       " 0.9843151,\n",
       " 0.98640639,\n",
       " 0.98466367,\n",
       " 0.97943532,\n",
       " 0.97455561,\n",
       " 0.96026492,\n",
       " 0.98396653,\n",
       " 0.98501217,\n",
       " 0.97420704,\n",
       " 0.98605788,\n",
       " 0.97350991,\n",
       " 0.97699547,\n",
       " 0.97699547,\n",
       " 0.98605788,\n",
       " 0.98187524,\n",
       " 0.97107005,\n",
       " 0.9878006,\n",
       " 0.97176719,\n",
       " 0.98605788,\n",
       " 0.98675495,\n",
       " 0.97804111,\n",
       " 0.98745209,\n",
       " 0.98675495,\n",
       " 0.9776926,\n",
       " 0.96479613,\n",
       " 0.9721157,\n",
       " 0.96932727,\n",
       " 0.98292089,\n",
       " 0.98640639,\n",
       " 0.97699547,\n",
       " 0.97385848,\n",
       " 0.9888463,\n",
       " 0.97107005,\n",
       " 0.98745209,\n",
       " 0.96584177,\n",
       " 0.98536074,\n",
       " 0.97560126,\n",
       " 0.95712793,\n",
       " 0.97455561,\n",
       " 0.98466367,\n",
       " 0.98570931,\n",
       " 0.98187524,\n",
       " 0.98745209,\n",
       " 0.9878006,\n",
       " 0.98745209,\n",
       " 0.98187524,\n",
       " 0.98326945,\n",
       " 0.98501217,\n",
       " 0.98710352,\n",
       " 0.98536074,\n",
       " 0.98501217,\n",
       " 0.97350991,\n",
       " 0.98710352,\n",
       " 0.97873825,\n",
       " 0.94736844,\n",
       " 0.98013246,\n",
       " 0.98570931,\n",
       " 0.9574765,\n",
       " 0.96200764,\n",
       " 0.9776926,\n",
       " 0.94876266,\n",
       " 0.93830603,\n",
       " 0.96409899,\n",
       " 0.89578253,\n",
       " 0.94945973,\n",
       " 0.95608228,\n",
       " 0.96061343,\n",
       " 0.97037297,\n",
       " 0.95329386,\n",
       " 0.95259672,\n",
       " 0.98048103,\n",
       " 0.96584177,\n",
       " 0.96897876,\n",
       " 0.95015687,\n",
       " 0.97594982,\n",
       " 0.98013246,\n",
       " 0.96653885,\n",
       " 0.95677936,\n",
       " 0.97594982,\n",
       " 0.95782501,\n",
       " 0.97943532,\n",
       " 0.94283724,\n",
       " 0.91286165,\n",
       " 0.88253748,\n",
       " 0.96653885,\n",
       " 0.88532591,\n",
       " 0.78006274,\n",
       " 0.94771695,\n",
       " 0.96828163,\n",
       " 0.96897876,\n",
       " 0.94039732,\n",
       " 0.91251308,\n",
       " 0.84803069,\n",
       " 0.97804111,\n",
       " 0.96619034,\n",
       " 0.93272918,\n",
       " 0.97594982,\n",
       " 0.89996517,\n",
       " 0.97350991,\n",
       " 0.93795747,\n",
       " 0.97002441,\n",
       " 0.92575812,\n",
       " 0.98082954,\n",
       " 0.94736844,\n",
       " 0.97176719,\n",
       " 0.9811781,\n",
       " 0.95677936,\n",
       " 0.98292089,\n",
       " 0.9776926,\n",
       " 0.97281283,\n",
       " 0.97107005,\n",
       " 0.9721157,\n",
       " 0.93063784,\n",
       " 0.93482047,\n",
       " 0.96793306,\n",
       " 0.94736844,\n",
       " 0.96619034,\n",
       " 0.9766469,\n",
       " 0.88532591,\n",
       " 0.97525269,\n",
       " 0.80934125,\n",
       " 0.91669571,\n",
       " 0.82188916,\n",
       " 0.86650401,\n",
       " 0.87312651,\n",
       " 0.84210527,\n",
       " 0.7636807,\n",
       " 0.73823631,\n",
       " 0.9072848,\n",
       " 0.9076333,\n",
       " 0.92366678,\n",
       " 0.90240502,\n",
       " 0.86336702,\n",
       " 0.96584177,\n",
       " 0.97385848,\n",
       " 0.97385848,\n",
       " 0.86162424,\n",
       " 0.82154059,\n",
       " 0.76054376,\n",
       " 0.85081911,\n",
       " 0.97699547,\n",
       " 0.89682817,\n",
       " 0.97350991,\n",
       " 0.92784941,\n",
       " 0.98326945,\n",
       " 0.97908682,\n",
       " 0.98292089,\n",
       " 0.98292089,\n",
       " 0.98361796,\n",
       " 0.98361796,\n",
       " 0.9811781,\n",
       " 0.98745209,\n",
       " 0.97978389,\n",
       " 0.9776926,\n",
       " 0.98222375,\n",
       " 0.97594982,\n",
       " 0.97838968,\n",
       " 0.97455561,\n",
       " 0.97804111,\n",
       " 0.97107005,\n",
       " 0.97246426,\n",
       " 0.9731614,\n",
       " 0.97246426,\n",
       " 0.97455561,\n",
       " 0.97420704,\n",
       " 0.9721157,\n",
       " 0.97141862,\n",
       " 0.98257232,\n",
       " 0.97873825,\n",
       " 0.97246426,\n",
       " 0.97525269,\n",
       " 0.97873825,\n",
       " 0.98222375,\n",
       " 0.97804111,\n",
       " 0.98675495,\n",
       " 0.97873825,\n",
       " 0.98222375,\n",
       " 0.98640639,\n",
       " 0.98501217,\n",
       " 0.9731614,\n",
       " 0.98745209,\n",
       " 0.97873825,\n",
       " 0.9776926,\n",
       " 0.98745209,\n",
       " 0.98640639,\n",
       " 0.9766469,\n",
       " 0.97838968,\n",
       " 0.9843151,\n",
       " 0.98082954,\n",
       " 0.97560126,\n",
       " 0.9843151,\n",
       " 0.98326945,\n",
       " 0.9731614,\n",
       " 0.98326945,\n",
       " 0.9843151,\n",
       " 0.97385848,\n",
       " 0.97873825,\n",
       " 0.9878006,\n",
       " 0.97908682,\n",
       " 0.9776926,\n",
       " 0.9843151,\n",
       " 0.98048103,\n",
       " 0.98745209,\n",
       " 0.9888463,\n",
       " 0.96653885,\n",
       " 0.98849773,\n",
       " 0.98954338,\n",
       " 0.97002441,\n",
       " 0.98710352,\n",
       " 0.98710352,\n",
       " 0.97560126,\n",
       " 0.98257232,\n",
       " 0.98501217,\n",
       " 0.97978389,\n",
       " 0.9721157,\n",
       " 0.98710352,\n",
       " 0.98048103,\n",
       " 0.98152667,\n",
       " 0.98710352,\n",
       " 0.98745209,\n",
       " 0.96758455,\n",
       " 0.97838968,\n",
       " 0.98989195,\n",
       " 0.9888463,\n",
       " 0.98814917,\n",
       " 0.97838968,\n",
       " 0.96897876,\n",
       " 0.97699547,\n",
       " 0.98570931,\n",
       " 0.96897876,\n",
       " 0.98814917,\n",
       " 0.97525269,\n",
       " 0.98292089,\n",
       " 0.96619034,\n",
       " 0.96932727,\n",
       " 0.97490412,\n",
       " 0.97978389,\n",
       " 0.9878006,\n",
       " 0.98501217,\n",
       " 0.97560126,\n",
       " 0.96131057,\n",
       " 0.98605788,\n",
       " 0.9878006,\n",
       " 0.98745209,\n",
       " 0.97350991,\n",
       " 0.98326945,\n",
       " 0.97141862,\n",
       " 0.98466367,\n",
       " 0.97804111,\n",
       " 0.98814917,\n",
       " 0.9888463,\n",
       " 0.97804111,\n",
       " 0.96793306,\n",
       " 0.96828163,\n",
       " 0.97525269,\n",
       " 0.96409899,\n",
       " 0.96619034,\n",
       " 0.98048103,\n",
       " 0.98954338,\n",
       " 0.95712793,\n",
       " 0.98466367,\n",
       " 0.9843151,\n",
       " 0.96514463,\n",
       " 0.96653885,\n",
       " 0.9654932,\n",
       " 0.96758455,\n",
       " 0.95677936,\n",
       " 0.94632274,\n",
       " 0.94458002,\n",
       " 0.93970025,\n",
       " 0.95050538,\n",
       " 0.95050538,\n",
       " 0.9498083,\n",
       " 0.92540956,\n",
       " 0.92854655,\n",
       " 0.93482047,\n",
       " 0.9574765,\n",
       " 0.96375042,\n",
       " 0.90554202,\n",
       " 0.98292089,\n",
       " 0.98013246,\n",
       " 0.9574765,\n",
       " 0.97385848,\n",
       " 0.91495293,\n",
       " 0.98152667,\n",
       " 0.98466367,\n",
       " 0.93586618,\n",
       " 0.98570931,\n",
       " 0.96688741,\n",
       " 0.97943532,\n",
       " 0.98048103,\n",
       " 0.96409899,\n",
       " 0.96723598,\n",
       " 0.96270478,\n",
       " 0.93935168,\n",
       " 0.93935168,\n",
       " 0.96235621,\n",
       " 0.98710352,\n",
       " 0.93691182,\n",
       " 0.98745209,\n",
       " 0.94562566,\n",
       " 0.92784941,\n",
       " 0.96235621,\n",
       " 0.97246426,\n",
       " 0.93482047,\n",
       " 0.9811781,\n",
       " 0.98361796,\n",
       " 0.97246426,\n",
       " 0.96270478,\n",
       " 0.96514463,\n",
       " 0.9543395,\n",
       " 0.96932727,\n",
       " 0.9721157,\n",
       " 0.95294529,\n",
       " 0.98257232,\n",
       " 0.97978389,\n",
       " 0.88915998,\n",
       " 0.96340185,\n",
       " 0.9574765,\n",
       " 0.97490412,\n",
       " 0.94597423,\n",
       " 0.97281283,\n",
       " 0.93691182]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98051703"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67000002"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngiAIgiAIgiAIgiAIQkbx/zEb8pUuxXYJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f60b54a6090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1000)\n",
    "\n",
    "plt.plot(x, train_acc)\n",
    "#plt.plot(x, test_acc)\n",
    "\n",
    "\n",
    "plt.legend(['y = training_samples', 'y = testing_samples'], loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f60b5492ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1000)\n",
    "\n",
    "plt.plot(x, test_acc)\n",
    "\n",
    "\n",
    "plt.legend(['y = testing_samples'], loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_list = pd.DataFrame(\n",
    "    {'Training_Acc': train_acc,\n",
    "     'Testing_Acc': test_acc,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes.AxesSubplot at 0x7f6118139810>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6123d7cb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_list.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
