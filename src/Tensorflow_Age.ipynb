{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import os,sys, shutil\n",
    "import time\n",
    "from datetime import date\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import pprint\n",
    "from collections import deque\n",
    "from shutil import copyfile\n",
    "import random\n",
    "import glob\n",
    "# Import the required modules\n",
    "import cv2, os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Logistic Regression\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import math\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_fl = open(\"linkedin_profiles.pickle\",\"rb\")\n",
    "my_original_list=pickle.load(pkl_fl) # errors out here\n",
    "pkl_fl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = \"Male\"\n",
    "    \n",
    "if os.path.exists(directory):\n",
    "    shutil.rmtree(directory)\n",
    "    os.makedirs(directory)     \n",
    "else:\n",
    "    os.makedirs(directory) \n",
    "\n",
    "directory1 = \"Female\"\n",
    "\n",
    "if os.path.exists(directory1):\n",
    "    shutil.rmtree(directory1)\n",
    "    os.makedirs(directory1)     \n",
    "else:\n",
    "    os.makedirs(directory1)     \n",
    "\n",
    "directory2 = \"Label_Images_Age\"\n",
    "\n",
    "if os.path.exists(directory2):\n",
    "    shutil.rmtree(directory2)\n",
    "    os.makedirs(directory2)     \n",
    "else:\n",
    "    os.makedirs(directory2)     \n",
    "    \n",
    "fileList = glob.glob(\"./Images/*.*\")\n",
    "\n",
    "for id,fp in enumerate(fileList):\n",
    "    filename, file_extension = os.path.splitext(fp)\n",
    "    uid = filename.split('/')[-1]\n",
    "    #print fp\n",
    "    for prof in my_original_list:\n",
    "        if prof['User_ID'] == uid:\n",
    "            prof_age = prof['age']\n",
    "            \n",
    "            if (0 <= prof_age <= 30):\n",
    "                new_file_extension = 'Youth'\n",
    "            else:\n",
    "                new_file_extension = 'Senior'\n",
    "            \n",
    "            copyfile(filename + \".jpg\", './Label_Images_Age/'+ uid + '.' + str(id) + \".\" + new_file_extension +'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For face detection we will use the Haar Cascade provided by OpenCV.\n",
    "cascadePath = \"haarcascade_frontalface_default.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cascadePath)\n",
    "\n",
    "# For face recognition we will the the LBPH Face Recognizer \n",
    "recognizer = cv2.createLBPHFaceRecognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_images_and_labels(path):\n",
    "    # Append all the absolute image paths in a list image_paths\n",
    "    \n",
    "    image_paths = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "    # images will contains face images\n",
    "    images = []\n",
    "    # labels will contains the label that is assigned to the image\n",
    "    labels = []\n",
    "    #gender will contains 1 or 0 indecating male or female\n",
    "    age =[]\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        # Read the image and convert to grayscale\n",
    "        try:\n",
    "            image_pil = Image.open(image_path).convert('L')\n",
    "            # Convert the image format into numpy array\n",
    "            image = np.array(image_pil, 'uint8')\n",
    "            # Get the label of the image\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        nbr = int(os.path.split(image_path)[1].split(\".\")[1])\n",
    "        age_current = os.path.split(image_path)[1].split(\".\")[2]\n",
    "        #print nbr\n",
    "        \n",
    "        # Detect the face in the image\n",
    "        faces = faceCascade.detectMultiScale(image)\n",
    "        # If face is detected, append the face to images and the label to labels\n",
    "        try:\n",
    "            for (x, y, w, h) in faces:\n",
    "\n",
    "                ref_image = image[y: y + h, x: x + w]\n",
    "                resized = cv2.resize(ref_image, (100, 100), interpolation = cv2.INTER_AREA)\n",
    "                #edge_images = cv2.Canny(resized,100,200)\n",
    "                resized_face = cv2.resize(ref_image, (100, 100), interpolation = cv2.INTER_AREA)\n",
    "                \n",
    "                images.append(np.array(resized))   #resized.reshape(1,10000)\n",
    "                labels.append(nbr)\n",
    "\n",
    "                if age_current == 'Youth':\n",
    "                    age.append(0)\n",
    "                \n",
    "                else:\n",
    "                    age.append(1)\n",
    "                \n",
    "                #face_file_name = \"faces/face_\" + str(y) + \".jpg\"\n",
    "                #cv2.imwrite(face_file_name, sub_face)\n",
    "                \n",
    "                cv2.imshow(\"Adding faces to traning set...\", resized)\n",
    "                cv2.waitKey(1)\n",
    "        except:\n",
    "            pass\n",
    "    # return the images list and labels list\n",
    "    #print \"lables\"\n",
    "    #print labels\n",
    "    #print \"Age_current\"\n",
    "    #print age\n",
    "    \n",
    "    return images, labels, age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images, labels, ageList = get_images_and_labels('Label_Images_Age')\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#res_images = []\n",
    "res_age = []\n",
    "\n",
    "for age in ageList:\n",
    "    res_age.append(np.array(age))\n",
    "                   \n",
    "res_age = np.array(res_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2968,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_age.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model parameters as external flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic model parameters as external flags.\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer('hidden1', 1500, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 1000, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_integer('hidden3', 500, 'Number of units in hidden layer 3.')\n",
    "flags.DEFINE_integer('batch_size', 100, 'Batch size.  '\n",
    "                     'Must divide evenly into the dataset sizes.')\n",
    "flags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')\n",
    "flags.DEFINE_boolean('fake_data', False, 'If true, uses fake data '\n",
    "                     'for unit testing.')\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 100\n",
    "#CHANNELS = 3\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_nodes = [IMAGE_PIXELS, 1500, 1000, 500, NUM_CLASSES]\n",
    "n_epochs = 10\n",
    "#NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    n_hidden_layers = 3\n",
    "    # define the layers\n",
    "    layers = [] \n",
    "    for i in range(n_hidden_layers + 1):\n",
    "        layers.append( {'weights':tf.Variable(tf.random_normal([n_nodes[i], n_nodes[i+1]])), \n",
    "                        'biases':tf.Variable(tf.random_normal([n_nodes[i+1]]))} )\n",
    "    \n",
    "    # calculate the nodal values for each layer\n",
    "    calcs = [data]\n",
    "    for i in range(n_hidden_layers):\n",
    "        calcs.append( tf.nn.relu(tf.matmul(calcs[i], layers[i]['weights']) + layers[i]['biases']) )\n",
    "\n",
    "    #  return the last layer of nodes\n",
    "    return tf.matmul(calcs[-1], layers[-1]['weights']) + layers[-1]['biases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "  \n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs():\n",
    "    images_placeholder = tf.placeholder(tf.float32, [None,IMAGE_PIXELS])\n",
    "    labels_placeholder = tf.placeholder(tf.float32, [None,NUM_CLASSES])\n",
    "    \n",
    "    return images_placeholder, labels_placeholder\n",
    "\n",
    "\n",
    "def fill_feed_dict(images_feed,labels_feed, images_pl, labels_pl):\n",
    "    feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "    }\n",
    "  \n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def do_eval(sess, eval_correct, images_placeholder, labels_placeholder, data_set):\n",
    "#     # And run one epoch of eval.\n",
    "#     true_count = 0  # Counts the number of correct predictions.\n",
    "#     steps_per_epoch = 47 // FLAGS.batch_size\n",
    "#     num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "#     for step in xrange(steps_per_epoch):\n",
    "#         feed_dict = fill_feed_dict(train_images,train_labels,\n",
    "#                                images_placeholder,\n",
    "#                                labels_placeholder)\n",
    "#         true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "#     precision = true_count / num_examples\n",
    "#     print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "#         (num_examples, true_count, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    \n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    with tf.Graph().as_default():\n",
    "        # Generate placeholders for the images and labels.\n",
    "        images_placeholder, labels_placeholder = placeholder_inputs()\n",
    "        \n",
    "        \n",
    "        logits = neural_network_model(images_placeholder)\n",
    "        \n",
    "        \n",
    "        cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits,labels_placeholder) )\n",
    "        training_acc = []\n",
    "        testing_acc = []\n",
    "       \n",
    "        #print cost\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "            subset_size = 128\n",
    "            for step in xrange(200):\n",
    "                start_time = time.time()\n",
    "                total_loss = 0\n",
    "                for i in range(int(train_images.shape[0] / subset_size) ):\n",
    "                    \n",
    "                    epoch_x = train_images[i * subset_size:][:subset_size]\n",
    "                    epoch_y = train_labels[i * subset_size:][:subset_size]\n",
    "                    \n",
    "                    feed_dict = fill_feed_dict(epoch_x, epoch_y, images_placeholder, labels_placeholder)\n",
    "                    \n",
    "                    _, loss_value = sess.run([optimizer, cost],\n",
    "                                           feed_dict=feed_dict)\n",
    "                    \n",
    "                    total_loss+=loss_value\n",
    "                    \n",
    "                duration = time.time() - start_time\n",
    "                #if step % 10 == 0:\n",
    "                    #Print status to stdout.\n",
    "                correct = tf.equal(tf.argmax(logits,1), tf.argmax(labels_placeholder,1))\n",
    "                #print correct\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "                \n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, total_loss, duration)),\n",
    "                \n",
    "                current_train_acc = accuracy.eval({images_placeholder: train_images, labels_placeholder: train_labels})\n",
    "                current_test_acc = accuracy.eval({images_placeholder: test_images, labels_placeholder: test_labels})\n",
    "                \n",
    "                training_acc.append(current_train_acc)\n",
    "                testing_acc.append(current_test_acc)\n",
    "                \n",
    "                \n",
    "                print('Training Accuracy:', current_train_acc),\n",
    "                print('Testing Accuracy:', current_test_acc)\n",
    "    \n",
    "    return training_acc, testing_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the sets of images and labels for training, validation, and\n",
    "\n",
    "images = np.array(images)\n",
    "images = images.reshape(images.shape[0],IMAGE_PIXELS)\n",
    "\n",
    "#label = res_gender\n",
    "labels = dense_to_one_hot(res_age,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = images[:-300]\n",
    "train_labels = labels[:-300]\n",
    "test_images = images[-300:]\n",
    "test_labels = labels[-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2668, 10000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 395528670.00 (20.735 sec) ('Training Accuracy:', 0.58523792) ('Testing Accuracy:', 0.55666667)\n",
      "Step 1: loss = 220396099.00 (18.116 sec) ('Training Accuracy:', 0.54589736) ('Testing Accuracy:', 0.54000002)\n",
      "Step 2: loss = 179621873.00 (18.783 sec) ('Training Accuracy:', 0.57474709) ('Testing Accuracy:', 0.57333332)\n",
      "Step 3: loss = 155101924.50 (18.809 sec) ('Training Accuracy:', 0.58673662) ('Testing Accuracy:', 0.56666666)\n",
      "Step 4: loss = 134338255.50 (18.091 sec) ('Training Accuracy:', 0.6140877) ('Testing Accuracy:', 0.55666667)\n",
      "Step 5: loss = 119382057.00 (18.524 sec) ('Training Accuracy:', 0.62233049) ('Testing Accuracy:', 0.56666666)\n",
      "Step 6: loss = 109150841.00 (18.371 sec) ('Training Accuracy:', 0.6140877) ('Testing Accuracy:', 0.52999997)\n",
      "Step 7: loss = 100355448.50 (18.288 sec) ('Training Accuracy:', 0.64368677) ('Testing Accuracy:', 0.55000001)\n",
      "Step 8: loss = 90316874.00 (18.259 sec) ('Training Accuracy:', 0.65979767) ('Testing Accuracy:', 0.56333333)\n",
      "Step 9: loss = 83114883.75 (19.799 sec) ('Training Accuracy:', 0.683402) ('Testing Accuracy:', 0.58333331)\n",
      "Step 10: loss = 80883845.25 (19.754 sec) ('Training Accuracy:', 0.63057327) ('Testing Accuracy:', 0.54333335)\n",
      "Step 11: loss = 76445174.25 (23.017 sec) ('Training Accuracy:', 0.6301986) ('Testing Accuracy:', 0.52666664)\n",
      "Step 12: loss = 70804721.50 (18.260 sec) ('Training Accuracy:', 0.65567631) ('Testing Accuracy:', 0.55000001)\n",
      "Step 13: loss = 65426590.75 (19.133 sec) ('Training Accuracy:', 0.71599853) ('Testing Accuracy:', 0.57999998)\n",
      "Step 14: loss = 58496118.25 (22.298 sec) ('Training Accuracy:', 0.72461593) ('Testing Accuracy:', 0.56666666)\n",
      "Step 15: loss = 56644456.00 (19.309 sec) ('Training Accuracy:', 0.71749717) ('Testing Accuracy:', 0.56333333)\n",
      "Step 16: loss = 58432280.50 (20.469 sec) ('Training Accuracy:', 0.64668417) ('Testing Accuracy:', 0.51999998)\n",
      "Step 17: loss = 55783294.38 (22.985 sec) ('Training Accuracy:', 0.68415135) ('Testing Accuracy:', 0.56)\n",
      "Step 18: loss = 48997586.62 (22.081 sec) ('Training Accuracy:', 0.73510677) ('Testing Accuracy:', 0.57333332)\n",
      "Step 19: loss = 42381009.88 (20.846 sec) ('Training Accuracy:', 0.7602098) ('Testing Accuracy:', 0.56999999)\n",
      "Step 20: loss = 39973975.25 (18.261 sec) ('Training Accuracy:', 0.76807791) ('Testing Accuracy:', 0.57666665)\n",
      "Step 21: loss = 39958214.75 (20.391 sec) ('Training Accuracy:', 0.76058447) ('Testing Accuracy:', 0.57666665)\n",
      "Step 22: loss = 46518738.62 (20.704 sec) ('Training Accuracy:', 0.65642565) ('Testing Accuracy:', 0.52999997)\n",
      "Step 23: loss = 47077415.62 (17.999 sec) ('Training Accuracy:', 0.71674782) ('Testing Accuracy:', 0.58333331)\n",
      "Step 24: loss = 38812797.19 (18.560 sec) ('Training Accuracy:', 0.78643686) ('Testing Accuracy:', 0.57666665)\n",
      "Step 25: loss = 30932883.81 (18.507 sec) ('Training Accuracy:', 0.79617834) ('Testing Accuracy:', 0.58666664)\n",
      "Step 26: loss = 29650082.25 (17.934 sec) ('Training Accuracy:', 0.79243159) ('Testing Accuracy:', 0.57333332)\n",
      "Step 27: loss = 34410482.25 (18.509 sec) ('Training Accuracy:', 0.73023605) ('Testing Accuracy:', 0.56999999)\n",
      "Step 28: loss = 32995481.94 (19.420 sec) ('Training Accuracy:', 0.6878981) ('Testing Accuracy:', 0.54666668)\n",
      "Step 29: loss = 33173641.38 (21.079 sec) ('Training Accuracy:', 0.8017984) ('Testing Accuracy:', 0.56333333)\n",
      "Step 30: loss = 30599068.00 (26.529 sec) ('Training Accuracy:', 0.80816787) ('Testing Accuracy:', 0.5933333)\n",
      "Step 31: loss = 26812000.25 (24.931 sec) ('Training Accuracy:', 0.81528664) ('Testing Accuracy:', 0.58666664)\n",
      "Step 32: loss = 29528569.69 (24.281 sec) ('Training Accuracy:', 0.72686398) ('Testing Accuracy:', 0.55333334)\n",
      "Step 33: loss = 29149880.19 (25.218 sec) ('Training Accuracy:', 0.77969277) ('Testing Accuracy:', 0.58333331)\n",
      "Step 34: loss = 22667415.34 (25.822 sec) ('Training Accuracy:', 0.83626825) ('Testing Accuracy:', 0.56)\n",
      "Step 35: loss = 18673262.44 (27.346 sec) ('Training Accuracy:', 0.83626825) ('Testing Accuracy:', 0.56)\n",
      "Step 36: loss = 20855827.88 (25.101 sec) ('Training Accuracy:', 0.83139753) ('Testing Accuracy:', 0.56999999)\n",
      "Step 37: loss = 22282441.66 (23.905 sec) ('Training Accuracy:', 0.78418881) ('Testing Accuracy:', 0.56)\n",
      "Step 38: loss = 22829820.62 (26.225 sec) ('Training Accuracy:', 0.71375048) ('Testing Accuracy:', 0.55333334)\n",
      "Step 39: loss = 22245457.81 (26.954 sec) ('Training Accuracy:', 0.83814162) ('Testing Accuracy:', 0.56)\n",
      "Step 40: loss = 19963534.31 (24.358 sec) ('Training Accuracy:', 0.84713376) ('Testing Accuracy:', 0.57666665)\n",
      "Step 41: loss = 19722497.25 (26.249 sec) ('Training Accuracy:', 0.79505432) ('Testing Accuracy:', 0.62)\n",
      "Step 42: loss = 23510871.66 (24.984 sec) ('Training Accuracy:', 0.81978267) ('Testing Accuracy:', 0.56333333)\n",
      "Step 43: loss = 19166439.72 (26.057 sec) ('Training Accuracy:', 0.77856874) ('Testing Accuracy:', 0.56666666)\n",
      "Step 44: loss = 15818942.69 (25.816 sec) ('Training Accuracy:', 0.85650057) ('Testing Accuracy:', 0.55666667)\n",
      "Step 45: loss = 13190035.11 (24.819 sec) ('Training Accuracy:', 0.85387784) ('Testing Accuracy:', 0.57999998)\n",
      "Step 46: loss = 13806201.25 (24.695 sec) ('Training Accuracy:', 0.863994) ('Testing Accuracy:', 0.56333333)\n",
      "Step 47: loss = 14714919.06 (25.895 sec) ('Training Accuracy:', 0.80591983) ('Testing Accuracy:', 0.57333332)\n",
      "Step 48: loss = 14885947.03 (25.921 sec) ('Training Accuracy:', 0.84001499) ('Testing Accuracy:', 0.56999999)\n",
      "Step 49: loss = 16717305.72 (25.257 sec) ('Training Accuracy:', 0.8077932) ('Testing Accuracy:', 0.62)\n",
      "Step 50: loss = 18009347.41 (25.201 sec) ('Training Accuracy:', 0.88047957) ('Testing Accuracy:', 0.57666665)\n",
      "Step 51: loss = 11516599.02 (25.491 sec) ('Training Accuracy:', 0.85500187) ('Testing Accuracy:', 0.57999998)\n",
      "Step 52: loss = 9653285.82 (22.732 sec) ('Training Accuracy:', 0.85949796) ('Testing Accuracy:', 0.57333332)\n",
      "Step 53: loss = 9678017.12 (20.977 sec) ('Training Accuracy:', 0.8786062) ('Testing Accuracy:', 0.57333332)\n",
      "Step 54: loss = 10106953.42 (24.663 sec) ('Training Accuracy:', 0.83476955) ('Testing Accuracy:', 0.61000001)\n",
      "Step 55: loss = 12767426.06 (20.331 sec) ('Training Accuracy:', 0.88909703) ('Testing Accuracy:', 0.56999999)\n",
      "Step 56: loss = 12256964.94 (18.805 sec) ('Training Accuracy:', 0.85275382) ('Testing Accuracy:', 0.57666665)\n",
      "Step 57: loss = 13983117.12 (18.510 sec) ('Training Accuracy:', 0.87223679) ('Testing Accuracy:', 0.57666665)\n",
      "Step 58: loss = 10849780.32 (18.433 sec) ('Training Accuracy:', 0.85462719) ('Testing Accuracy:', 0.61000001)\n",
      "Step 59: loss = 11475276.03 (18.343 sec) ('Training Accuracy:', 0.88047957) ('Testing Accuracy:', 0.60000002)\n",
      "Step 60: loss = 10135785.66 (18.627 sec) ('Training Accuracy:', 0.89958787) ('Testing Accuracy:', 0.57333332)\n",
      "Step 61: loss = 13481515.30 (19.093 sec) ('Training Accuracy:', 0.88235295) ('Testing Accuracy:', 0.57333332)\n",
      "Step 62: loss = 8455843.21 (18.362 sec) ('Training Accuracy:', 0.86137128) ('Testing Accuracy:', 0.60333335)\n",
      "Step 63: loss = 8857971.64 (17.993 sec) ('Training Accuracy:', 0.89921319) ('Testing Accuracy:', 0.57666665)\n",
      "Step 64: loss = 7981046.94 (18.455 sec) ('Training Accuracy:', 0.90071189) ('Testing Accuracy:', 0.57666665)\n",
      "Step 65: loss = 8506238.92 (18.654 sec) ('Training Accuracy:', 0.89921319) ('Testing Accuracy:', 0.56999999)\n",
      "Step 66: loss = 7148400.58 (18.652 sec) ('Training Accuracy:', 0.83739227) ('Testing Accuracy:', 0.61333334)\n",
      "Step 67: loss = 7399944.48 (18.475 sec) ('Training Accuracy:', 0.8801049) ('Testing Accuracy:', 0.5933333)\n",
      "Step 68: loss = 7592826.45 (18.734 sec) ('Training Accuracy:', 0.90146124) ('Testing Accuracy:', 0.58333331)\n",
      "Step 69: loss = 8530550.36 (18.594 sec) ('Training Accuracy:', 0.90333456) ('Testing Accuracy:', 0.57999998)\n",
      "Step 70: loss = 8917605.17 (18.609 sec) ('Training Accuracy:', 0.88759834) ('Testing Accuracy:', 0.5933333)\n",
      "Step 71: loss = 9181566.79 (19.142 sec) ('Training Accuracy:', 0.76395655) ('Testing Accuracy:', 0.63)\n",
      "Step 72: loss = 11113199.07 (18.253 sec) ('Training Accuracy:', 0.89171976) ('Testing Accuracy:', 0.5933333)\n",
      "Step 73: loss = 10964444.16 (18.636 sec) ('Training Accuracy:', 0.89958787) ('Testing Accuracy:', 0.57999998)\n",
      "Step 74: loss = 10920355.54 (18.682 sec) ('Training Accuracy:', 0.86624205) ('Testing Accuracy:', 0.60000002)\n",
      "Step 75: loss = 9520502.06 (19.101 sec) ('Training Accuracy:', 0.75271636) ('Testing Accuracy:', 0.63999999)\n",
      "Step 76: loss = 12823183.77 (19.034 sec) ('Training Accuracy:', 0.89359313) ('Testing Accuracy:', 0.5933333)\n",
      "Step 77: loss = 14376796.55 (18.359 sec) ('Training Accuracy:', 0.85799927) ('Testing Accuracy:', 0.60333335)\n",
      "Step 78: loss = 10358715.42 (18.704 sec) ('Training Accuracy:', 0.75683779) ('Testing Accuracy:', 0.63)\n",
      "Step 79: loss = 9613111.28 (18.739 sec) ('Training Accuracy:', 0.81491196) ('Testing Accuracy:', 0.63999999)\n",
      "Step 80: loss = 13734371.75 (18.540 sec) ('Training Accuracy:', 0.87448484) ('Testing Accuracy:', 0.61666667)\n",
      "Step 81: loss = 16758030.50 (18.828 sec) ('Training Accuracy:', 0.81266391) ('Testing Accuracy:', 0.61666667)\n",
      "Step 82: loss = 13494503.95 (18.513 sec) ('Training Accuracy:', 0.69426751) ('Testing Accuracy:', 0.64333332)\n",
      "Step 83: loss = 19960144.81 (18.549 sec) ('Training Accuracy:', 0.81303859) ('Testing Accuracy:', 0.63333333)\n",
      "Step 84: loss = 29600476.73 (18.369 sec) ('Training Accuracy:', 0.7010116) ('Testing Accuracy:', 0.64999998)\n",
      "Step 85: loss = 29474308.66 (18.691 sec) ('Training Accuracy:', 0.70063692) ('Testing Accuracy:', 0.65333331)\n",
      "Step 86: loss = 54626339.88 (18.969 sec) ('Training Accuracy:', 0.83889097) ('Testing Accuracy:', 0.62333333)\n",
      "Step 87: loss = 79810417.00 (18.407 sec) ('Training Accuracy:', 0.55638814) ('Testing Accuracy:', 0.47999999)\n",
      "Step 88: loss = 51759860.06 (18.392 sec) ('Training Accuracy:', 0.72386664) ('Testing Accuracy:', 0.52666664)\n",
      "Step 89: loss = 19125332.47 (17.981 sec) ('Training Accuracy:', 0.80966651) ('Testing Accuracy:', 0.51999998)\n",
      "Step 90: loss = 10198429.27 (17.612 sec) ('Training Accuracy:', 0.8254028) ('Testing Accuracy:', 0.53333336)\n",
      "Step 91: loss = 5774459.58 (18.848 sec) ('Training Accuracy:', 0.88385165) ('Testing Accuracy:', 0.53333336)\n",
      "Step 92: loss = 3832364.97 (18.248 sec) ('Training Accuracy:', 0.89996254) ('Testing Accuracy:', 0.55333334)\n",
      "Step 93: loss = 3600217.03 (18.402 sec) ('Training Accuracy:', 0.84825778) ('Testing Accuracy:', 0.54333335)\n",
      "Step 94: loss = 2834735.17 (18.749 sec) ('Training Accuracy:', 0.91270137) ('Testing Accuracy:', 0.56333333)\n",
      "Step 95: loss = 2483986.63 (21.205 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.5933333)\n",
      "Step 96: loss = 2488870.22 (18.592 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.58999997)\n",
      "Step 97: loss = 6099974.95 (24.486 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.58666664)\n",
      "Step 98: loss = 10518841.50 (18.998 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.58666664)\n",
      "Step 99: loss = 19011843.37 (18.609 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.57999998)\n",
      "Step 100: loss = 26509186.40 (18.661 sec) ('Training Accuracy:', 0.88797301) ('Testing Accuracy:', 0.56333333)\n",
      "Step 101: loss = 34970640.25 (18.755 sec) ('Training Accuracy:', 0.70513302) ('Testing Accuracy:', 0.50999999)\n",
      "Step 102: loss = 33019193.95 (20.832 sec) ('Training Accuracy:', 0.61034095) ('Testing Accuracy:', 0.51333332)\n",
      "Step 103: loss = 33624793.92 (22.864 sec) ('Training Accuracy:', 0.59722745) ('Testing Accuracy:', 0.49666667)\n",
      "Step 104: loss = 29889762.48 (26.473 sec) ('Training Accuracy:', 0.63694268) ('Testing Accuracy:', 0.52333331)\n",
      "Step 105: loss = 25318065.02 (20.331 sec) ('Training Accuracy:', 0.78493816) ('Testing Accuracy:', 0.52999997)\n",
      "Step 106: loss = 16578960.23 (18.543 sec) ('Training Accuracy:', 0.93518174) ('Testing Accuracy:', 0.57333332)\n",
      "Step 107: loss = 8810454.80 (18.595 sec) ('Training Accuracy:', 0.91532409) ('Testing Accuracy:', 0.58999997)\n",
      "Step 108: loss = 4014625.62 (18.779 sec) ('Training Accuracy:', 0.9437992) ('Testing Accuracy:', 0.55333334)\n",
      "Step 109: loss = 2028400.20 (19.126 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.57666665)\n",
      "Step 110: loss = 1291151.69 (20.425 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.55666667)\n",
      "Step 111: loss = 1005738.67 (22.640 sec) ('Training Accuracy:', 0.94267517) ('Testing Accuracy:', 0.56)\n",
      "Step 112: loss = 881171.88 (22.072 sec) ('Training Accuracy:', 0.96028477) ('Testing Accuracy:', 0.58666664)\n",
      "Step 113: loss = 726453.29 (18.441 sec) ('Training Accuracy:', 0.95653802) ('Testing Accuracy:', 0.56)\n",
      "Step 114: loss = 808755.73 (18.473 sec) ('Training Accuracy:', 0.95653802) ('Testing Accuracy:', 0.56)\n",
      "Step 115: loss = 818544.90 (18.560 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.55666667)\n",
      "Step 116: loss = 742247.03 (18.486 sec) ('Training Accuracy:', 0.94792056) ('Testing Accuracy:', 0.57333332)\n",
      "Step 117: loss = 546955.98 (18.538 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.56)\n",
      "Step 118: loss = 500832.01 (18.696 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.56)\n",
      "Step 119: loss = 477957.42 (18.407 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.56999999)\n",
      "Step 120: loss = 530397.55 (18.263 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.54666668)\n",
      "Step 121: loss = 520169.84 (18.710 sec) ('Training Accuracy:', 0.90970403) ('Testing Accuracy:', 0.54666668)\n",
      "Step 122: loss = 720789.46 (18.322 sec) ('Training Accuracy:', 0.95616335) ('Testing Accuracy:', 0.58666664)\n",
      "Step 123: loss = 476493.73 (20.140 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.56)\n",
      "Step 124: loss = 434232.17 (20.813 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.54666668)\n",
      "Step 125: loss = 386569.96 (19.303 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.55333334)\n",
      "Step 126: loss = 505985.57 (18.582 sec) ('Training Accuracy:', 0.91644812) ('Testing Accuracy:', 0.56333333)\n",
      "Step 127: loss = 452760.37 (19.209 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.55000001)\n",
      "Step 128: loss = 667435.76 (19.690 sec) ('Training Accuracy:', 0.9393031) ('Testing Accuracy:', 0.56)\n",
      "Step 129: loss = 737116.00 (19.419 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.57666665)\n",
      "Step 130: loss = 791851.41 (18.720 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.56333333)\n",
      "Step 131: loss = 1062663.68 (19.156 sec) ('Training Accuracy:', 0.93330836) ('Testing Accuracy:', 0.55000001)\n",
      "Step 132: loss = 1549395.81 (18.852 sec) ('Training Accuracy:', 0.89696515) ('Testing Accuracy:', 0.55666667)\n",
      "Step 133: loss = 948608.04 (19.588 sec) ('Training Accuracy:', 0.91120267) ('Testing Accuracy:', 0.56333333)\n",
      "Step 134: loss = 905151.49 (18.816 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.57999998)\n",
      "Step 135: loss = 449450.18 (19.002 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.56)\n",
      "Step 136: loss = 411582.90 (18.578 sec) ('Training Accuracy:', 0.93180966) ('Testing Accuracy:', 0.55666667)\n",
      "Step 137: loss = 487857.95 (19.103 sec) ('Training Accuracy:', 0.92881227) ('Testing Accuracy:', 0.56)\n",
      "Step 138: loss = 485285.46 (19.355 sec) ('Training Accuracy:', 0.95466465) ('Testing Accuracy:', 0.57333332)\n",
      "Step 139: loss = 512884.69 (20.375 sec) ('Training Accuracy:', 0.91082805) ('Testing Accuracy:', 0.55666667)\n",
      "Step 140: loss = 428029.55 (19.056 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.55000001)\n",
      "Step 141: loss = 524102.02 (18.614 sec) ('Training Accuracy:', 0.90595728) ('Testing Accuracy:', 0.56333333)\n",
      "Step 142: loss = 526997.01 (19.216 sec) ('Training Accuracy:', 0.95578867) ('Testing Accuracy:', 0.56999999)\n",
      "Step 143: loss = 346189.91 (19.781 sec) ('Training Accuracy:', 0.94042712) ('Testing Accuracy:', 0.56666666)\n",
      "Step 144: loss = 407018.55 (20.084 sec) ('Training Accuracy:', 0.9246909) ('Testing Accuracy:', 0.56666666)\n",
      "Step 145: loss = 583977.98 (20.186 sec) ('Training Accuracy:', 0.96065944) ('Testing Accuracy:', 0.57333332)\n",
      "Step 146: loss = 641364.68 (20.337 sec) ('Training Accuracy:', 0.93443239) ('Testing Accuracy:', 0.56)\n",
      "Step 147: loss = 418056.84 (20.494 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.56)\n",
      "Step 148: loss = 484374.75 (20.290 sec) ('Training Accuracy:', 0.91607344) ('Testing Accuracy:', 0.56333333)\n",
      "Step 149: loss = 450067.21 (20.135 sec) ('Training Accuracy:', 0.94792056) ('Testing Accuracy:', 0.56)\n",
      "Step 150: loss = 472774.74 (20.517 sec) ('Training Accuracy:', 0.91494942) ('Testing Accuracy:', 0.55333334)\n",
      "Step 151: loss = 497126.85 (20.328 sec) ('Training Accuracy:', 0.95578867) ('Testing Accuracy:', 0.56666666)\n",
      "Step 152: loss = 490250.77 (20.691 sec) ('Training Accuracy:', 0.90370923) ('Testing Accuracy:', 0.55666667)\n",
      "Step 153: loss = 368840.56 (20.495 sec) ('Training Accuracy:', 0.92956161) ('Testing Accuracy:', 0.55000001)\n",
      "Step 154: loss = 535823.58 (19.908 sec) ('Training Accuracy:', 0.92806292) ('Testing Accuracy:', 0.55666667)\n",
      "Step 155: loss = 482863.35 (19.886 sec) ('Training Accuracy:', 0.92731363) ('Testing Accuracy:', 0.56333333)\n",
      "Step 156: loss = 347522.20 (19.304 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.56666666)\n",
      "Step 157: loss = 695685.58 (20.076 sec) ('Training Accuracy:', 0.92806292) ('Testing Accuracy:', 0.56666666)\n",
      "Step 158: loss = 380563.73 (19.875 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.56333333)\n",
      "Step 159: loss = 530029.11 (19.948 sec) ('Training Accuracy:', 0.88797301) ('Testing Accuracy:', 0.55333334)\n",
      "Step 160: loss = 708251.66 (20.051 sec) ('Training Accuracy:', 0.89808917) ('Testing Accuracy:', 0.56333333)\n",
      "Step 161: loss = 634554.94 (19.921 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.56999999)\n",
      "Step 162: loss = 457547.76 (19.983 sec) ('Training Accuracy:', 0.94192582) ('Testing Accuracy:', 0.58333331)\n",
      "Step 163: loss = 412397.88 (19.437 sec) ('Training Accuracy:', 0.92881227) ('Testing Accuracy:', 0.56666666)\n",
      "Step 164: loss = 475227.25 (19.914 sec) ('Training Accuracy:', 0.91157734) ('Testing Accuracy:', 0.55666667)\n",
      "Step 165: loss = 325684.73 (19.952 sec) ('Training Accuracy:', 0.92056948) ('Testing Accuracy:', 0.55666667)\n",
      "Step 166: loss = 543957.46 (20.221 sec) ('Training Accuracy:', 0.91982013) ('Testing Accuracy:', 0.56666666)\n",
      "Step 167: loss = 628894.66 (19.794 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.56333333)\n",
      "Step 168: loss = 348035.80 (19.634 sec) ('Training Accuracy:', 0.90445858) ('Testing Accuracy:', 0.55000001)\n",
      "Step 169: loss = 495778.09 (19.877 sec) ('Training Accuracy:', 0.95616335) ('Testing Accuracy:', 0.57333332)\n",
      "Step 170: loss = 386648.65 (19.973 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.56333333)\n",
      "Step 171: loss = 427082.45 (20.438 sec) ('Training Accuracy:', 0.9261896) ('Testing Accuracy:', 0.55333334)\n",
      "Step 172: loss = 332340.49 (19.964 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.56999999)\n",
      "Step 173: loss = 554492.00 (20.348 sec) ('Training Accuracy:', 0.94042712) ('Testing Accuracy:', 0.57333332)\n",
      "Step 174: loss = 669353.57 (19.676 sec) ('Training Accuracy:', 0.93368304) ('Testing Accuracy:', 0.56999999)\n",
      "Step 175: loss = 722732.74 (19.810 sec) ('Training Accuracy:', 0.90108657) ('Testing Accuracy:', 0.56)\n",
      "Step 176: loss = 569300.93 (20.043 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.57333332)\n",
      "Step 177: loss = 702314.55 (19.776 sec) ('Training Accuracy:', 0.92544025) ('Testing Accuracy:', 0.57333332)\n",
      "Step 178: loss = 434440.93 (19.939 sec) ('Training Accuracy:', 0.90783066) ('Testing Accuracy:', 0.55666667)\n",
      "Step 179: loss = 661501.71 (19.651 sec) ('Training Accuracy:', 0.87148744) ('Testing Accuracy:', 0.55666667)\n",
      "Step 180: loss = 753493.39 (20.084 sec) ('Training Accuracy:', 0.89059573) ('Testing Accuracy:', 0.56)\n",
      "Step 181: loss = 2278272.88 (19.722 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.56999999)\n",
      "Step 182: loss = 965835.36 (19.282 sec) ('Training Accuracy:', 0.91569877) ('Testing Accuracy:', 0.56333333)\n",
      "Step 183: loss = 453197.38 (19.867 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.57999998)\n",
      "Step 184: loss = 447842.86 (19.383 sec) ('Training Accuracy:', 0.9408018) ('Testing Accuracy:', 0.56333333)\n",
      "Step 185: loss = 247947.00 (20.147 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.58333331)\n",
      "Step 186: loss = 413876.28 (19.523 sec) ('Training Accuracy:', 0.90745598) ('Testing Accuracy:', 0.56666666)\n",
      "Step 187: loss = 552991.07 (19.597 sec) ('Training Accuracy:', 0.92881227) ('Testing Accuracy:', 0.56999999)\n",
      "Step 188: loss = 765477.68 (19.745 sec) ('Training Accuracy:', 0.96140879) ('Testing Accuracy:', 0.56999999)\n",
      "Step 189: loss = 673428.16 (19.387 sec) ('Training Accuracy:', 0.94979393) ('Testing Accuracy:', 0.56666666)\n",
      "Step 190: loss = 571760.30 (19.633 sec) ('Training Accuracy:', 0.94979393) ('Testing Accuracy:', 0.55666667)\n",
      "Step 191: loss = 314596.90 (19.286 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.56999999)\n",
      "Step 192: loss = 497101.23 (19.703 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.55333334)\n",
      "Step 193: loss = 484139.30 (19.459 sec) ('Training Accuracy:', 0.91345072) ('Testing Accuracy:', 0.56)\n",
      "Step 194: loss = 416408.98 (19.741 sec) ('Training Accuracy:', 0.95466465) ('Testing Accuracy:', 0.56)\n",
      "Step 195: loss = 589691.84 (19.978 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.55666667)\n",
      "Step 196: loss = 346290.78 (19.859 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.56333333)\n",
      "Step 197: loss = 285855.92 (19.648 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.56666666)\n",
      "Step 198: loss = 357525.41 (19.589 sec) ('Training Accuracy:', 0.93180966) ('Testing Accuracy:', 0.56)\n",
      "Step 199: loss = 264433.79 (19.996 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.56999999)\n",
      "Step 200: loss = 305804.99 (19.117 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.56)\n",
      "Step 201: loss = 372789.33 (19.643 sec) ('Training Accuracy:', 0.93817908) ('Testing Accuracy:', 0.56666666)\n",
      "Step 202: loss = 224685.29 (19.587 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.57666665)\n",
      "Step 203: loss = 308731.07 (19.678 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.56)\n",
      "Step 204: loss = 255179.55 (20.490 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.56666666)\n",
      "Step 205: loss = 414352.77 (19.977 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.56666666)\n",
      "Step 206: loss = 422845.05 (19.650 sec) ('Training Accuracy:', 0.96740353) ('Testing Accuracy:', 0.56333333)\n",
      "Step 207: loss = 404293.19 (20.274 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.56666666)\n",
      "Step 208: loss = 495015.66 (20.134 sec) ('Training Accuracy:', 0.96140879) ('Testing Accuracy:', 0.56999999)\n",
      "Step 209: loss = 253650.08 (20.036 sec) ('Training Accuracy:', 0.9216935) ('Testing Accuracy:', 0.56666666)\n",
      "Step 210: loss = 360968.85 (19.897 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.56999999)\n",
      "Step 211: loss = 448394.76 (19.948 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.55666667)\n",
      "Step 212: loss = 280270.40 (19.744 sec) ('Training Accuracy:', 0.95279133) ('Testing Accuracy:', 0.56333333)\n",
      "Step 213: loss = 334648.75 (20.788 sec) ('Training Accuracy:', 0.94042712) ('Testing Accuracy:', 0.56333333)\n",
      "Step 214: loss = 265454.50 (20.290 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.56)\n",
      "Step 215: loss = 453869.40 (20.064 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.57666665)\n",
      "Step 216: loss = 328018.92 (20.195 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.56666666)\n",
      "Step 217: loss = 225655.80 (20.313 sec) ('Training Accuracy:', 0.92206818) ('Testing Accuracy:', 0.56333333)\n",
      "Step 218: loss = 437707.95 (20.332 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.55666667)\n",
      "Step 219: loss = 259201.14 (20.384 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.55666667)\n",
      "Step 220: loss = 428738.93 (19.990 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.56666666)\n",
      "Step 221: loss = 570143.33 (19.912 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.56)\n",
      "Step 222: loss = 840541.49 (25.522 sec) ('Training Accuracy:', 0.88535035) ('Testing Accuracy:', 0.55333334)\n",
      "Step 223: loss = 1469882.89 (24.638 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.56999999)\n",
      "Step 224: loss = 525639.05 (24.045 sec) ('Training Accuracy:', 0.95578867) ('Testing Accuracy:', 0.57333332)\n",
      "Step 225: loss = 410434.89 (24.219 sec) ('Training Accuracy:', 0.92956161) ('Testing Accuracy:', 0.56333333)\n",
      "Step 226: loss = 628714.96 (20.389 sec) ('Training Accuracy:', 0.93817908) ('Testing Accuracy:', 0.56333333)\n",
      "Step 227: loss = 543289.63 (24.148 sec) ('Training Accuracy:', 0.97152489) ('Testing Accuracy:', 0.57999998)\n",
      "Step 228: loss = 608845.15 (24.604 sec) ('Training Accuracy:', 0.91232669) ('Testing Accuracy:', 0.56333333)\n",
      "Step 229: loss = 672304.75 (22.251 sec) ('Training Accuracy:', 0.91345072) ('Testing Accuracy:', 0.56333333)\n",
      "Step 230: loss = 996096.17 (20.501 sec) ('Training Accuracy:', 0.93330836) ('Testing Accuracy:', 0.55000001)\n",
      "Step 231: loss = 1526849.56 (19.566 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.5933333)\n",
      "Step 232: loss = 3809939.59 (19.218 sec) ('Training Accuracy:', 0.94267517) ('Testing Accuracy:', 0.56333333)\n",
      "Step 233: loss = 3389452.82 (19.979 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.55333334)\n",
      "Step 234: loss = 2183025.43 (19.541 sec) ('Training Accuracy:', 0.8624953) ('Testing Accuracy:', 0.64333332)\n",
      "Step 235: loss = 5238474.12 (19.236 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.57333332)\n",
      "Step 236: loss = 9339312.41 (19.450 sec) ('Training Accuracy:', 0.86736608) ('Testing Accuracy:', 0.63666666)\n",
      "Step 237: loss = 4565297.18 (19.274 sec) ('Training Accuracy:', 0.87448484) ('Testing Accuracy:', 0.61666667)\n",
      "Step 238: loss = 8421358.66 (19.032 sec) ('Training Accuracy:', 0.89921319) ('Testing Accuracy:', 0.62)\n",
      "Step 239: loss = 23878209.06 (19.395 sec) ('Training Accuracy:', 0.88422632) ('Testing Accuracy:', 0.64333332)\n",
      "Step 240: loss = 46003665.50 (19.003 sec) ('Training Accuracy:', 0.59010863) ('Testing Accuracy:', 0.47666666)\n",
      "Step 241: loss = 55187746.98 (18.967 sec) ('Training Accuracy:', 0.89996254) ('Testing Accuracy:', 0.55666667)\n",
      "Step 242: loss = 11303700.06 (18.753 sec) ('Training Accuracy:', 0.91832149) ('Testing Accuracy:', 0.62333333)\n",
      "Step 243: loss = 2699350.59 (18.698 sec) ('Training Accuracy:', 0.96553016) ('Testing Accuracy:', 0.57666665)\n",
      "Step 244: loss = 1277992.41 (18.838 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.57333332)\n",
      "Step 245: loss = 1158161.91 (19.072 sec) ('Training Accuracy:', 0.87748218) ('Testing Accuracy:', 0.54666668)\n",
      "Step 246: loss = 2104946.16 (18.744 sec) ('Training Accuracy:', 0.90333456) ('Testing Accuracy:', 0.56)\n",
      "Step 247: loss = 1591197.25 (18.706 sec) ('Training Accuracy:', 0.86624205) ('Testing Accuracy:', 0.55000001)\n",
      "Step 248: loss = 1774371.47 (18.658 sec) ('Training Accuracy:', 0.90820533) ('Testing Accuracy:', 0.56333333)\n",
      "Step 249: loss = 1897967.26 (18.809 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.56)\n",
      "Step 250: loss = 2538284.75 (18.856 sec) ('Training Accuracy:', 0.9393031) ('Testing Accuracy:', 0.56333333)\n",
      "Step 251: loss = 3152986.30 (19.001 sec) ('Training Accuracy:', 0.96627951) ('Testing Accuracy:', 0.56999999)\n",
      "Step 252: loss = 2021620.47 (18.756 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.57333332)\n",
      "Step 253: loss = 1925774.48 (23.611 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.60666668)\n",
      "Step 254: loss = 1530740.54 (23.576 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.57999998)\n",
      "Step 255: loss = 667232.01 (19.331 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.56)\n",
      "Step 256: loss = 445244.97 (23.482 sec) ('Training Accuracy:', 0.95279133) ('Testing Accuracy:', 0.56333333)\n",
      "Step 257: loss = 382514.83 (21.249 sec) ('Training Accuracy:', 0.95054328) ('Testing Accuracy:', 0.56)\n",
      "Step 258: loss = 396881.96 (23.714 sec) ('Training Accuracy:', 0.96590483) ('Testing Accuracy:', 0.56)\n",
      "Step 259: loss = 244298.68 (19.724 sec) ('Training Accuracy:', 0.94679654) ('Testing Accuracy:', 0.55000001)\n",
      "Step 260: loss = 449878.29 (18.746 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.55000001)\n",
      "Step 261: loss = 585870.39 (18.579 sec) ('Training Accuracy:', 0.95616335) ('Testing Accuracy:', 0.56666666)\n",
      "Step 262: loss = 236986.59 (18.984 sec) ('Training Accuracy:', 0.9569127) ('Testing Accuracy:', 0.56333333)\n",
      "Step 263: loss = 358134.38 (19.301 sec) ('Training Accuracy:', 0.94042712) ('Testing Accuracy:', 0.54666668)\n",
      "Step 264: loss = 504308.16 (19.020 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.56)\n",
      "Step 265: loss = 601731.66 (19.249 sec) ('Training Accuracy:', 0.96103412) ('Testing Accuracy:', 0.55000001)\n",
      "Step 266: loss = 392540.63 (18.656 sec) ('Training Accuracy:', 0.92356688) ('Testing Accuracy:', 0.56)\n",
      "Step 267: loss = 727373.18 (19.079 sec) ('Training Accuracy:', 0.96103412) ('Testing Accuracy:', 0.57999998)\n",
      "Step 268: loss = 303537.76 (18.491 sec) ('Training Accuracy:', 0.94866991) ('Testing Accuracy:', 0.56)\n",
      "Step 269: loss = 285368.65 (19.031 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.56)\n",
      "Step 270: loss = 264224.48 (18.792 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.56333333)\n",
      "Step 271: loss = 288954.95 (18.890 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.57333332)\n",
      "Step 272: loss = 179199.31 (18.494 sec) ('Training Accuracy:', 0.95953542) ('Testing Accuracy:', 0.57333332)\n",
      "Step 273: loss = 207059.68 (18.567 sec) ('Training Accuracy:', 0.96590483) ('Testing Accuracy:', 0.57666665)\n",
      "Step 274: loss = 265274.43 (18.797 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.56999999)\n",
      "Step 275: loss = 412165.29 (18.534 sec) ('Training Accuracy:', 0.94904459) ('Testing Accuracy:', 0.55333334)\n",
      "Step 276: loss = 270334.72 (18.751 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.56)\n",
      "Step 277: loss = 209313.73 (18.654 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.56333333)\n",
      "Step 278: loss = 294275.04 (19.170 sec) ('Training Accuracy:', 0.94642186) ('Testing Accuracy:', 0.55333334)\n",
      "Step 279: loss = 386953.01 (19.307 sec) ('Training Accuracy:', 0.93031096) ('Testing Accuracy:', 0.55333334)\n",
      "Step 280: loss = 428894.59 (19.611 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.56)\n",
      "Step 281: loss = 319422.20 (19.954 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.56)\n",
      "Step 282: loss = 345292.67 (19.692 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.56)\n",
      "Step 283: loss = 361160.38 (20.102 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.56333333)\n",
      "Step 284: loss = 357877.72 (20.073 sec) ('Training Accuracy:', 0.94117647) ('Testing Accuracy:', 0.55666667)\n",
      "Step 285: loss = 332382.53 (20.017 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.56333333)\n",
      "Step 286: loss = 246122.60 (20.099 sec) ('Training Accuracy:', 0.95653802) ('Testing Accuracy:', 0.56333333)\n",
      "Step 287: loss = 262981.84 (20.658 sec) ('Training Accuracy:', 0.96253276) ('Testing Accuracy:', 0.57999998)\n",
      "Step 288: loss = 262501.74 (20.169 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.56333333)\n",
      "Step 289: loss = 277193.47 (20.705 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.56)\n",
      "Step 290: loss = 205147.18 (20.359 sec) ('Training Accuracy:', 0.94866991) ('Testing Accuracy:', 0.55666667)\n",
      "Step 291: loss = 366031.94 (20.238 sec) ('Training Accuracy:', 0.95728737) ('Testing Accuracy:', 0.56666666)\n",
      "Step 292: loss = 510765.25 (21.488 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.55333334)\n",
      "Step 293: loss = 339358.57 (21.577 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.5933333)\n",
      "Step 294: loss = 393983.11 (20.947 sec) ('Training Accuracy:', 0.94454855) ('Testing Accuracy:', 0.56333333)\n",
      "Step 295: loss = 455986.42 (21.576 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.57999998)\n",
      "Step 296: loss = 492631.43 (20.852 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.56999999)\n",
      "Step 297: loss = 330349.36 (20.680 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.56999999)\n",
      "Step 298: loss = 418616.94 (20.569 sec) ('Training Accuracy:', 0.97452229) ('Testing Accuracy:', 0.57999998)\n",
      "Step 299: loss = 418981.60 (20.743 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.56999999)\n",
      "Step 300: loss = 321820.07 (21.246 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.57666665)\n",
      "Step 301: loss = 590561.46 (20.901 sec) ('Training Accuracy:', 0.94567251) ('Testing Accuracy:', 0.55333334)\n",
      "Step 302: loss = 915065.43 (20.220 sec) ('Training Accuracy:', 0.91644812) ('Testing Accuracy:', 0.56)\n",
      "Step 303: loss = 739542.97 (20.692 sec) ('Training Accuracy:', 0.88535035) ('Testing Accuracy:', 0.56)\n",
      "Step 304: loss = 1182975.73 (20.356 sec) ('Training Accuracy:', 0.95653802) ('Testing Accuracy:', 0.56999999)\n",
      "Step 305: loss = 277350.03 (20.569 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.56666666)\n",
      "Step 306: loss = 2299128.64 (20.046 sec) ('Training Accuracy:', 0.96927691) ('Testing Accuracy:', 0.58333331)\n",
      "Step 307: loss = 492118.19 (20.576 sec) ('Training Accuracy:', 0.96403146) ('Testing Accuracy:', 0.5933333)\n",
      "Step 308: loss = 767958.64 (19.996 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.54000002)\n",
      "Step 309: loss = 716800.89 (20.211 sec) ('Training Accuracy:', 0.91270137) ('Testing Accuracy:', 0.55000001)\n",
      "Step 310: loss = 1289987.46 (20.053 sec) ('Training Accuracy:', 0.87298614) ('Testing Accuracy:', 0.54666668)\n",
      "Step 311: loss = 687521.51 (19.648 sec) ('Training Accuracy:', 0.88122892) ('Testing Accuracy:', 0.56333333)\n",
      "Step 312: loss = 1376906.65 (21.062 sec) ('Training Accuracy:', 0.93405771) ('Testing Accuracy:', 0.56666666)\n",
      "Step 313: loss = 1476966.94 (19.497 sec) ('Training Accuracy:', 0.91307604) ('Testing Accuracy:', 0.54000002)\n",
      "Step 314: loss = 377234.00 (19.766 sec) ('Training Accuracy:', 0.91195202) ('Testing Accuracy:', 0.55000001)\n",
      "Step 315: loss = 1771370.27 (19.883 sec) ('Training Accuracy:', 0.83889097) ('Testing Accuracy:', 0.56333333)\n",
      "Step 316: loss = 1546759.52 (19.775 sec) ('Training Accuracy:', 0.90932935) ('Testing Accuracy:', 0.55333334)\n",
      "Step 317: loss = 1247847.20 (19.207 sec) ('Training Accuracy:', 0.95953542) ('Testing Accuracy:', 0.57999998)\n",
      "Step 318: loss = 350348.94 (19.654 sec) ('Training Accuracy:', 0.89846385) ('Testing Accuracy:', 0.55666667)\n",
      "Step 319: loss = 951479.44 (19.350 sec) ('Training Accuracy:', 0.8801049) ('Testing Accuracy:', 0.56)\n",
      "Step 320: loss = 2382463.16 (19.314 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.58666664)\n",
      "Step 321: loss = 1883305.55 (19.486 sec) ('Training Accuracy:', 0.91494942) ('Testing Accuracy:', 0.54333335)\n",
      "Step 322: loss = 2174300.59 (20.473 sec) ('Training Accuracy:', 0.86024725) ('Testing Accuracy:', 0.54000002)\n",
      "Step 323: loss = 2349490.51 (21.101 sec) ('Training Accuracy:', 0.93443239) ('Testing Accuracy:', 0.57333332)\n",
      "Step 324: loss = 1631848.65 (23.972 sec) ('Training Accuracy:', 0.91719747) ('Testing Accuracy:', 0.57666665)\n",
      "Step 325: loss = 826275.38 (24.338 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.56333333)\n",
      "Step 326: loss = 934518.31 (21.978 sec) ('Training Accuracy:', 0.86511803) ('Testing Accuracy:', 0.53666669)\n",
      "Step 327: loss = 771802.32 (21.900 sec) ('Training Accuracy:', 0.88047957) ('Testing Accuracy:', 0.52999997)\n",
      "Step 328: loss = 1678407.27 (19.368 sec) ('Training Accuracy:', 0.88759834) ('Testing Accuracy:', 0.56666666)\n",
      "Step 329: loss = 953371.66 (19.452 sec) ('Training Accuracy:', 0.92881227) ('Testing Accuracy:', 0.53666669)\n",
      "Step 330: loss = 509733.47 (19.291 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.57666665)\n",
      "Step 331: loss = 559819.01 (19.406 sec) ('Training Accuracy:', 0.9569127) ('Testing Accuracy:', 0.57333332)\n",
      "Step 332: loss = 404575.08 (19.268 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.57333332)\n",
      "Step 333: loss = 371172.84 (19.516 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.57999998)\n",
      "Step 334: loss = 365972.12 (19.094 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.58333331)\n",
      "Step 335: loss = 183500.40 (19.383 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.59666669)\n",
      "Step 336: loss = 279910.67 (19.178 sec) ('Training Accuracy:', 0.97564632) ('Testing Accuracy:', 0.60666668)\n",
      "Step 337: loss = 1240769.55 (19.587 sec) ('Training Accuracy:', 0.88572496) ('Testing Accuracy:', 0.55333334)\n",
      "Step 338: loss = 989629.76 (19.086 sec) ('Training Accuracy:', 0.93106031) ('Testing Accuracy:', 0.58333331)\n",
      "Step 339: loss = 1280432.30 (19.263 sec) ('Training Accuracy:', 0.90370923) ('Testing Accuracy:', 0.55666667)\n",
      "Step 340: loss = 1387839.50 (18.996 sec) ('Training Accuracy:', 0.90071189) ('Testing Accuracy:', 0.54333335)\n",
      "Step 341: loss = 959423.27 (19.474 sec) ('Training Accuracy:', 0.91869617) ('Testing Accuracy:', 0.56666666)\n",
      "Step 342: loss = 1519716.87 (19.637 sec) ('Training Accuracy:', 0.84526038) ('Testing Accuracy:', 0.53666669)\n",
      "Step 343: loss = 3547225.60 (18.827 sec) ('Training Accuracy:', 0.91982013) ('Testing Accuracy:', 0.56)\n",
      "Step 344: loss = 1385736.52 (19.333 sec) ('Training Accuracy:', 0.89321846) ('Testing Accuracy:', 0.55333334)\n",
      "Step 345: loss = 1317245.96 (19.108 sec) ('Training Accuracy:', 0.96065944) ('Testing Accuracy:', 0.5933333)\n",
      "Step 346: loss = 875414.17 (19.316 sec) ('Training Accuracy:', 0.96590483) ('Testing Accuracy:', 0.57666665)\n",
      "Step 347: loss = 653999.89 (19.715 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.54000002)\n",
      "Step 348: loss = 868965.49 (19.463 sec) ('Training Accuracy:', 0.90670663) ('Testing Accuracy:', 0.55333334)\n",
      "Step 349: loss = 1456880.22 (19.644 sec) ('Training Accuracy:', 0.95466465) ('Testing Accuracy:', 0.57999998)\n",
      "Step 350: loss = 656078.70 (18.695 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.58333331)\n",
      "Step 351: loss = 556967.87 (19.757 sec) ('Training Accuracy:', 0.96927691) ('Testing Accuracy:', 0.60666668)\n",
      "Step 352: loss = 844270.74 (19.391 sec) ('Training Accuracy:', 0.97489697) ('Testing Accuracy:', 0.58333331)\n",
      "Step 353: loss = 1381874.44 (19.163 sec) ('Training Accuracy:', 0.91719747) ('Testing Accuracy:', 0.54666668)\n",
      "Step 354: loss = 677886.80 (19.556 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.58666664)\n",
      "Step 355: loss = 529450.37 (19.139 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.57666665)\n",
      "Step 356: loss = 565118.02 (19.317 sec) ('Training Accuracy:', 0.97077554) ('Testing Accuracy:', 0.58666664)\n",
      "Step 357: loss = 708147.56 (19.624 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.59666669)\n",
      "Step 358: loss = 993169.12 (19.727 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.57333332)\n",
      "Step 359: loss = 359483.45 (19.692 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.58333331)\n",
      "Step 360: loss = 343091.67 (18.985 sec) ('Training Accuracy:', 0.94304985) ('Testing Accuracy:', 0.55000001)\n",
      "Step 361: loss = 406277.48 (19.424 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.58333331)\n",
      "Step 362: loss = 393048.93 (19.553 sec) ('Training Accuracy:', 0.96927691) ('Testing Accuracy:', 0.60000002)\n",
      "Step 363: loss = 296825.50 (19.632 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.55666667)\n",
      "Step 364: loss = 297763.74 (19.305 sec) ('Training Accuracy:', 0.94979393) ('Testing Accuracy:', 0.55000001)\n",
      "Step 365: loss = 499308.50 (19.229 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.59666669)\n",
      "Step 366: loss = 280085.76 (19.489 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.55666667)\n",
      "Step 367: loss = 468691.65 (19.122 sec) ('Training Accuracy:', 0.90558261) ('Testing Accuracy:', 0.53666669)\n",
      "Step 368: loss = 666324.14 (19.885 sec) ('Training Accuracy:', 0.91232669) ('Testing Accuracy:', 0.56)\n",
      "Step 369: loss = 1222028.23 (19.482 sec) ('Training Accuracy:', 0.93068564) ('Testing Accuracy:', 0.55333334)\n",
      "Step 370: loss = 648291.21 (19.313 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.55333334)\n",
      "Step 371: loss = 520332.42 (19.748 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.60000002)\n",
      "Step 372: loss = 305955.07 (19.639 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.56666666)\n",
      "Step 373: loss = 861091.16 (24.042 sec) ('Training Accuracy:', 0.93668038) ('Testing Accuracy:', 0.54666668)\n",
      "Step 374: loss = 3730714.13 (23.526 sec) ('Training Accuracy:', 0.89659047) ('Testing Accuracy:', 0.55333334)\n",
      "Step 375: loss = 957610.64 (23.061 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.55666667)\n",
      "Step 376: loss = 655158.06 (24.187 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.56666666)\n",
      "Step 377: loss = 764899.61 (23.325 sec) ('Training Accuracy:', 0.93705505) ('Testing Accuracy:', 0.60000002)\n",
      "Step 378: loss = 471086.57 (23.507 sec) ('Training Accuracy:', 0.92843759) ('Testing Accuracy:', 0.55000001)\n",
      "Step 379: loss = 678653.32 (23.421 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.62333333)\n",
      "Step 380: loss = 713180.54 (25.603 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.57333332)\n",
      "Step 381: loss = 325509.68 (20.549 sec) ('Training Accuracy:', 0.90820533) ('Testing Accuracy:', 0.56)\n",
      "Step 382: loss = 640644.85 (22.145 sec) ('Training Accuracy:', 0.9569127) ('Testing Accuracy:', 0.60666668)\n",
      "Step 383: loss = 363779.48 (21.699 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.57333332)\n",
      "Step 384: loss = 489012.46 (24.064 sec) ('Training Accuracy:', 0.97377294) ('Testing Accuracy:', 0.60666668)\n",
      "Step 385: loss = 963574.02 (18.816 sec) ('Training Accuracy:', 0.94866991) ('Testing Accuracy:', 0.60333335)\n",
      "Step 386: loss = 1004067.59 (19.453 sec) ('Training Accuracy:', 0.90932935) ('Testing Accuracy:', 0.56)\n",
      "Step 387: loss = 628198.40 (18.912 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.59666669)\n",
      "Step 388: loss = 266313.38 (18.957 sec) ('Training Accuracy:', 0.97264892) ('Testing Accuracy:', 0.58999997)\n",
      "Step 389: loss = 460146.56 (19.007 sec) ('Training Accuracy:', 0.96028477) ('Testing Accuracy:', 0.60000002)\n",
      "Step 390: loss = 530747.27 (19.141 sec) ('Training Accuracy:', 0.97677034) ('Testing Accuracy:', 0.56999999)\n",
      "Step 391: loss = 899848.55 (19.062 sec) ('Training Accuracy:', 0.97564632) ('Testing Accuracy:', 0.57999998)\n",
      "Step 392: loss = 963993.59 (19.000 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.57999998)\n",
      "Step 393: loss = 789662.77 (19.517 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.56333333)\n",
      "Step 394: loss = 858612.05 (19.278 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.57999998)\n",
      "Step 395: loss = 1295348.84 (18.886 sec) ('Training Accuracy:', 0.88797301) ('Testing Accuracy:', 0.55000001)\n",
      "Step 396: loss = 1624568.53 (18.704 sec) ('Training Accuracy:', 0.95054328) ('Testing Accuracy:', 0.57666665)\n",
      "Step 397: loss = 3438507.09 (19.355 sec) ('Training Accuracy:', 0.93518174) ('Testing Accuracy:', 0.57999998)\n",
      "Step 398: loss = 1092920.72 (18.850 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.55666667)\n",
      "Step 399: loss = 1040252.20 (18.965 sec) ('Training Accuracy:', 0.96965158) ('Testing Accuracy:', 0.57666665)\n",
      "Step 400: loss = 873784.67 (19.265 sec) ('Training Accuracy:', 0.93031096) ('Testing Accuracy:', 0.55000001)\n",
      "Step 401: loss = 601359.71 (19.245 sec) ('Training Accuracy:', 0.96852756) ('Testing Accuracy:', 0.58333331)\n",
      "Step 402: loss = 514614.92 (19.037 sec) ('Training Accuracy:', 0.91382539) ('Testing Accuracy:', 0.54666668)\n",
      "Step 403: loss = 780267.72 (19.220 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.58666664)\n",
      "Step 404: loss = 467166.28 (18.852 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.55333334)\n",
      "Step 405: loss = 911971.64 (19.347 sec) ('Training Accuracy:', 0.97751969) ('Testing Accuracy:', 0.58666664)\n",
      "Step 406: loss = 741138.77 (19.225 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.57999998)\n",
      "Step 407: loss = 951487.12 (19.058 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.60000002)\n",
      "Step 408: loss = 583725.32 (19.441 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.56666666)\n",
      "Step 409: loss = 438476.23 (19.013 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.59666669)\n",
      "Step 410: loss = 495430.66 (19.341 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.56)\n",
      "Step 411: loss = 449787.61 (19.191 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.58999997)\n",
      "Step 412: loss = 311049.72 (19.270 sec) ('Training Accuracy:', 0.91832149) ('Testing Accuracy:', 0.54666668)\n",
      "Step 413: loss = 721353.71 (18.983 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.57666665)\n",
      "Step 414: loss = 320671.58 (19.367 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.57666665)\n",
      "Step 415: loss = 501334.40 (19.128 sec) ('Training Accuracy:', 0.96253276) ('Testing Accuracy:', 0.58666664)\n",
      "Step 416: loss = 441209.00 (19.559 sec) ('Training Accuracy:', 0.96777821) ('Testing Accuracy:', 0.58999997)\n",
      "Step 417: loss = 576242.66 (20.233 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.61333334)\n",
      "Step 418: loss = 8111827.07 (19.083 sec) ('Training Accuracy:', 0.72461593) ('Testing Accuracy:', 0.50999999)\n",
      "Step 419: loss = 36788859.62 (18.995 sec) ('Training Accuracy:', 0.90146124) ('Testing Accuracy:', 0.61666667)\n",
      "Step 420: loss = 5711719.10 (19.147 sec) ('Training Accuracy:', 0.96103412) ('Testing Accuracy:', 0.57333332)\n",
      "Step 421: loss = 3226278.73 (18.819 sec) ('Training Accuracy:', 0.93518174) ('Testing Accuracy:', 0.57666665)\n",
      "Step 422: loss = 1268828.81 (18.954 sec) ('Training Accuracy:', 0.97152489) ('Testing Accuracy:', 0.57666665)\n",
      "Step 423: loss = 444991.80 (18.785 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.56333333)\n",
      "Step 424: loss = 494005.33 (18.954 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.55333334)\n",
      "Step 425: loss = 866440.68 (18.697 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.56666666)\n",
      "Step 426: loss = 535066.41 (18.667 sec) ('Training Accuracy:', 0.97489697) ('Testing Accuracy:', 0.59666669)\n",
      "Step 427: loss = 160649.31 (18.894 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.58333331)\n",
      "Step 428: loss = 401642.68 (19.005 sec) ('Training Accuracy:', 0.96627951) ('Testing Accuracy:', 0.56666666)\n",
      "Step 429: loss = 452435.31 (18.796 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.56333333)\n",
      "Step 430: loss = 582845.63 (19.005 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.58999997)\n",
      "Step 431: loss = 537027.00 (18.732 sec) ('Training Accuracy:', 0.96403146) ('Testing Accuracy:', 0.57666665)\n",
      "Step 432: loss = 402189.07 (19.262 sec) ('Training Accuracy:', 0.95279133) ('Testing Accuracy:', 0.56333333)\n",
      "Step 433: loss = 642207.19 (19.011 sec) ('Training Accuracy:', 0.96065944) ('Testing Accuracy:', 0.56333333)\n",
      "Step 434: loss = 235278.59 (19.143 sec) ('Training Accuracy:', 0.96665418) ('Testing Accuracy:', 0.57999998)\n",
      "Step 435: loss = 167250.43 (18.835 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.57333332)\n",
      "Step 436: loss = 365816.99 (18.893 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.56666666)\n",
      "Step 437: loss = 333363.16 (19.020 sec) ('Training Accuracy:', 0.97264892) ('Testing Accuracy:', 0.57999998)\n",
      "Step 438: loss = 131672.05 (18.906 sec) ('Training Accuracy:', 0.97564632) ('Testing Accuracy:', 0.57999998)\n",
      "Step 439: loss = 108510.65 (19.167 sec) ('Training Accuracy:', 0.97602099) ('Testing Accuracy:', 0.57333332)\n",
      "Step 440: loss = 285900.48 (19.373 sec) ('Training Accuracy:', 0.95578867) ('Testing Accuracy:', 0.56666666)\n",
      "Step 441: loss = 423199.09 (18.634 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.56666666)\n",
      "Step 442: loss = 308870.00 (19.156 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.56999999)\n",
      "Step 443: loss = 321266.15 (19.133 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.55666667)\n",
      "Step 444: loss = 518270.08 (19.225 sec) ('Training Accuracy:', 0.97152489) ('Testing Accuracy:', 0.56999999)\n",
      "Step 445: loss = 129279.09 (18.439 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.61000001)\n",
      "Step 446: loss = 160710.94 (19.227 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.56666666)\n",
      "Step 447: loss = 524659.44 (19.104 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.58666664)\n",
      "Step 448: loss = 199309.45 (19.101 sec) ('Training Accuracy:', 0.97414762) ('Testing Accuracy:', 0.60333335)\n",
      "Step 449: loss = 285235.94 (18.731 sec) ('Training Accuracy:', 0.97489697) ('Testing Accuracy:', 0.57999998)\n",
      "Step 450: loss = 578254.50 (18.786 sec) ('Training Accuracy:', 0.97527164) ('Testing Accuracy:', 0.5933333)\n",
      "Step 451: loss = 559414.16 (19.206 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.56)\n",
      "Step 452: loss = 1093991.04 (18.754 sec) ('Training Accuracy:', 0.87073809) ('Testing Accuracy:', 0.52666664)\n",
      "Step 453: loss = 989997.44 (20.057 sec) ('Training Accuracy:', 0.92693895) ('Testing Accuracy:', 0.54333335)\n",
      "Step 454: loss = 1006533.07 (19.430 sec) ('Training Accuracy:', 0.93668038) ('Testing Accuracy:', 0.55666667)\n",
      "Step 455: loss = 956959.55 (18.931 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.55666667)\n",
      "Step 456: loss = 471455.00 (19.784 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.56666666)\n",
      "Step 457: loss = 260920.55 (19.644 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.58666664)\n",
      "Step 458: loss = 491269.73 (20.151 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.55666667)\n",
      "Step 459: loss = 252419.92 (20.210 sec) ('Training Accuracy:', 0.97002625) ('Testing Accuracy:', 0.58333331)\n",
      "Step 460: loss = 229969.13 (19.813 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.56999999)\n",
      "Step 461: loss = 203182.23 (20.559 sec) ('Training Accuracy:', 0.96103412) ('Testing Accuracy:', 0.56)\n",
      "Step 462: loss = 542953.42 (20.868 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.59666669)\n",
      "Step 463: loss = 212298.90 (20.785 sec) ('Training Accuracy:', 0.94604719) ('Testing Accuracy:', 0.56333333)\n",
      "Step 464: loss = 917311.89 (20.918 sec) ('Training Accuracy:', 0.89883852) ('Testing Accuracy:', 0.53333336)\n",
      "Step 465: loss = 850307.66 (21.390 sec) ('Training Accuracy:', 0.91420007) ('Testing Accuracy:', 0.55333334)\n",
      "Step 466: loss = 1023736.55 (20.634 sec) ('Training Accuracy:', 0.92056948) ('Testing Accuracy:', 0.55666667)\n",
      "Step 467: loss = 2692151.47 (20.332 sec) ('Training Accuracy:', 0.96253276) ('Testing Accuracy:', 0.56999999)\n",
      "Step 468: loss = 2562172.32 (20.042 sec) ('Training Accuracy:', 0.9216935) ('Testing Accuracy:', 0.56333333)\n",
      "Step 469: loss = 5213046.23 (20.120 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.5933333)\n",
      "Step 470: loss = 3553616.74 (19.727 sec) ('Training Accuracy:', 0.93293369) ('Testing Accuracy:', 0.54666668)\n",
      "Step 471: loss = 5209717.74 (19.221 sec) ('Training Accuracy:', 0.92056948) ('Testing Accuracy:', 0.55666667)\n",
      "Step 472: loss = 2984381.06 (19.130 sec) ('Training Accuracy:', 0.84526038) ('Testing Accuracy:', 0.53666669)\n",
      "Step 473: loss = 17806772.37 (19.539 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.61666667)\n",
      "Step 474: loss = 12608122.61 (19.368 sec) ('Training Accuracy:', 0.75533909) ('Testing Accuracy:', 0.51666665)\n",
      "Step 475: loss = 12246604.77 (19.572 sec) ('Training Accuracy:', 0.87223679) ('Testing Accuracy:', 0.56333333)\n",
      "Step 476: loss = 21480273.67 (19.536 sec) ('Training Accuracy:', 0.9231922) ('Testing Accuracy:', 0.62666667)\n",
      "Step 477: loss = 6581888.50 (19.569 sec) ('Training Accuracy:', 0.90408391) ('Testing Accuracy:', 0.56)\n",
      "Step 478: loss = 1674687.59 (19.160 sec) ('Training Accuracy:', 0.97002625) ('Testing Accuracy:', 0.56333333)\n",
      "Step 479: loss = 1202672.82 (19.061 sec) ('Training Accuracy:', 0.97527164) ('Testing Accuracy:', 0.55666667)\n",
      "Step 480: loss = 1109854.22 (19.209 sec) ('Training Accuracy:', 0.97002625) ('Testing Accuracy:', 0.56999999)\n",
      "Step 481: loss = 276510.97 (18.741 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.57333332)\n",
      "Step 482: loss = 355774.39 (19.380 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.56999999)\n",
      "Step 483: loss = 188909.28 (18.632 sec) ('Training Accuracy:', 0.96852756) ('Testing Accuracy:', 0.58666664)\n",
      "Step 484: loss = 167545.79 (19.392 sec) ('Training Accuracy:', 0.96403146) ('Testing Accuracy:', 0.57999998)\n",
      "Step 485: loss = 368611.26 (19.139 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.57666665)\n",
      "Step 486: loss = 477897.59 (18.811 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.54333335)\n",
      "Step 487: loss = 637223.26 (19.284 sec) ('Training Accuracy:', 0.94604719) ('Testing Accuracy:', 0.54666668)\n",
      "Step 488: loss = 894284.56 (19.041 sec) ('Training Accuracy:', 0.93668038) ('Testing Accuracy:', 0.54000002)\n",
      "Step 489: loss = 693407.72 (19.652 sec) ('Training Accuracy:', 0.9201948) ('Testing Accuracy:', 0.54000002)\n",
      "Step 490: loss = 810623.46 (18.930 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.56999999)\n",
      "Step 491: loss = 463959.88 (18.999 sec) ('Training Accuracy:', 0.95129263) ('Testing Accuracy:', 0.55333334)\n",
      "Step 492: loss = 854348.97 (19.008 sec) ('Training Accuracy:', 0.90708131) ('Testing Accuracy:', 0.55666667)\n",
      "Step 493: loss = 1419203.46 (19.083 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.56)\n",
      "Step 494: loss = 877289.20 (19.477 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.56333333)\n",
      "Step 495: loss = 457857.19 (19.283 sec) ('Training Accuracy:', 0.95728737) ('Testing Accuracy:', 0.57999998)\n",
      "Step 496: loss = 313340.03 (18.926 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.57666665)\n",
      "Step 497: loss = 414592.71 (19.163 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.57333332)\n",
      "Step 498: loss = 643465.54 (19.043 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.56999999)\n",
      "Step 499: loss = 733003.79 (19.234 sec) ('Training Accuracy:', 0.93705505) ('Testing Accuracy:', 0.56999999)\n",
      "Step 500: loss = 1248046.27 (18.877 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.56)\n",
      "Step 501: loss = 529425.42 (19.129 sec) ('Training Accuracy:', 0.93068564) ('Testing Accuracy:', 0.55666667)\n",
      "Step 502: loss = 506171.52 (19.418 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.56666666)\n",
      "Step 503: loss = 581678.24 (19.125 sec) ('Training Accuracy:', 0.90633196) ('Testing Accuracy:', 0.55333334)\n",
      "Step 504: loss = 724136.99 (19.352 sec) ('Training Accuracy:', 0.89958787) ('Testing Accuracy:', 0.54333335)\n",
      "Step 505: loss = 1116952.96 (18.898 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.56)\n",
      "Step 506: loss = 1077193.89 (19.593 sec) ('Training Accuracy:', 0.91757214) ('Testing Accuracy:', 0.56)\n",
      "Step 507: loss = 931108.88 (18.815 sec) ('Training Accuracy:', 0.90445858) ('Testing Accuracy:', 0.56)\n",
      "Step 508: loss = 783305.91 (19.338 sec) ('Training Accuracy:', 0.9276883) ('Testing Accuracy:', 0.56999999)\n",
      "Step 509: loss = 1287758.25 (18.919 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.57333332)\n",
      "Step 510: loss = 439491.52 (19.044 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.56)\n",
      "Step 511: loss = 890519.02 (19.126 sec) ('Training Accuracy:', 0.91157734) ('Testing Accuracy:', 0.55333334)\n",
      "Step 512: loss = 1156475.36 (19.142 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.55666667)\n",
      "Step 513: loss = 464505.78 (19.835 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.56)\n",
      "Step 514: loss = 648137.28 (20.001 sec) ('Training Accuracy:', 0.91982013) ('Testing Accuracy:', 0.56333333)\n",
      "Step 515: loss = 1198233.61 (20.096 sec) ('Training Accuracy:', 0.96965158) ('Testing Accuracy:', 0.56999999)\n",
      "Step 516: loss = 1030794.61 (20.501 sec) ('Training Accuracy:', 0.97714502) ('Testing Accuracy:', 0.5933333)\n",
      "Step 517: loss = 1316629.21 (20.443 sec) ('Training Accuracy:', 0.95728737) ('Testing Accuracy:', 0.54000002)\n",
      "Step 518: loss = 677498.73 (20.824 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.55333334)\n",
      "Step 519: loss = 712937.71 (20.583 sec) ('Training Accuracy:', 0.96965158) ('Testing Accuracy:', 0.56666666)\n",
      "Step 520: loss = 542259.32 (20.575 sec) ('Training Accuracy:', 0.97377294) ('Testing Accuracy:', 0.56999999)\n",
      "Step 521: loss = 457864.32 (19.951 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.54666668)\n",
      "Step 522: loss = 558957.34 (20.509 sec) ('Training Accuracy:', 0.93668038) ('Testing Accuracy:', 0.55000001)\n",
      "Step 523: loss = 787281.96 (25.483 sec) ('Training Accuracy:', 0.90970403) ('Testing Accuracy:', 0.53666669)\n",
      "Step 524: loss = 665727.26 (24.681 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.55666667)\n",
      "Step 525: loss = 549171.57 (24.879 sec) ('Training Accuracy:', 0.87710756) ('Testing Accuracy:', 0.54666668)\n",
      "Step 526: loss = 926213.81 (22.501 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.60333335)\n",
      "Step 527: loss = 901695.34 (21.986 sec) ('Training Accuracy:', 0.97264892) ('Testing Accuracy:', 0.55666667)\n",
      "Step 528: loss = 767663.63 (24.713 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.55000001)\n",
      "Step 529: loss = 611848.54 (22.650 sec) ('Training Accuracy:', 0.97639567) ('Testing Accuracy:', 0.58333331)\n",
      "Step 530: loss = 990816.85 (23.342 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.55000001)\n",
      "Step 531: loss = 1122284.87 (21.725 sec) ('Training Accuracy:', 0.92544025) ('Testing Accuracy:', 0.55333334)\n",
      "Step 532: loss = 652422.82 (21.806 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.55000001)\n",
      "Step 533: loss = 626402.92 (23.407 sec) ('Training Accuracy:', 0.94717121) ('Testing Accuracy:', 0.55666667)\n",
      "Step 534: loss = 661281.13 (23.041 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.54000002)\n",
      "Step 535: loss = 492912.13 (24.455 sec) ('Training Accuracy:', 0.92843759) ('Testing Accuracy:', 0.55333334)\n",
      "Step 536: loss = 573986.76 (18.459 sec) ('Training Accuracy:', 0.93405771) ('Testing Accuracy:', 0.54333335)\n",
      "Step 537: loss = 759771.28 (19.713 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.54333335)\n",
      "Step 538: loss = 416368.12 (19.623 sec) ('Training Accuracy:', 0.92131883) ('Testing Accuracy:', 0.53333336)\n",
      "Step 539: loss = 654735.53 (20.508 sec) ('Training Accuracy:', 0.93705505) ('Testing Accuracy:', 0.53333336)\n",
      "Step 540: loss = 671546.53 (19.138 sec) ('Training Accuracy:', 0.91045338) ('Testing Accuracy:', 0.53333336)\n",
      "Step 541: loss = 695624.89 (19.334 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.57333332)\n",
      "Step 542: loss = 860072.22 (19.116 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.55666667)\n",
      "Step 543: loss = 1098139.81 (19.293 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.54666668)\n",
      "Step 544: loss = 539632.23 (19.499 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.57333332)\n",
      "Step 545: loss = 436194.15 (18.991 sec) ('Training Accuracy:', 0.94304985) ('Testing Accuracy:', 0.57999998)\n",
      "Step 546: loss = 355845.07 (19.204 sec) ('Training Accuracy:', 0.94866991) ('Testing Accuracy:', 0.55666667)\n",
      "Step 547: loss = 514512.09 (19.692 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.61333334)\n",
      "Step 548: loss = 199163.35 (19.394 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.58333331)\n",
      "Step 549: loss = 548881.95 (19.336 sec) ('Training Accuracy:', 0.94642186) ('Testing Accuracy:', 0.55666667)\n",
      "Step 550: loss = 317810.74 (18.986 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.56666666)\n",
      "Step 551: loss = 354835.32 (19.311 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.57666665)\n",
      "Step 552: loss = 316754.93 (19.697 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.55666667)\n",
      "Step 553: loss = 474043.45 (19.784 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.56333333)\n",
      "Step 554: loss = 448501.90 (19.267 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.55666667)\n",
      "Step 555: loss = 350076.86 (19.020 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.55333334)\n",
      "Step 556: loss = 486492.92 (19.334 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.58666664)\n",
      "Step 557: loss = 272609.24 (19.043 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.55000001)\n",
      "Step 558: loss = 576911.19 (19.624 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.58333331)\n",
      "Step 559: loss = 400540.48 (19.389 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.55333334)\n",
      "Step 560: loss = 261010.19 (19.579 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.57999998)\n",
      "Step 561: loss = 502106.48 (19.176 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.57333332)\n",
      "Step 562: loss = 604618.64 (19.667 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.56999999)\n",
      "Step 563: loss = 962802.25 (19.571 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.56666666)\n",
      "Step 564: loss = 550002.04 (19.260 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.56999999)\n",
      "Step 565: loss = 407361.03 (19.256 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.57333332)\n",
      "Step 566: loss = 520512.54 (19.553 sec) ('Training Accuracy:', 0.91907084) ('Testing Accuracy:', 0.54333335)\n",
      "Step 567: loss = 690529.88 (19.358 sec) ('Training Accuracy:', 0.9261896) ('Testing Accuracy:', 0.55333334)\n",
      "Step 568: loss = 904621.29 (19.826 sec) ('Training Accuracy:', 0.87148744) ('Testing Accuracy:', 0.55000001)\n",
      "Step 569: loss = 785262.43 (19.517 sec) ('Training Accuracy:', 0.93668038) ('Testing Accuracy:', 0.56666666)\n",
      "Step 570: loss = 706672.84 (19.191 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.57666665)\n",
      "Step 571: loss = 951369.16 (20.187 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.56999999)\n",
      "Step 572: loss = 439585.05 (19.309 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.55666667)\n",
      "Step 573: loss = 248097.98 (19.889 sec) ('Training Accuracy:', 0.9261896) ('Testing Accuracy:', 0.56333333)\n",
      "Step 574: loss = 1040029.95 (19.267 sec) ('Training Accuracy:', 0.92731363) ('Testing Accuracy:', 0.56999999)\n",
      "Step 575: loss = 874416.51 (19.569 sec) ('Training Accuracy:', 0.91382539) ('Testing Accuracy:', 0.54666668)\n",
      "Step 576: loss = 755725.91 (19.349 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.56333333)\n",
      "Step 577: loss = 333058.73 (19.309 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.56)\n",
      "Step 578: loss = 601937.80 (19.024 sec) ('Training Accuracy:', 0.94454855) ('Testing Accuracy:', 0.57666665)\n",
      "Step 579: loss = 722746.05 (18.810 sec) ('Training Accuracy:', 0.93180966) ('Testing Accuracy:', 0.56666666)\n",
      "Step 580: loss = 649476.14 (19.501 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.56)\n",
      "Step 581: loss = 524050.54 (19.352 sec) ('Training Accuracy:', 0.88722366) ('Testing Accuracy:', 0.57666665)\n",
      "Step 582: loss = 648687.48 (19.771 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.58999997)\n",
      "Step 583: loss = 516938.87 (19.319 sec) ('Training Accuracy:', 0.93405771) ('Testing Accuracy:', 0.56999999)\n",
      "Step 584: loss = 1303795.26 (19.311 sec) ('Training Accuracy:', 0.92544025) ('Testing Accuracy:', 0.56333333)\n",
      "Step 585: loss = 782971.81 (19.231 sec) ('Training Accuracy:', 0.93293369) ('Testing Accuracy:', 0.56333333)\n",
      "Step 586: loss = 1008747.66 (19.080 sec) ('Training Accuracy:', 0.8816036) ('Testing Accuracy:', 0.53333336)\n",
      "Step 587: loss = 3216989.13 (19.404 sec) ('Training Accuracy:', 0.83626825) ('Testing Accuracy:', 0.55666667)\n",
      "Step 588: loss = 1662080.92 (19.254 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.55000001)\n",
      "Step 589: loss = 1413261.02 (18.802 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.56333333)\n",
      "Step 590: loss = 724865.50 (18.844 sec) ('Training Accuracy:', 0.96777821) ('Testing Accuracy:', 0.57666665)\n",
      "Step 591: loss = 448772.30 (19.320 sec) ('Training Accuracy:', 0.93443239) ('Testing Accuracy:', 0.58666664)\n",
      "Step 592: loss = 797700.02 (19.214 sec) ('Training Accuracy:', 0.90295988) ('Testing Accuracy:', 0.55000001)\n",
      "Step 593: loss = 1529566.32 (19.232 sec) ('Training Accuracy:', 0.96740353) ('Testing Accuracy:', 0.60666668)\n",
      "Step 594: loss = 686185.04 (19.934 sec) ('Training Accuracy:', 0.92356688) ('Testing Accuracy:', 0.55000001)\n",
      "Step 595: loss = 656186.81 (19.492 sec) ('Training Accuracy:', 0.94005245) ('Testing Accuracy:', 0.56)\n",
      "Step 596: loss = 1046219.83 (19.035 sec) ('Training Accuracy:', 0.95878607) ('Testing Accuracy:', 0.56999999)\n",
      "Step 597: loss = 613938.43 (19.414 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.58333331)\n",
      "Step 598: loss = 508403.96 (19.121 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.57333332)\n",
      "Step 599: loss = 566591.49 (19.176 sec) ('Training Accuracy:', 0.95878607) ('Testing Accuracy:', 0.5933333)\n",
      "Step 600: loss = 448650.65 (18.547 sec) ('Training Accuracy:', 0.8947171) ('Testing Accuracy:', 0.55333334)\n",
      "Step 601: loss = 789707.64 (19.402 sec) ('Training Accuracy:', 0.94117647) ('Testing Accuracy:', 0.57333332)\n",
      "Step 602: loss = 624934.54 (19.223 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.61000001)\n",
      "Step 603: loss = 226029.17 (18.978 sec) ('Training Accuracy:', 0.96065944) ('Testing Accuracy:', 0.58333331)\n",
      "Step 604: loss = 433831.04 (19.422 sec) ('Training Accuracy:', 0.93106031) ('Testing Accuracy:', 0.56999999)\n",
      "Step 605: loss = 611305.67 (19.222 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.5933333)\n",
      "Step 606: loss = 1276424.60 (19.346 sec) ('Training Accuracy:', 0.9408018) ('Testing Accuracy:', 0.57666665)\n",
      "Step 607: loss = 565404.46 (18.095 sec) ('Training Accuracy:', 0.96890223) ('Testing Accuracy:', 0.56999999)\n",
      "Step 608: loss = 450392.04 (19.171 sec) ('Training Accuracy:', 0.89321846) ('Testing Accuracy:', 0.56333333)\n",
      "Step 609: loss = 961504.70 (19.062 sec) ('Training Accuracy:', 0.94192582) ('Testing Accuracy:', 0.57666665)\n",
      "Step 610: loss = 574392.20 (19.124 sec) ('Training Accuracy:', 0.9216935) ('Testing Accuracy:', 0.57333332)\n",
      "Step 611: loss = 886699.46 (19.299 sec) ('Training Accuracy:', 0.91082805) ('Testing Accuracy:', 0.55666667)\n",
      "Step 612: loss = 1192621.12 (19.474 sec) ('Training Accuracy:', 0.9231922) ('Testing Accuracy:', 0.58333331)\n",
      "Step 613: loss = 814387.89 (19.475 sec) ('Training Accuracy:', 0.84750843) ('Testing Accuracy:', 0.52999997)\n",
      "Step 614: loss = 1193902.43 (18.963 sec) ('Training Accuracy:', 0.95054328) ('Testing Accuracy:', 0.57666665)\n",
      "Step 615: loss = 469523.73 (19.002 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.57666665)\n",
      "Step 616: loss = 460720.16 (18.650 sec) ('Training Accuracy:', 0.92544025) ('Testing Accuracy:', 0.56333333)\n",
      "Step 617: loss = 754228.31 (19.055 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.57666665)\n",
      "Step 618: loss = 472556.86 (19.269 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.56666666)\n",
      "Step 619: loss = 702023.73 (19.279 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.56)\n",
      "Step 620: loss = 335883.58 (19.312 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.58333331)\n",
      "Step 621: loss = 739894.54 (19.051 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.56999999)\n",
      "Step 622: loss = 1122739.12 (19.073 sec) ('Training Accuracy:', 0.91607344) ('Testing Accuracy:', 0.60000002)\n",
      "Step 623: loss = 1030411.06 (19.072 sec) ('Training Accuracy:', 0.92918694) ('Testing Accuracy:', 0.56999999)\n",
      "Step 624: loss = 1061627.49 (19.037 sec) ('Training Accuracy:', 0.90633196) ('Testing Accuracy:', 0.57666665)\n",
      "Step 625: loss = 2734303.37 (18.851 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.60333335)\n",
      "Step 626: loss = 1596084.38 (19.489 sec) ('Training Accuracy:', 0.94042712) ('Testing Accuracy:', 0.64666665)\n",
      "Step 627: loss = 2522245.45 (18.816 sec) ('Training Accuracy:', 0.96403146) ('Testing Accuracy:', 0.58666664)\n",
      "Step 628: loss = 2063735.57 (18.680 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.58333331)\n",
      "Step 629: loss = 3059568.50 (18.570 sec) ('Training Accuracy:', 0.96627951) ('Testing Accuracy:', 0.60333335)\n",
      "Step 630: loss = 9611176.91 (19.102 sec) ('Training Accuracy:', 0.86998874) ('Testing Accuracy:', 0.56666666)\n",
      "Step 631: loss = 4127130.77 (19.436 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.62333333)\n",
      "Step 632: loss = 1334706.35 (18.871 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.58666664)\n",
      "Step 633: loss = 802493.63 (18.710 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.56999999)\n",
      "Step 634: loss = 602553.92 (18.943 sec) ('Training Accuracy:', 0.8947171) ('Testing Accuracy:', 0.54666668)\n",
      "Step 635: loss = 693891.10 (18.816 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.58666664)\n",
      "Step 636: loss = 523260.58 (18.989 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.58666664)\n",
      "Step 637: loss = 466842.89 (18.034 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.56)\n",
      "Step 638: loss = 471540.96 (18.709 sec) ('Training Accuracy:', 0.96065944) ('Testing Accuracy:', 0.60000002)\n",
      "Step 639: loss = 348624.44 (19.035 sec) ('Training Accuracy:', 0.95016861) ('Testing Accuracy:', 0.5933333)\n",
      "Step 640: loss = 540497.87 (19.063 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.5933333)\n",
      "Step 641: loss = 215136.16 (18.163 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.60000002)\n",
      "Step 642: loss = 281170.47 (18.956 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.58999997)\n",
      "Step 643: loss = 263084.12 (18.980 sec) ('Training Accuracy:', 0.97527164) ('Testing Accuracy:', 0.60333335)\n",
      "Step 644: loss = 91631.38 (18.594 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.60666668)\n",
      "Step 645: loss = 593035.17 (19.035 sec) ('Training Accuracy:', 0.90558261) ('Testing Accuracy:', 0.56666666)\n",
      "Step 646: loss = 953088.45 (18.960 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.57666665)\n",
      "Step 647: loss = 833800.94 (18.674 sec) ('Training Accuracy:', 0.97639567) ('Testing Accuracy:', 0.61000001)\n",
      "Step 648: loss = 628186.12 (19.053 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.58999997)\n",
      "Step 649: loss = 437868.32 (18.930 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.57666665)\n",
      "Step 650: loss = 420058.08 (19.136 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.57999998)\n",
      "Step 651: loss = 3107110.56 (18.464 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.60000002)\n",
      "Step 652: loss = 2377390.75 (18.764 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.56333333)\n",
      "Step 653: loss = 1569104.18 (18.677 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.58333331)\n",
      "Step 654: loss = 539655.21 (18.502 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.57666665)\n",
      "Step 655: loss = 498429.18 (19.188 sec) ('Training Accuracy:', 0.96665418) ('Testing Accuracy:', 0.59666669)\n",
      "Step 656: loss = 612586.67 (18.874 sec) ('Training Accuracy:', 0.96965158) ('Testing Accuracy:', 0.62666667)\n",
      "Step 657: loss = 1841519.77 (19.292 sec) ('Training Accuracy:', 0.92956161) ('Testing Accuracy:', 0.57333332)\n",
      "Step 658: loss = 350425.77 (19.126 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.58666664)\n",
      "Step 659: loss = 285051.71 (18.819 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.57333332)\n",
      "Step 660: loss = 1514821.93 (19.270 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.60333335)\n",
      "Step 661: loss = 336514.31 (18.730 sec) ('Training Accuracy:', 0.96140879) ('Testing Accuracy:', 0.55666667)\n",
      "Step 662: loss = 399644.12 (18.994 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.58333331)\n",
      "Step 663: loss = 981689.68 (18.645 sec) ('Training Accuracy:', 0.90783066) ('Testing Accuracy:', 0.56666666)\n",
      "Step 664: loss = 667019.94 (18.582 sec) ('Training Accuracy:', 0.96140879) ('Testing Accuracy:', 0.5933333)\n",
      "Step 665: loss = 247754.20 (19.273 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.5933333)\n",
      "Step 666: loss = 251479.40 (19.213 sec) ('Training Accuracy:', 0.97602099) ('Testing Accuracy:', 0.58999997)\n",
      "Step 667: loss = 387519.28 (19.264 sec) ('Training Accuracy:', 0.95616335) ('Testing Accuracy:', 0.57999998)\n",
      "Step 668: loss = 412643.29 (18.722 sec) ('Training Accuracy:', 0.96065944) ('Testing Accuracy:', 0.58666664)\n",
      "Step 669: loss = 493661.70 (19.374 sec) ('Training Accuracy:', 0.94492316) ('Testing Accuracy:', 0.57999998)\n",
      "Step 670: loss = 715654.78 (19.640 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.57999998)\n",
      "Step 671: loss = 515671.13 (19.159 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.58999997)\n",
      "Step 672: loss = 262682.72 (19.105 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.57999998)\n",
      "Step 673: loss = 219926.78 (18.976 sec) ('Training Accuracy:', 0.96028477) ('Testing Accuracy:', 0.58666664)\n",
      "Step 674: loss = 442611.48 (19.098 sec) ('Training Accuracy:', 0.94304985) ('Testing Accuracy:', 0.57999998)\n",
      "Step 675: loss = 380220.77 (19.606 sec) ('Training Accuracy:', 0.97377294) ('Testing Accuracy:', 0.5933333)\n",
      "Step 676: loss = 269309.39 (19.268 sec) ('Training Accuracy:', 0.97789437) ('Testing Accuracy:', 0.58666664)\n",
      "Step 677: loss = 383575.25 (19.152 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.5933333)\n",
      "Step 678: loss = 166858.72 (18.645 sec) ('Training Accuracy:', 0.97677034) ('Testing Accuracy:', 0.57999998)\n",
      "Step 679: loss = 363467.29 (19.304 sec) ('Training Accuracy:', 0.96553016) ('Testing Accuracy:', 0.59666669)\n",
      "Step 680: loss = 130901.32 (19.378 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.58333331)\n",
      "Step 681: loss = 969032.61 (18.965 sec) ('Training Accuracy:', 0.90483326) ('Testing Accuracy:', 0.5933333)\n",
      "Step 682: loss = 1345345.28 (19.364 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.56333333)\n",
      "Step 683: loss = 488787.77 (19.143 sec) ('Training Accuracy:', 0.94492316) ('Testing Accuracy:', 0.57999998)\n",
      "Step 684: loss = 1109797.03 (19.660 sec) ('Training Accuracy:', 0.97751969) ('Testing Accuracy:', 0.61000001)\n",
      "Step 685: loss = 199939.59 (19.334 sec) ('Training Accuracy:', 0.9423005) ('Testing Accuracy:', 0.57333332)\n",
      "Step 686: loss = 544952.15 (19.031 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.56999999)\n",
      "Step 687: loss = 887155.49 (54.917 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.55666667)\n",
      "Step 688: loss = 578995.61 (62.659 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.5933333)\n",
      "Step 689: loss = 353138.99 (63.104 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.58999997)\n",
      "Step 690: loss = 395309.83 (76.293 sec) ('Training Accuracy:', 0.94492316) ('Testing Accuracy:', 0.57333332)\n",
      "Step 691: loss = 527019.50 (63.849 sec) ('Training Accuracy:', 0.92431623) ('Testing Accuracy:', 0.56999999)\n",
      "Step 692: loss = 702364.89 (64.087 sec) ('Training Accuracy:', 0.92356688) ('Testing Accuracy:', 0.57333332)\n",
      "Step 693: loss = 1235957.95 (20.415 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.5933333)\n",
      "Step 694: loss = 488613.97 (18.355 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.58666664)\n",
      "Step 695: loss = 438935.59 (20.070 sec) ('Training Accuracy:', 0.9408018) ('Testing Accuracy:', 0.57666665)\n",
      "Step 696: loss = 525060.23 (20.400 sec) ('Training Accuracy:', 0.9569127) ('Testing Accuracy:', 0.60000002)\n",
      "Step 697: loss = 446188.91 (20.789 sec) ('Training Accuracy:', 0.97377294) ('Testing Accuracy:', 0.60333335)\n",
      "Step 698: loss = 498349.39 (20.737 sec) ('Training Accuracy:', 0.91757214) ('Testing Accuracy:', 0.57666665)\n",
      "Step 699: loss = 882155.93 (20.753 sec) ('Training Accuracy:', 0.86062193) ('Testing Accuracy:', 0.59666669)\n",
      "Step 700: loss = 1407696.75 (21.602 sec) ('Training Accuracy:', 0.85987264) ('Testing Accuracy:', 0.57999998)\n",
      "Step 701: loss = 5561970.37 (20.276 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.61666667)\n",
      "Step 702: loss = 1863965.64 (20.170 sec) ('Training Accuracy:', 0.82053202) ('Testing Accuracy:', 0.52999997)\n",
      "Step 703: loss = 3238706.04 (19.562 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.58333331)\n",
      "Step 704: loss = 1701089.11 (20.130 sec) ('Training Accuracy:', 0.85237914) ('Testing Accuracy:', 0.55000001)\n",
      "Step 705: loss = 3093093.82 (24.072 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.56333333)\n",
      "Step 706: loss = 5965874.66 (24.147 sec) ('Training Accuracy:', 0.86324465) ('Testing Accuracy:', 0.57666665)\n",
      "Step 707: loss = 7630970.27 (24.033 sec) ('Training Accuracy:', 0.97527164) ('Testing Accuracy:', 0.60666668)\n",
      "Step 708: loss = 10297302.69 (23.773 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.59666669)\n",
      "Step 709: loss = 17502744.17 (23.332 sec) ('Training Accuracy:', 0.84863245) ('Testing Accuracy:', 0.65333331)\n",
      "Step 710: loss = 17072267.58 (23.615 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.58666664)\n",
      "Step 711: loss = 3462656.47 (23.681 sec) ('Training Accuracy:', 0.97527164) ('Testing Accuracy:', 0.62333333)\n",
      "Step 712: loss = 638935.39 (23.619 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.60666668)\n",
      "Step 713: loss = 358155.12 (23.668 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.61000001)\n",
      "Step 714: loss = 301704.38 (24.138 sec) ('Training Accuracy:', 0.97564632) ('Testing Accuracy:', 0.62333333)\n",
      "Step 715: loss = 92300.77 (23.600 sec) ('Training Accuracy:', 0.97414762) ('Testing Accuracy:', 0.63333333)\n",
      "Step 716: loss = 115064.45 (22.621 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.60333335)\n",
      "Step 717: loss = 242489.74 (23.556 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.5933333)\n",
      "Step 718: loss = 272572.92 (23.960 sec) ('Training Accuracy:', 0.9569127) ('Testing Accuracy:', 0.57666665)\n",
      "Step 719: loss = 482054.10 (24.005 sec) ('Training Accuracy:', 0.95466465) ('Testing Accuracy:', 0.57666665)\n",
      "Step 720: loss = 449090.19 (23.805 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.57333332)\n",
      "Step 721: loss = 691287.07 (24.092 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.58666664)\n",
      "Step 722: loss = 144324.34 (24.315 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.57333332)\n",
      "Step 723: loss = 364054.14 (24.325 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.56666666)\n",
      "Step 724: loss = 973844.66 (24.017 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.58999997)\n",
      "Step 725: loss = 186069.90 (23.950 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.59666669)\n",
      "Step 726: loss = 303176.52 (23.606 sec) ('Training Accuracy:', 0.93330836) ('Testing Accuracy:', 0.57999998)\n",
      "Step 727: loss = 808084.89 (24.072 sec) ('Training Accuracy:', 0.91120267) ('Testing Accuracy:', 0.56333333)\n",
      "Step 728: loss = 2613758.78 (23.826 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.56666666)\n",
      "Step 729: loss = 3219129.48 (24.220 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.5933333)\n",
      "Step 730: loss = 3381204.45 (23.770 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.57333332)\n",
      "Step 731: loss = 3457847.05 (23.876 sec) ('Training Accuracy:', 0.97751969) ('Testing Accuracy:', 0.62)\n",
      "Step 732: loss = 4140097.09 (23.954 sec) ('Training Accuracy:', 0.97302359) ('Testing Accuracy:', 0.57666665)\n",
      "Step 733: loss = 2521914.82 (24.321 sec) ('Training Accuracy:', 0.96103412) ('Testing Accuracy:', 0.62333333)\n",
      "Step 734: loss = 3321219.23 (23.802 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.66666669)\n",
      "Step 735: loss = 3534369.88 (23.884 sec) ('Training Accuracy:', 0.95728737) ('Testing Accuracy:', 0.63666666)\n",
      "Step 736: loss = 3525653.95 (23.583 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.62)\n",
      "Step 737: loss = 3536171.90 (25.005 sec) ('Training Accuracy:', 0.9569127) ('Testing Accuracy:', 0.60333335)\n",
      "Step 738: loss = 708073.27 (23.418 sec) ('Training Accuracy:', 0.9393031) ('Testing Accuracy:', 0.56666666)\n",
      "Step 739: loss = 972416.58 (24.367 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.57666665)\n",
      "Step 740: loss = 322847.94 (24.202 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.56333333)\n",
      "Step 741: loss = 1261775.95 (25.094 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.58999997)\n",
      "Step 742: loss = 724651.99 (23.804 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.57999998)\n",
      "Step 743: loss = 485120.25 (23.579 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.5933333)\n",
      "Step 744: loss = 257114.69 (24.005 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.58333331)\n",
      "Step 745: loss = 440375.84 (24.047 sec) ('Training Accuracy:', 0.93218434) ('Testing Accuracy:', 0.57666665)\n",
      "Step 746: loss = 980826.24 (23.835 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.56999999)\n",
      "Step 747: loss = 135766.56 (24.052 sec) ('Training Accuracy:', 0.95728737) ('Testing Accuracy:', 0.57333332)\n",
      "Step 748: loss = 444725.74 (24.019 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.56999999)\n",
      "Step 749: loss = 990744.68 (26.149 sec) ('Training Accuracy:', 0.90033722) ('Testing Accuracy:', 0.56333333)\n",
      "Step 750: loss = 1204066.68 (24.308 sec) ('Training Accuracy:', 0.93518174) ('Testing Accuracy:', 0.57333332)\n",
      "Step 751: loss = 583031.17 (24.200 sec) ('Training Accuracy:', 0.91082805) ('Testing Accuracy:', 0.57333332)\n",
      "Step 752: loss = 1321199.36 (24.705 sec) ('Training Accuracy:', 0.93218434) ('Testing Accuracy:', 0.56333333)\n",
      "Step 753: loss = 936746.95 (25.280 sec) ('Training Accuracy:', 0.90932935) ('Testing Accuracy:', 0.57666665)\n",
      "Step 754: loss = 688519.55 (25.176 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.62333333)\n",
      "Step 755: loss = 361860.35 (24.428 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.57666665)\n",
      "Step 756: loss = 1224356.14 (24.894 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.56333333)\n",
      "Step 757: loss = 703085.58 (23.442 sec) ('Training Accuracy:', 0.90708131) ('Testing Accuracy:', 0.57666665)\n",
      "Step 758: loss = 881599.19 (24.090 sec) ('Training Accuracy:', 0.93068564) ('Testing Accuracy:', 0.56)\n",
      "Step 759: loss = 839190.47 (24.191 sec) ('Training Accuracy:', 0.87710756) ('Testing Accuracy:', 0.55000001)\n",
      "Step 760: loss = 790963.62 (23.766 sec) ('Training Accuracy:', 0.9276883) ('Testing Accuracy:', 0.55666667)\n",
      "Step 761: loss = 982249.13 (11.253 sec) ('Training Accuracy:', 0.8684901) ('Testing Accuracy:', 0.55000001)\n",
      "Step 762: loss = 1070710.64 (11.531 sec) ('Training Accuracy:', 0.96890223) ('Testing Accuracy:', 0.62)\n",
      "Step 763: loss = 573341.98 (11.731 sec) ('Training Accuracy:', 0.93218434) ('Testing Accuracy:', 0.56)\n",
      "Step 764: loss = 491265.61 (12.055 sec) ('Training Accuracy:', 0.94267517) ('Testing Accuracy:', 0.57333332)\n",
      "Step 765: loss = 516379.69 (12.066 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.56666666)\n",
      "Step 766: loss = 1002670.57 (12.067 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.58333331)\n",
      "Step 767: loss = 192270.81 (12.537 sec) ('Training Accuracy:', 0.9378044) ('Testing Accuracy:', 0.56999999)\n",
      "Step 768: loss = 566057.66 (12.463 sec) ('Training Accuracy:', 0.92131883) ('Testing Accuracy:', 0.56)\n",
      "Step 769: loss = 1142202.66 (12.623 sec) ('Training Accuracy:', 0.96777821) ('Testing Accuracy:', 0.58666664)\n",
      "Step 770: loss = 224131.96 (12.905 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.58333331)\n",
      "Step 771: loss = 253125.00 (13.047 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.58666664)\n",
      "Step 772: loss = 384010.06 (13.164 sec) ('Training Accuracy:', 0.96927691) ('Testing Accuracy:', 0.60333335)\n",
      "Step 773: loss = 154694.59 (13.113 sec) ('Training Accuracy:', 0.96403146) ('Testing Accuracy:', 0.5933333)\n",
      "Step 774: loss = 243199.84 (13.283 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.58999997)\n",
      "Step 775: loss = 207659.62 (13.374 sec) ('Training Accuracy:', 0.94454855) ('Testing Accuracy:', 0.56333333)\n",
      "Step 776: loss = 1187078.86 (13.269 sec) ('Training Accuracy:', 0.91270137) ('Testing Accuracy:', 0.56666666)\n",
      "Step 777: loss = 642584.85 (13.245 sec) ('Training Accuracy:', 0.93106031) ('Testing Accuracy:', 0.56333333)\n",
      "Step 778: loss = 989564.79 (12.797 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.56333333)\n",
      "Step 779: loss = 1205817.72 (12.552 sec) ('Training Accuracy:', 0.90858001) ('Testing Accuracy:', 0.54666668)\n",
      "Step 780: loss = 2136651.13 (12.157 sec) ('Training Accuracy:', 0.92431623) ('Testing Accuracy:', 0.56)\n",
      "Step 781: loss = 252357.27 (11.969 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.58333331)\n",
      "Step 782: loss = 686796.59 (11.965 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.5933333)\n",
      "Step 783: loss = 719518.89 (11.751 sec) ('Training Accuracy:', 0.96590483) ('Testing Accuracy:', 0.62333333)\n",
      "Step 784: loss = 351703.70 (11.373 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.58333331)\n",
      "Step 785: loss = 453712.89 (11.300 sec) ('Training Accuracy:', 0.94529784) ('Testing Accuracy:', 0.57333332)\n",
      "Step 786: loss = 765039.24 (11.291 sec) ('Training Accuracy:', 0.90370923) ('Testing Accuracy:', 0.56)\n",
      "Step 787: loss = 1010468.91 (11.185 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.60666668)\n",
      "Step 788: loss = 310721.08 (11.319 sec) ('Training Accuracy:', 0.93143499) ('Testing Accuracy:', 0.55666667)\n",
      "Step 789: loss = 711201.17 (11.518 sec) ('Training Accuracy:', 0.94492316) ('Testing Accuracy:', 0.57999998)\n",
      "Step 790: loss = 761383.82 (11.262 sec) ('Training Accuracy:', 0.90146124) ('Testing Accuracy:', 0.56999999)\n",
      "Step 791: loss = 1013784.04 (11.139 sec) ('Training Accuracy:', 0.96140879) ('Testing Accuracy:', 0.60000002)\n",
      "Step 792: loss = 579297.27 (11.227 sec) ('Training Accuracy:', 0.91494942) ('Testing Accuracy:', 0.56999999)\n",
      "Step 793: loss = 849744.41 (11.151 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.58999997)\n",
      "Step 794: loss = 926108.82 (11.237 sec) ('Training Accuracy:', 0.88272762) ('Testing Accuracy:', 0.57333332)\n",
      "Step 795: loss = 896763.92 (11.166 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.59666669)\n",
      "Step 796: loss = 311202.98 (11.136 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.5933333)\n",
      "Step 797: loss = 339360.30 (11.343 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.60333335)\n",
      "Step 798: loss = 400262.28 (11.273 sec) ('Training Accuracy:', 0.93368304) ('Testing Accuracy:', 0.57333332)\n",
      "Step 799: loss = 752236.62 (11.197 sec) ('Training Accuracy:', 0.90071189) ('Testing Accuracy:', 0.57999998)\n",
      "Step 800: loss = 1056755.02 (11.189 sec) ('Training Accuracy:', 0.97077554) ('Testing Accuracy:', 0.58999997)\n",
      "Step 801: loss = 1044630.85 (11.187 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.59666669)\n",
      "Step 802: loss = 367625.48 (11.373 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.5933333)\n",
      "Step 803: loss = 529645.75 (11.325 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.60000002)\n",
      "Step 804: loss = 501218.23 (11.750 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.58666664)\n",
      "Step 805: loss = 675950.56 (11.674 sec) ('Training Accuracy:', 0.9246909) ('Testing Accuracy:', 0.56999999)\n",
      "Step 806: loss = 824946.28 (11.621 sec) ('Training Accuracy:', 0.86137128) ('Testing Accuracy:', 0.55666667)\n",
      "Step 807: loss = 796520.66 (11.637 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.59666669)\n",
      "Step 808: loss = 747665.85 (11.646 sec) ('Training Accuracy:', 0.98051703) ('Testing Accuracy:', 0.60000002)\n",
      "Step 809: loss = 446034.95 (11.760 sec) ('Training Accuracy:', 0.91420007) ('Testing Accuracy:', 0.57333332)\n",
      "Step 810: loss = 799903.98 (11.626 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.58333331)\n",
      "Step 811: loss = 912891.84 (11.591 sec) ('Training Accuracy:', 0.93668038) ('Testing Accuracy:', 0.55333334)\n",
      "Step 812: loss = 418907.48 (11.756 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.58333331)\n",
      "Step 813: loss = 233088.27 (11.713 sec) ('Training Accuracy:', 0.94492316) ('Testing Accuracy:', 0.57333332)\n",
      "Step 814: loss = 849716.66 (11.777 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.57999998)\n",
      "Step 815: loss = 216098.02 (11.529 sec) ('Training Accuracy:', 0.92881227) ('Testing Accuracy:', 0.56)\n",
      "Step 816: loss = 554258.33 (11.433 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.58999997)\n",
      "Step 817: loss = 372413.82 (11.518 sec) ('Training Accuracy:', 0.93817908) ('Testing Accuracy:', 0.56333333)\n",
      "Step 818: loss = 586194.16 (11.591 sec) ('Training Accuracy:', 0.93180966) ('Testing Accuracy:', 0.56666666)\n",
      "Step 819: loss = 842703.43 (12.059 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.57999998)\n",
      "Step 820: loss = 291698.70 (12.300 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.61333334)\n",
      "Step 821: loss = 286336.50 (12.357 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.57666665)\n",
      "Step 822: loss = 532794.25 (12.445 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.56666666)\n",
      "Step 823: loss = 368015.84 (12.618 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.57333332)\n",
      "Step 824: loss = 364087.22 (12.951 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.56333333)\n",
      "Step 825: loss = 384709.14 (13.113 sec) ('Training Accuracy:', 0.97414762) ('Testing Accuracy:', 0.59666669)\n",
      "Step 826: loss = 455919.12 (13.372 sec) ('Training Accuracy:', 0.94754589) ('Testing Accuracy:', 0.57999998)\n",
      "Step 827: loss = 480213.08 (13.350 sec) ('Training Accuracy:', 0.9408018) ('Testing Accuracy:', 0.57999998)\n",
      "Step 828: loss = 361685.36 (13.364 sec) ('Training Accuracy:', 0.93705505) ('Testing Accuracy:', 0.56666666)\n",
      "Step 829: loss = 625999.29 (13.244 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.57333332)\n",
      "Step 830: loss = 472917.45 (13.278 sec) ('Training Accuracy:', 0.96140879) ('Testing Accuracy:', 0.58666664)\n",
      "Step 831: loss = 331048.58 (13.266 sec) ('Training Accuracy:', 0.96702886) ('Testing Accuracy:', 0.61000001)\n",
      "Step 832: loss = 354988.39 (13.267 sec) ('Training Accuracy:', 0.91232669) ('Testing Accuracy:', 0.58333331)\n",
      "Step 833: loss = 1035122.88 (13.415 sec) ('Training Accuracy:', 0.93480706) ('Testing Accuracy:', 0.57666665)\n",
      "Step 834: loss = 713306.24 (13.482 sec) ('Training Accuracy:', 0.94642186) ('Testing Accuracy:', 0.58666664)\n",
      "Step 835: loss = 597733.67 (13.239 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.60333335)\n",
      "Step 836: loss = 403030.30 (12.960 sec) ('Training Accuracy:', 0.92544025) ('Testing Accuracy:', 0.57666665)\n",
      "Step 837: loss = 630649.94 (12.857 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.59666669)\n",
      "Step 838: loss = 712958.34 (12.742 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.60000002)\n",
      "Step 839: loss = 1120086.43 (12.836 sec) ('Training Accuracy:', 0.96627951) ('Testing Accuracy:', 0.64999998)\n",
      "Step 840: loss = 1082229.75 (12.274 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.5933333)\n",
      "Step 841: loss = 798092.55 (12.167 sec) ('Training Accuracy:', 0.96965158) ('Testing Accuracy:', 0.59666669)\n",
      "Step 842: loss = 864857.45 (11.625 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.58333331)\n",
      "Step 843: loss = 239969.07 (11.586 sec) ('Training Accuracy:', 0.94604719) ('Testing Accuracy:', 0.56666666)\n",
      "Step 844: loss = 874075.18 (11.333 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.62333333)\n",
      "Step 845: loss = 612324.71 (11.203 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.61666667)\n",
      "Step 846: loss = 834163.77 (11.202 sec) ('Training Accuracy:', 0.97377294) ('Testing Accuracy:', 0.60000002)\n",
      "Step 847: loss = 791717.70 (11.191 sec) ('Training Accuracy:', 0.93892843) ('Testing Accuracy:', 0.58333331)\n",
      "Step 848: loss = 953758.38 (11.137 sec) ('Training Accuracy:', 0.97452229) ('Testing Accuracy:', 0.60000002)\n",
      "Step 849: loss = 1184147.41 (11.216 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.60666668)\n",
      "Step 850: loss = 3957443.34 (11.122 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.63)\n",
      "Step 851: loss = 2006978.40 (11.131 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.58666664)\n",
      "Step 852: loss = 3319390.41 (10.928 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.58999997)\n",
      "Step 853: loss = 7904248.43 (10.901 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.67000002)\n",
      "Step 854: loss = 2820237.60 (10.967 sec) ('Training Accuracy:', 0.91457474) ('Testing Accuracy:', 0.56333333)\n",
      "Step 855: loss = 4465946.69 (10.871 sec) ('Training Accuracy:', 0.91682279) ('Testing Accuracy:', 0.57999998)\n",
      "Step 856: loss = 1616203.31 (11.148 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.57666665)\n",
      "Step 857: loss = 847953.19 (10.969 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.54666668)\n",
      "Step 858: loss = 1669757.42 (11.432 sec) ('Training Accuracy:', 0.82727611) ('Testing Accuracy:', 0.53666669)\n",
      "Step 859: loss = 15871869.81 (11.346 sec) ('Training Accuracy:', 0.97826904) ('Testing Accuracy:', 0.61000001)\n",
      "Step 860: loss = 10932904.21 (11.657 sec) ('Training Accuracy:', 0.92693895) ('Testing Accuracy:', 0.58666664)\n",
      "Step 861: loss = 2153564.64 (11.965 sec) ('Training Accuracy:', 0.93855375) ('Testing Accuracy:', 0.59666669)\n",
      "Step 862: loss = 1786166.23 (12.353 sec) ('Training Accuracy:', 0.97527164) ('Testing Accuracy:', 0.63333333)\n",
      "Step 863: loss = 743743.04 (12.499 sec) ('Training Accuracy:', 0.95953542) ('Testing Accuracy:', 0.5933333)\n",
      "Step 864: loss = 586921.01 (12.988 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.58333331)\n",
      "Step 865: loss = 558757.90 (12.822 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.60000002)\n",
      "Step 866: loss = 544317.71 (12.823 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.58666664)\n",
      "Step 867: loss = 1129469.18 (12.809 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.5933333)\n",
      "Step 868: loss = 289465.22 (12.443 sec) ('Training Accuracy:', 0.95279133) ('Testing Accuracy:', 0.58999997)\n",
      "Step 869: loss = 431382.58 (12.244 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.58666664)\n",
      "Step 870: loss = 698896.63 (12.134 sec) ('Training Accuracy:', 0.92056948) ('Testing Accuracy:', 0.57333332)\n",
      "Step 871: loss = 1306140.55 (11.631 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.5933333)\n",
      "Step 872: loss = 372310.25 (11.517 sec) ('Training Accuracy:', 0.97264892) ('Testing Accuracy:', 0.61333334)\n",
      "Step 873: loss = 365347.89 (11.892 sec) ('Training Accuracy:', 0.95054328) ('Testing Accuracy:', 0.58666664)\n",
      "Step 874: loss = 701791.09 (11.472 sec) ('Training Accuracy:', 0.91682279) ('Testing Accuracy:', 0.58333331)\n",
      "Step 875: loss = 1045956.04 (11.104 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.58333331)\n",
      "Step 876: loss = 1179461.16 (10.827 sec) ('Training Accuracy:', 0.8962158) ('Testing Accuracy:', 0.57333332)\n",
      "Step 877: loss = 1002432.44 (10.997 sec) ('Training Accuracy:', 0.93555641) ('Testing Accuracy:', 0.57666665)\n",
      "Step 878: loss = 1073795.03 (10.760 sec) ('Training Accuracy:', 0.8977145) ('Testing Accuracy:', 0.56999999)\n",
      "Step 879: loss = 891989.93 (10.918 sec) ('Training Accuracy:', 0.90108657) ('Testing Accuracy:', 0.58666664)\n",
      "Step 880: loss = 2185221.25 (10.781 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.58333331)\n",
      "Step 881: loss = 787858.70 (10.956 sec) ('Training Accuracy:', 0.94117647) ('Testing Accuracy:', 0.5933333)\n",
      "Step 882: loss = 768611.76 (11.131 sec) ('Training Accuracy:', 0.88722366) ('Testing Accuracy:', 0.56666666)\n",
      "Step 883: loss = 1599319.87 (11.498 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.60333335)\n",
      "Step 884: loss = 951472.23 (11.839 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.5933333)\n",
      "Step 885: loss = 183150.63 (12.272 sec) ('Training Accuracy:', 0.96478081) ('Testing Accuracy:', 0.5933333)\n",
      "Step 886: loss = 307360.93 (12.556 sec) ('Training Accuracy:', 0.96777821) ('Testing Accuracy:', 0.60333335)\n",
      "Step 887: loss = 264896.72 (12.924 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.61000001)\n",
      "Step 888: loss = 283339.18 (12.778 sec) ('Training Accuracy:', 0.96178347) ('Testing Accuracy:', 0.60333335)\n",
      "Step 889: loss = 292427.28 (13.021 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.59666669)\n",
      "Step 890: loss = 507084.72 (12.596 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.58999997)\n",
      "Step 891: loss = 1110972.79 (12.218 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.58666664)\n",
      "Step 892: loss = 604296.02 (13.876 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.60000002)\n",
      "Step 893: loss = 881166.25 (12.938 sec) ('Training Accuracy:', 0.89396781) ('Testing Accuracy:', 0.57666665)\n",
      "Step 894: loss = 1150711.96 (12.652 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.61000001)\n",
      "Step 895: loss = 822471.44 (12.494 sec) ('Training Accuracy:', 0.89134508) ('Testing Accuracy:', 0.57999998)\n",
      "Step 896: loss = 703315.38 (12.793 sec) ('Training Accuracy:', 0.93368304) ('Testing Accuracy:', 0.59666669)\n",
      "Step 897: loss = 999029.85 (12.734 sec) ('Training Accuracy:', 0.90970403) ('Testing Accuracy:', 0.5933333)\n",
      "Step 898: loss = 999253.79 (12.783 sec) ('Training Accuracy:', 0.88909703) ('Testing Accuracy:', 0.57333332)\n",
      "Step 899: loss = 1116532.05 (13.012 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.60000002)\n",
      "Step 900: loss = 456768.74 (13.398 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.57666665)\n",
      "Step 901: loss = 591314.30 (13.375 sec) ('Training Accuracy:', 0.91382539) ('Testing Accuracy:', 0.56666666)\n",
      "Step 902: loss = 1265156.50 (13.546 sec) ('Training Accuracy:', 0.92206818) ('Testing Accuracy:', 0.58666664)\n",
      "Step 903: loss = 503841.43 (13.380 sec) ('Training Accuracy:', 0.90408391) ('Testing Accuracy:', 0.58666664)\n",
      "Step 904: loss = 1084026.39 (13.438 sec) ('Training Accuracy:', 0.88535035) ('Testing Accuracy:', 0.56666666)\n",
      "Step 905: loss = 1056244.03 (13.382 sec) ('Training Accuracy:', 0.95316601) ('Testing Accuracy:', 0.58666664)\n",
      "Step 906: loss = 590577.75 (13.204 sec) ('Training Accuracy:', 0.94567251) ('Testing Accuracy:', 0.5933333)\n",
      "Step 907: loss = 891407.32 (13.801 sec) ('Training Accuracy:', 0.91794682) ('Testing Accuracy:', 0.56666666)\n",
      "Step 908: loss = 1053621.45 (11.858 sec) ('Training Accuracy:', 0.89022106) ('Testing Accuracy:', 0.56999999)\n",
      "Step 909: loss = 1212511.80 (11.666 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.62666667)\n",
      "Step 910: loss = 573538.61 (11.149 sec) ('Training Accuracy:', 0.92094415) ('Testing Accuracy:', 0.58333331)\n",
      "Step 911: loss = 585711.93 (11.220 sec) ('Training Accuracy:', 0.87635821) ('Testing Accuracy:', 0.57333332)\n",
      "Step 912: loss = 992812.83 (11.170 sec) ('Training Accuracy:', 0.95616335) ('Testing Accuracy:', 0.58666664)\n",
      "Step 913: loss = 740081.88 (11.092 sec) ('Training Accuracy:', 0.88984638) ('Testing Accuracy:', 0.56999999)\n",
      "Step 914: loss = 793573.42 (11.033 sec) ('Training Accuracy:', 0.95578867) ('Testing Accuracy:', 0.60666668)\n",
      "Step 915: loss = 541643.12 (11.204 sec) ('Training Accuracy:', 0.92544025) ('Testing Accuracy:', 0.57666665)\n",
      "Step 916: loss = 728866.03 (11.048 sec) ('Training Accuracy:', 0.9201948) ('Testing Accuracy:', 0.57333332)\n",
      "Step 917: loss = 956054.31 (11.088 sec) ('Training Accuracy:', 0.91307604) ('Testing Accuracy:', 0.57999998)\n",
      "Step 918: loss = 705640.52 (11.076 sec) ('Training Accuracy:', 0.88872236) ('Testing Accuracy:', 0.57333332)\n",
      "Step 919: loss = 2140193.05 (11.062 sec) ('Training Accuracy:', 0.89246911) ('Testing Accuracy:', 0.56)\n",
      "Step 920: loss = 1477797.57 (11.049 sec) ('Training Accuracy:', 0.94304985) ('Testing Accuracy:', 0.60000002)\n",
      "Step 921: loss = 525357.09 (11.021 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.5933333)\n",
      "Step 922: loss = 874092.49 (11.117 sec) ('Training Accuracy:', 0.9276883) ('Testing Accuracy:', 0.60333335)\n",
      "Step 923: loss = 417302.99 (13.527 sec) ('Training Accuracy:', 0.96777821) ('Testing Accuracy:', 0.60000002)\n",
      "Step 924: loss = 751873.00 (13.317 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.56333333)\n",
      "Step 925: loss = 274455.61 (13.110 sec) ('Training Accuracy:', 0.96740353) ('Testing Accuracy:', 0.61333334)\n",
      "Step 926: loss = 404899.99 (13.185 sec) ('Training Accuracy:', 0.91982013) ('Testing Accuracy:', 0.5933333)\n",
      "Step 927: loss = 959896.07 (13.064 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.58999997)\n",
      "Step 928: loss = 320031.28 (13.082 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.60333335)\n",
      "Step 929: loss = 636499.95 (13.078 sec) ('Training Accuracy:', 0.94941926) ('Testing Accuracy:', 0.60333335)\n",
      "Step 930: loss = 969610.42 (13.157 sec) ('Training Accuracy:', 0.97639567) ('Testing Accuracy:', 0.58666664)\n",
      "Step 931: loss = 301817.22 (12.991 sec) ('Training Accuracy:', 0.94642186) ('Testing Accuracy:', 0.57666665)\n",
      "Step 932: loss = 917463.52 (13.112 sec) ('Training Accuracy:', 0.89171976) ('Testing Accuracy:', 0.58999997)\n",
      "Step 933: loss = 1451408.43 (12.948 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.61000001)\n",
      "Step 934: loss = 453455.28 (13.100 sec) ('Training Accuracy:', 0.92581493) ('Testing Accuracy:', 0.58333331)\n",
      "Step 935: loss = 902871.98 (12.810 sec) ('Training Accuracy:', 0.90932935) ('Testing Accuracy:', 0.56333333)\n",
      "Step 936: loss = 693517.92 (12.619 sec) ('Training Accuracy:', 0.93480706) ('Testing Accuracy:', 0.56333333)\n",
      "Step 937: loss = 663374.99 (12.338 sec) ('Training Accuracy:', 0.94567251) ('Testing Accuracy:', 0.58666664)\n",
      "Step 938: loss = 711988.21 (12.406 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.58333331)\n",
      "Step 939: loss = 807003.00 (12.685 sec) ('Training Accuracy:', 0.90670663) ('Testing Accuracy:', 0.57666665)\n",
      "Step 940: loss = 589956.34 (12.400 sec) ('Training Accuracy:', 0.9408018) ('Testing Accuracy:', 0.55333334)\n",
      "Step 941: loss = 806389.66 (12.365 sec) ('Training Accuracy:', 0.91832149) ('Testing Accuracy:', 0.55666667)\n",
      "Step 942: loss = 684001.78 (12.127 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.57999998)\n",
      "Step 943: loss = 343493.10 (12.004 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.57999998)\n",
      "Step 944: loss = 258029.27 (11.827 sec) ('Training Accuracy:', 0.94492316) ('Testing Accuracy:', 0.58333331)\n",
      "Step 945: loss = 520096.56 (11.875 sec) ('Training Accuracy:', 0.86661673) ('Testing Accuracy:', 0.57333332)\n",
      "Step 946: loss = 835124.89 (11.774 sec) ('Training Accuracy:', 0.95503932) ('Testing Accuracy:', 0.57999998)\n",
      "Step 947: loss = 655629.37 (12.021 sec) ('Training Accuracy:', 0.89134508) ('Testing Accuracy:', 0.56666666)\n",
      "Step 948: loss = 967404.89 (11.772 sec) ('Training Accuracy:', 0.90745598) ('Testing Accuracy:', 0.56666666)\n",
      "Step 949: loss = 678117.81 (11.814 sec) ('Training Accuracy:', 0.95428997) ('Testing Accuracy:', 0.60333335)\n",
      "Step 950: loss = 364316.78 (11.649 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.57999998)\n",
      "Step 951: loss = 601778.16 (11.538 sec) ('Training Accuracy:', 0.97264892) ('Testing Accuracy:', 0.62)\n",
      "Step 952: loss = 573506.99 (11.642 sec) ('Training Accuracy:', 0.96365678) ('Testing Accuracy:', 0.61333334)\n",
      "Step 953: loss = 309869.80 (12.950 sec) ('Training Accuracy:', 0.94005245) ('Testing Accuracy:', 0.56999999)\n",
      "Step 954: loss = 548634.56 (12.777 sec) ('Training Accuracy:', 0.92581493) ('Testing Accuracy:', 0.54333335)\n",
      "Step 955: loss = 761629.91 (12.572 sec) ('Training Accuracy:', 0.96403146) ('Testing Accuracy:', 0.58666664)\n",
      "Step 956: loss = 575045.57 (12.635 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.58999997)\n",
      "Step 957: loss = 233247.51 (12.700 sec) ('Training Accuracy:', 0.96777821) ('Testing Accuracy:', 0.57666665)\n",
      "Step 958: loss = 1030901.35 (12.700 sec) ('Training Accuracy:', 0.96515548) ('Testing Accuracy:', 0.57666665)\n",
      "Step 959: loss = 436845.45 (12.699 sec) ('Training Accuracy:', 0.92956161) ('Testing Accuracy:', 0.56999999)\n",
      "Step 960: loss = 784764.55 (12.660 sec) ('Training Accuracy:', 0.89659047) ('Testing Accuracy:', 0.57333332)\n",
      "Step 961: loss = 931682.81 (12.723 sec) ('Training Accuracy:', 0.92956161) ('Testing Accuracy:', 0.57333332)\n",
      "Step 962: loss = 729839.30 (12.975 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.57666665)\n",
      "Step 963: loss = 743693.99 (12.928 sec) ('Training Accuracy:', 0.92281753) ('Testing Accuracy:', 0.57666665)\n",
      "Step 964: loss = 1055940.61 (13.198 sec) ('Training Accuracy:', 0.97377294) ('Testing Accuracy:', 0.61000001)\n",
      "Step 965: loss = 1117884.80 (13.163 sec) ('Training Accuracy:', 0.91195202) ('Testing Accuracy:', 0.57999998)\n",
      "Step 966: loss = 1491177.35 (12.994 sec) ('Training Accuracy:', 0.8801049) ('Testing Accuracy:', 0.55333334)\n",
      "Step 967: loss = 841525.38 (12.694 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.61333334)\n",
      "Step 968: loss = 415584.00 (12.706 sec) ('Training Accuracy:', 0.95279133) ('Testing Accuracy:', 0.59666669)\n",
      "Step 969: loss = 443077.20 (12.966 sec) ('Training Accuracy:', 0.94342452) ('Testing Accuracy:', 0.56666666)\n",
      "Step 970: loss = 504314.62 (12.569 sec) ('Training Accuracy:', 0.95091796) ('Testing Accuracy:', 0.58333331)\n",
      "Step 971: loss = 516254.59 (12.442 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.60333335)\n",
      "Step 972: loss = 478385.20 (12.690 sec) ('Training Accuracy:', 0.95279133) ('Testing Accuracy:', 0.60666668)\n",
      "Step 973: loss = 602866.78 (12.282 sec) ('Training Accuracy:', 0.93443239) ('Testing Accuracy:', 0.58999997)\n",
      "Step 974: loss = 686923.28 (12.183 sec) ('Training Accuracy:', 0.97264892) ('Testing Accuracy:', 0.60000002)\n",
      "Step 975: loss = 172173.94 (12.016 sec) ('Training Accuracy:', 0.95054328) ('Testing Accuracy:', 0.58333331)\n",
      "Step 976: loss = 517676.69 (12.033 sec) ('Training Accuracy:', 0.97189957) ('Testing Accuracy:', 0.59666669)\n",
      "Step 977: loss = 267033.06 (12.222 sec) ('Training Accuracy:', 0.97602099) ('Testing Accuracy:', 0.61333334)\n",
      "Step 978: loss = 4672687.09 (12.016 sec) ('Training Accuracy:', 0.90108657) ('Testing Accuracy:', 0.6566667)\n",
      "Step 979: loss = 4094497.29 (11.745 sec) ('Training Accuracy:', 0.90783066) ('Testing Accuracy:', 0.57666665)\n",
      "Step 980: loss = 5566997.66 (11.591 sec) ('Training Accuracy:', 0.95466465) ('Testing Accuracy:', 0.57666665)\n",
      "Step 981: loss = 4494088.17 (11.202 sec) ('Training Accuracy:', 0.89284378) ('Testing Accuracy:', 0.57333332)\n",
      "Step 982: loss = 3334961.63 (10.907 sec) ('Training Accuracy:', 0.93405771) ('Testing Accuracy:', 0.58333331)\n",
      "Step 983: loss = 1285707.84 (10.967 sec) ('Training Accuracy:', 0.92394155) ('Testing Accuracy:', 0.58666664)\n",
      "Step 984: loss = 706279.55 (13.476 sec) ('Training Accuracy:', 0.95878607) ('Testing Accuracy:', 0.5933333)\n",
      "Step 985: loss = 577954.70 (12.875 sec) ('Training Accuracy:', 0.97789437) ('Testing Accuracy:', 0.62)\n",
      "Step 986: loss = 1390533.20 (12.664 sec) ('Training Accuracy:', 0.97339827) ('Testing Accuracy:', 0.62333333)\n",
      "Step 987: loss = 260757.04 (12.390 sec) ('Training Accuracy:', 0.96890223) ('Testing Accuracy:', 0.60333335)\n",
      "Step 988: loss = 296113.90 (12.086 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.60000002)\n",
      "Step 989: loss = 417159.83 (11.833 sec) ('Training Accuracy:', 0.92244285) ('Testing Accuracy:', 0.5933333)\n",
      "Step 990: loss = 1083802.70 (11.448 sec) ('Training Accuracy:', 0.95728737) ('Testing Accuracy:', 0.62)\n",
      "Step 991: loss = 242461.63 (11.415 sec) ('Training Accuracy:', 0.96852756) ('Testing Accuracy:', 0.62)\n",
      "Step 992: loss = 126421.27 (11.352 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.61333334)\n",
      "Step 993: loss = 149449.24 (10.919 sec) ('Training Accuracy:', 0.96740353) ('Testing Accuracy:', 0.62666667)\n",
      "Step 994: loss = 283666.40 (10.769 sec) ('Training Accuracy:', 0.94005245) ('Testing Accuracy:', 0.61333334)\n",
      "Step 995: loss = 774035.92 (16.658 sec) ('Training Accuracy:', 0.97040087) ('Testing Accuracy:', 0.61000001)\n",
      "Step 996: loss = 359850.90 (15.574 sec) ('Training Accuracy:', 0.96028477) ('Testing Accuracy:', 0.60666668)\n",
      "Step 997: loss = 492872.47 (13.036 sec) ('Training Accuracy:', 0.94904459) ('Testing Accuracy:', 0.58999997)\n",
      "Step 998: loss = 1239235.01 (12.963 sec) ('Training Accuracy:', 0.97751969) ('Testing Accuracy:', 0.61000001)\n",
      "Step 999: loss = 533878.74 (13.145 sec) ('Training Accuracy:', 0.95803672) ('Testing Accuracy:', 0.58333331)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_acc, test_acc = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 426029532.00 (9.126 sec) ('Training Accuracy:', 0.58883059) ('Testing Accuracy:', 0.57999998)\n",
      "Step 1: loss = 224952667.00 (10.641 sec) ('Training Accuracy:', 0.54610193) ('Testing Accuracy:', 0.47666666)\n",
      "Step 2: loss = 186282770.50 (8.754 sec) ('Training Accuracy:', 0.57608694) ('Testing Accuracy:', 0.52666664)\n",
      "Step 3: loss = 157188716.00 (8.744 sec) ('Training Accuracy:', 0.6064468) ('Testing Accuracy:', 0.55000001)\n",
      "Step 4: loss = 141595672.00 (9.254 sec) ('Training Accuracy:', 0.60607195) ('Testing Accuracy:', 0.53666669)\n",
      "Step 5: loss = 126091908.00 (9.422 sec) ('Training Accuracy:', 0.61731637) ('Testing Accuracy:', 0.56333333)\n",
      "Step 6: loss = 114848935.00 (9.681 sec) ('Training Accuracy:', 0.62668663) ('Testing Accuracy:', 0.56333333)\n",
      "Step 7: loss = 108071643.00 (10.035 sec) ('Training Accuracy:', 0.61731637) ('Testing Accuracy:', 0.52666664)\n",
      "Step 8: loss = 101031114.25 (10.724 sec) ('Training Accuracy:', 0.6555472) ('Testing Accuracy:', 0.55000001)\n",
      "Step 9: loss = 89654806.25 (11.714 sec) ('Training Accuracy:', 0.67278862) ('Testing Accuracy:', 0.55000001)\n",
      "Step 10: loss = 85552654.00 (9.702 sec) ('Training Accuracy:', 0.69265366) ('Testing Accuracy:', 0.55666667)\n",
      "Step 11: loss = 81913617.00 (9.555 sec) ('Training Accuracy:', 0.65929538) ('Testing Accuracy:', 0.52999997)\n",
      "Step 12: loss = 77064685.00 (9.489 sec) ('Training Accuracy:', 0.62293851) ('Testing Accuracy:', 0.53333336)\n",
      "Step 13: loss = 80015069.50 (9.322 sec) ('Training Accuracy:', 0.71026987) ('Testing Accuracy:', 0.56333333)\n",
      "Step 14: loss = 67080248.25 (9.416 sec) ('Training Accuracy:', 0.72526234) ('Testing Accuracy:', 0.57333332)\n",
      "Step 15: loss = 58869580.75 (9.331 sec) ('Training Accuracy:', 0.72901052) ('Testing Accuracy:', 0.56999999)\n",
      "Step 16: loss = 58326910.12 (10.160 sec) ('Training Accuracy:', 0.73313344) ('Testing Accuracy:', 0.58666664)\n",
      "Step 17: loss = 58034449.00 (9.773 sec) ('Training Accuracy:', 0.71251875) ('Testing Accuracy:', 0.57999998)\n",
      "Step 18: loss = 62831454.50 (9.645 sec) ('Training Accuracy:', 0.6248126) ('Testing Accuracy:', 0.52333331)\n",
      "Step 19: loss = 63375930.75 (10.027 sec) ('Training Accuracy:', 0.74925035) ('Testing Accuracy:', 0.5933333)\n",
      "Step 20: loss = 46641569.62 (9.908 sec) ('Training Accuracy:', 0.72901052) ('Testing Accuracy:', 0.57333332)\n",
      "Step 21: loss = 41932935.50 (9.403 sec) ('Training Accuracy:', 0.70502251) ('Testing Accuracy:', 0.56)\n",
      "Step 22: loss = 41006496.12 (9.426 sec) ('Training Accuracy:', 0.69527733) ('Testing Accuracy:', 0.54333335)\n",
      "Step 23: loss = 44299790.62 (9.426 sec) ('Training Accuracy:', 0.66491753) ('Testing Accuracy:', 0.54333335)\n",
      "Step 24: loss = 49466509.12 (11.818 sec) ('Training Accuracy:', 0.77398801) ('Testing Accuracy:', 0.5933333)\n",
      "Step 25: loss = 42891045.44 (10.288 sec) ('Training Accuracy:', 0.78073466) ('Testing Accuracy:', 0.5933333)\n",
      "Step 26: loss = 39914399.81 (9.320 sec) ('Training Accuracy:', 0.78335834) ('Testing Accuracy:', 0.60333335)\n",
      "Step 27: loss = 37795694.94 (9.350 sec) ('Training Accuracy:', 0.75) ('Testing Accuracy:', 0.56)\n",
      "Step 28: loss = 32752923.56 (9.339 sec) ('Training Accuracy:', 0.69902551) ('Testing Accuracy:', 0.55333334)\n",
      "Step 29: loss = 35684171.25 (9.404 sec) ('Training Accuracy:', 0.77136433) ('Testing Accuracy:', 0.55666667)\n",
      "Step 30: loss = 38982585.25 (9.385 sec) ('Training Accuracy:', 0.77698648) ('Testing Accuracy:', 0.5933333)\n",
      "Step 31: loss = 34695286.62 (9.366 sec) ('Training Accuracy:', 0.80509746) ('Testing Accuracy:', 0.58666664)\n",
      "Step 32: loss = 26054191.62 (9.333 sec) ('Training Accuracy:', 0.77548724) ('Testing Accuracy:', 0.56333333)\n",
      "Step 33: loss = 25358123.75 (9.357 sec) ('Training Accuracy:', 0.76424289) ('Testing Accuracy:', 0.56999999)\n",
      "Step 34: loss = 30589545.75 (9.348 sec) ('Training Accuracy:', 0.81334335) ('Testing Accuracy:', 0.60666668)\n",
      "Step 35: loss = 27675356.38 (9.376 sec) ('Training Accuracy:', 0.80547225) ('Testing Accuracy:', 0.61333334)\n",
      "Step 36: loss = 33145911.19 (9.356 sec) ('Training Accuracy:', 0.82571214) ('Testing Accuracy:', 0.60666668)\n",
      "Step 37: loss = 30411526.19 (9.364 sec) ('Training Accuracy:', 0.74325335) ('Testing Accuracy:', 0.54666668)\n",
      "Step 38: loss = 25455991.81 (9.402 sec) ('Training Accuracy:', 0.82046479) ('Testing Accuracy:', 0.57999998)\n",
      "Step 39: loss = 22723115.19 (9.565 sec) ('Training Accuracy:', 0.82158923) ('Testing Accuracy:', 0.61000001)\n",
      "Step 40: loss = 20002658.56 (9.375 sec) ('Training Accuracy:', 0.83508247) ('Testing Accuracy:', 0.61000001)\n",
      "Step 41: loss = 19361546.19 (9.749 sec) ('Training Accuracy:', 0.83995503) ('Testing Accuracy:', 0.60333335)\n",
      "Step 42: loss = 22202976.50 (9.386 sec) ('Training Accuracy:', 0.75712144) ('Testing Accuracy:', 0.54333335)\n",
      "Step 43: loss = 23513616.38 (9.391 sec) ('Training Accuracy:', 0.77023989) ('Testing Accuracy:', 0.55000001)\n",
      "Step 44: loss = 27179202.81 (9.410 sec) ('Training Accuracy:', 0.82833582) ('Testing Accuracy:', 0.60000002)\n",
      "Step 45: loss = 21226474.00 (9.425 sec) ('Training Accuracy:', 0.84032983) ('Testing Accuracy:', 0.62)\n",
      "Step 46: loss = 15283243.44 (9.393 sec) ('Training Accuracy:', 0.84932536) ('Testing Accuracy:', 0.58666664)\n",
      "Step 47: loss = 14546324.36 (9.395 sec) ('Training Accuracy:', 0.84782606) ('Testing Accuracy:', 0.56999999)\n",
      "Step 48: loss = 15312461.91 (9.396 sec) ('Training Accuracy:', 0.86431783) ('Testing Accuracy:', 0.60333335)\n",
      "Step 49: loss = 14166081.03 (9.617 sec) ('Training Accuracy:', 0.82421291) ('Testing Accuracy:', 0.61666667)\n",
      "Step 50: loss = 12861097.22 (9.408 sec) ('Training Accuracy:', 0.85194904) ('Testing Accuracy:', 0.59666669)\n",
      "Step 51: loss = 13107681.19 (9.410 sec) ('Training Accuracy:', 0.86019492) ('Testing Accuracy:', 0.60333335)\n",
      "Step 52: loss = 17308273.98 (9.435 sec) ('Training Accuracy:', 0.86731637) ('Testing Accuracy:', 0.56666666)\n",
      "Step 53: loss = 18621086.22 (9.418 sec) ('Training Accuracy:', 0.78073466) ('Testing Accuracy:', 0.56666666)\n",
      "Step 54: loss = 22935028.00 (9.415 sec) ('Training Accuracy:', 0.83508247) ('Testing Accuracy:', 0.61000001)\n",
      "Step 55: loss = 15029956.69 (10.997 sec) ('Training Accuracy:', 0.84107947) ('Testing Accuracy:', 0.63)\n",
      "Step 56: loss = 11003999.48 (10.723 sec) ('Training Accuracy:', 0.89692652) ('Testing Accuracy:', 0.59666669)\n",
      "Step 57: loss = 12986028.48 (10.746 sec) ('Training Accuracy:', 0.88530737) ('Testing Accuracy:', 0.58999997)\n",
      "Step 58: loss = 11954772.16 (9.421 sec) ('Training Accuracy:', 0.83958024) ('Testing Accuracy:', 0.62333333)\n",
      "Step 59: loss = 9056116.42 (13.001 sec) ('Training Accuracy:', 0.85269862) ('Testing Accuracy:', 0.62666667)\n",
      "Step 60: loss = 8344364.16 (12.653 sec) ('Training Accuracy:', 0.89430285) ('Testing Accuracy:', 0.58999997)\n",
      "Step 61: loss = 9189772.67 (11.761 sec) ('Training Accuracy:', 0.8871814) ('Testing Accuracy:', 0.60333335)\n",
      "Step 62: loss = 9341318.45 (11.193 sec) ('Training Accuracy:', 0.87593704) ('Testing Accuracy:', 0.61000001)\n",
      "Step 63: loss = 9920537.85 (10.898 sec) ('Training Accuracy:', 0.80434781) ('Testing Accuracy:', 0.63666666)\n",
      "Step 64: loss = 8383309.82 (10.722 sec) ('Training Accuracy:', 0.8384558) ('Testing Accuracy:', 0.62333333)\n",
      "Step 65: loss = 10826752.83 (10.730 sec) ('Training Accuracy:', 0.88380808) ('Testing Accuracy:', 0.60333335)\n",
      "Step 66: loss = 13013856.51 (10.853 sec) ('Training Accuracy:', 0.89242882) ('Testing Accuracy:', 0.60666668)\n",
      "Step 67: loss = 14072208.31 (11.010 sec) ('Training Accuracy:', 0.80097449) ('Testing Accuracy:', 0.5933333)\n",
      "Step 68: loss = 18675499.52 (9.556 sec) ('Training Accuracy:', 0.87143928) ('Testing Accuracy:', 0.59666669)\n",
      "Step 69: loss = 13050844.09 (10.715 sec) ('Training Accuracy:', 0.78673166) ('Testing Accuracy:', 0.63)\n",
      "Step 70: loss = 8885308.95 (12.792 sec) ('Training Accuracy:', 0.89880061) ('Testing Accuracy:', 0.61000001)\n",
      "Step 71: loss = 9413656.33 (11.315 sec) ('Training Accuracy:', 0.89580208) ('Testing Accuracy:', 0.60333335)\n",
      "Step 72: loss = 8710781.31 (13.270 sec) ('Training Accuracy:', 0.84482759) ('Testing Accuracy:', 0.62333333)\n",
      "Step 73: loss = 8195955.78 (10.734 sec) ('Training Accuracy:', 0.80847079) ('Testing Accuracy:', 0.62666667)\n",
      "Step 74: loss = 6936969.05 (11.333 sec) ('Training Accuracy:', 0.82008994) ('Testing Accuracy:', 0.63)\n",
      "Step 75: loss = 10303023.97 (11.494 sec) ('Training Accuracy:', 0.88530737) ('Testing Accuracy:', 0.61666667)\n",
      "Step 76: loss = 13775262.54 (10.492 sec) ('Training Accuracy:', 0.89805096) ('Testing Accuracy:', 0.62)\n",
      "Step 77: loss = 10242179.84 (10.786 sec) ('Training Accuracy:', 0.81934035) ('Testing Accuracy:', 0.63)\n",
      "Step 78: loss = 8890276.34 (12.698 sec) ('Training Accuracy:', 0.75149924) ('Testing Accuracy:', 0.63666666)\n",
      "Step 79: loss = 8227712.47 (9.078 sec) ('Training Accuracy:', 0.84032983) ('Testing Accuracy:', 0.63)\n",
      "Step 80: loss = 13244309.14 (11.328 sec) ('Training Accuracy:', 0.89430285) ('Testing Accuracy:', 0.61000001)\n",
      "Step 81: loss = 10337012.60 (11.848 sec) ('Training Accuracy:', 0.81559223) ('Testing Accuracy:', 0.62333333)\n",
      "Step 82: loss = 8453461.79 (12.584 sec) ('Training Accuracy:', 0.78373313) ('Testing Accuracy:', 0.62666667)\n",
      "Step 83: loss = 9870046.39 (10.715 sec) ('Training Accuracy:', 0.80247378) ('Testing Accuracy:', 0.63)\n",
      "Step 84: loss = 17328368.84 (13.047 sec) ('Training Accuracy:', 0.8931784) ('Testing Accuracy:', 0.62)\n",
      "Step 85: loss = 12948314.46 (11.349 sec) ('Training Accuracy:', 0.84670162) ('Testing Accuracy:', 0.62666667)\n",
      "Step 86: loss = 15412731.34 (11.874 sec) ('Training Accuracy:', 0.72226387) ('Testing Accuracy:', 0.63333333)\n",
      "Step 87: loss = 15804714.06 (10.422 sec) ('Training Accuracy:', 0.83658171) ('Testing Accuracy:', 0.63333333)\n",
      "Step 88: loss = 21457124.75 (13.039 sec) ('Training Accuracy:', 0.83583206) ('Testing Accuracy:', 0.62666667)\n",
      "Step 89: loss = 20142729.45 (11.190 sec) ('Training Accuracy:', 0.71889055) ('Testing Accuracy:', 0.63333333)\n",
      "Step 90: loss = 27826314.36 (9.843 sec) ('Training Accuracy:', 0.84032983) ('Testing Accuracy:', 0.63333333)\n",
      "Step 91: loss = 31311189.73 (10.694 sec) ('Training Accuracy:', 0.76686656) ('Testing Accuracy:', 0.63)\n",
      "Step 92: loss = 45068816.97 (10.714 sec) ('Training Accuracy:', 0.85569715) ('Testing Accuracy:', 0.62666667)\n",
      "Step 93: loss = 88961379.75 (13.035 sec) ('Training Accuracy:', 0.52286357) ('Testing Accuracy:', 0.46333334)\n",
      "Step 94: loss = 75792273.81 (11.719 sec) ('Training Accuracy:', 0.60869563) ('Testing Accuracy:', 0.51333332)\n",
      "Step 95: loss = 36363155.62 (12.313 sec) ('Training Accuracy:', 0.67391306) ('Testing Accuracy:', 0.54333335)\n",
      "Step 96: loss = 21577088.25 (12.695 sec) ('Training Accuracy:', 0.73988008) ('Testing Accuracy:', 0.56)\n",
      "Step 97: loss = 15113438.94 (12.439 sec) ('Training Accuracy:', 0.73200899) ('Testing Accuracy:', 0.56)\n",
      "Step 98: loss = 14594591.00 (12.147 sec) ('Training Accuracy:', 0.71326834) ('Testing Accuracy:', 0.55333334)\n",
      "Step 99: loss = 18162409.05 (11.401 sec) ('Training Accuracy:', 0.71551722) ('Testing Accuracy:', 0.55333334)\n",
      "Step 100: loss = 16155284.47 (8.666 sec) ('Training Accuracy:', 0.64805096) ('Testing Accuracy:', 0.53666669)\n",
      "Step 101: loss = 19141610.17 (12.564 sec) ('Training Accuracy:', 0.66416794) ('Testing Accuracy:', 0.55000001)\n",
      "Step 102: loss = 23858619.38 (13.051 sec) ('Training Accuracy:', 0.60869563) ('Testing Accuracy:', 0.52999997)\n",
      "Step 103: loss = 25700369.62 (12.754 sec) ('Training Accuracy:', 0.57721138) ('Testing Accuracy:', 0.51333332)\n",
      "Step 104: loss = 33211937.44 (12.527 sec) ('Training Accuracy:', 0.62856072) ('Testing Accuracy:', 0.54000002)\n",
      "Step 105: loss = 46563278.11 (10.677 sec) ('Training Accuracy:', 0.82908547) ('Testing Accuracy:', 0.5933333)\n",
      "Step 106: loss = 37289355.83 (10.901 sec) ('Training Accuracy:', 0.90967017) ('Testing Accuracy:', 0.61666667)\n",
      "Step 107: loss = 14006467.02 (8.691 sec) ('Training Accuracy:', 0.8871814) ('Testing Accuracy:', 0.63333333)\n",
      "Step 108: loss = 8373297.75 (13.584 sec) ('Training Accuracy:', 0.93590707) ('Testing Accuracy:', 0.58666664)\n",
      "Step 109: loss = 3751938.35 (8.694 sec) ('Training Accuracy:', 0.8875562) ('Testing Accuracy:', 0.60666668)\n",
      "Step 110: loss = 2435829.56 (12.222 sec) ('Training Accuracy:', 0.94040477) ('Testing Accuracy:', 0.59666669)\n",
      "Step 111: loss = 1653715.44 (12.649 sec) ('Training Accuracy:', 0.9374063) ('Testing Accuracy:', 0.58999997)\n",
      "Step 112: loss = 1125858.38 (8.695 sec) ('Training Accuracy:', 0.94790107) ('Testing Accuracy:', 0.58333331)\n",
      "Step 113: loss = 1395072.32 (12.360 sec) ('Training Accuracy:', 0.93590707) ('Testing Accuracy:', 0.5933333)\n",
      "Step 114: loss = 1441289.46 (11.490 sec) ('Training Accuracy:', 0.89580208) ('Testing Accuracy:', 0.59666669)\n",
      "Step 115: loss = 1858063.37 (10.998 sec) ('Training Accuracy:', 0.85907048) ('Testing Accuracy:', 0.58999997)\n",
      "Step 116: loss = 2930907.20 (10.667 sec) ('Training Accuracy:', 0.9115442) ('Testing Accuracy:', 0.60000002)\n",
      "Step 117: loss = 4108250.93 (12.347 sec) ('Training Accuracy:', 0.95089954) ('Testing Accuracy:', 0.5933333)\n",
      "Step 118: loss = 2316005.00 (11.432 sec) ('Training Accuracy:', 0.96064466) ('Testing Accuracy:', 0.60000002)\n",
      "Step 119: loss = 1655669.61 (10.844 sec) ('Training Accuracy:', 0.95239878) ('Testing Accuracy:', 0.59666669)\n",
      "Step 120: loss = 1332849.53 (10.710 sec) ('Training Accuracy:', 0.88080961) ('Testing Accuracy:', 0.58666664)\n",
      "Step 121: loss = 1052116.82 (12.520 sec) ('Training Accuracy:', 0.93290854) ('Testing Accuracy:', 0.60333335)\n",
      "Step 122: loss = 851671.72 (8.727 sec) ('Training Accuracy:', 0.9314093) ('Testing Accuracy:', 0.61000001)\n",
      "Step 123: loss = 834169.01 (12.641 sec) ('Training Accuracy:', 0.93103451) ('Testing Accuracy:', 0.59666669)\n",
      "Step 124: loss = 708624.39 (12.476 sec) ('Training Accuracy:', 0.89842576) ('Testing Accuracy:', 0.61333334)\n",
      "Step 125: loss = 762462.31 (12.298 sec) ('Training Accuracy:', 0.93215895) ('Testing Accuracy:', 0.60666668)\n",
      "Step 126: loss = 783008.54 (12.683 sec) ('Training Accuracy:', 0.92166418) ('Testing Accuracy:', 0.60666668)\n",
      "Step 127: loss = 594597.42 (8.680 sec) ('Training Accuracy:', 0.90367317) ('Testing Accuracy:', 0.59666669)\n",
      "Step 128: loss = 539832.56 (11.750 sec) ('Training Accuracy:', 0.91191906) ('Testing Accuracy:', 0.61000001)\n",
      "Step 129: loss = 728731.84 (10.376 sec) ('Training Accuracy:', 0.92616194) ('Testing Accuracy:', 0.60000002)\n",
      "Step 130: loss = 815291.16 (13.088 sec) ('Training Accuracy:', 0.91866565) ('Testing Accuracy:', 0.5933333)\n",
      "Step 131: loss = 725704.27 (9.136 sec) ('Training Accuracy:', 0.94002998) ('Testing Accuracy:', 0.59666669)\n",
      "Step 132: loss = 1232904.54 (13.123 sec) ('Training Accuracy:', 0.94827586) ('Testing Accuracy:', 0.60333335)\n",
      "Step 133: loss = 1591561.16 (10.751 sec) ('Training Accuracy:', 0.87368816) ('Testing Accuracy:', 0.58999997)\n",
      "Step 134: loss = 2303481.95 (8.699 sec) ('Training Accuracy:', 0.80097449) ('Testing Accuracy:', 0.58666664)\n",
      "Step 135: loss = 2976461.80 (13.041 sec) ('Training Accuracy:', 0.76386809) ('Testing Accuracy:', 0.57333332)\n",
      "Step 136: loss = 2992737.08 (13.073 sec) ('Training Accuracy:', 0.74137932) ('Testing Accuracy:', 0.55666667)\n",
      "Step 137: loss = 4582258.09 (12.731 sec) ('Training Accuracy:', 0.80509746) ('Testing Accuracy:', 0.57666665)\n",
      "Step 138: loss = 5710100.31 (13.074 sec) ('Training Accuracy:', 0.78523237) ('Testing Accuracy:', 0.55666667)\n",
      "Step 139: loss = 4966673.62 (10.608 sec) ('Training Accuracy:', 0.78223389) ('Testing Accuracy:', 0.56999999)\n",
      "Step 140: loss = 6762539.03 (12.610 sec) ('Training Accuracy:', 0.8564468) ('Testing Accuracy:', 0.58999997)\n",
      "Step 141: loss = 8871429.79 (12.223 sec) ('Training Accuracy:', 0.77098948) ('Testing Accuracy:', 0.58333331)\n",
      "Step 142: loss = 5689932.70 (8.598 sec) ('Training Accuracy:', 0.8931784) ('Testing Accuracy:', 0.60000002)\n",
      "Step 143: loss = 8287650.51 (10.776 sec) ('Training Accuracy:', 0.89692652) ('Testing Accuracy:', 0.60666668)\n",
      "Step 144: loss = 12436754.34 (13.284 sec) ('Training Accuracy:', 0.76086956) ('Testing Accuracy:', 0.57666665)\n",
      "Step 145: loss = 7391531.65 (9.517 sec) ('Training Accuracy:', 0.91754121) ('Testing Accuracy:', 0.60666668)\n",
      "Step 146: loss = 11401076.64 (10.806 sec) ('Training Accuracy:', 0.91829085) ('Testing Accuracy:', 0.60666668)\n",
      "Step 147: loss = 14405756.42 (12.990 sec) ('Training Accuracy:', 0.90217394) ('Testing Accuracy:', 0.61666667)\n",
      "Step 148: loss = 13239142.30 (12.875 sec) ('Training Accuracy:', 0.94265366) ('Testing Accuracy:', 0.62)\n",
      "Step 149: loss = 13013731.06 (12.871 sec) ('Training Accuracy:', 0.90779608) ('Testing Accuracy:', 0.62)\n",
      "Step 150: loss = 17781802.04 (12.288 sec) ('Training Accuracy:', 0.92728639) ('Testing Accuracy:', 0.63666666)\n",
      "Step 151: loss = 16908961.40 (10.729 sec) ('Training Accuracy:', 0.84820092) ('Testing Accuracy:', 0.62333333)\n",
      "Step 152: loss = 19222037.67 (13.021 sec) ('Training Accuracy:', 0.81634182) ('Testing Accuracy:', 0.62333333)\n",
      "Step 153: loss = 24364147.20 (12.439 sec) ('Training Accuracy:', 0.73838079) ('Testing Accuracy:', 0.63666666)\n",
      "Step 154: loss = 33248213.39 (11.548 sec) ('Training Accuracy:', 0.73088455) ('Testing Accuracy:', 0.63666666)\n",
      "Step 155: loss = 41211383.05 (8.712 sec) ('Training Accuracy:', 0.81971514) ('Testing Accuracy:', 0.63666666)\n",
      "Step 156: loss = 55149071.14 (13.299 sec) ('Training Accuracy:', 0.84407794) ('Testing Accuracy:', 0.60666668)\n",
      "Step 157: loss = 41013604.23 (10.599 sec) ('Training Accuracy:', 0.69827586) ('Testing Accuracy:', 0.56333333)\n",
      "Step 158: loss = 16904060.17 (12.878 sec) ('Training Accuracy:', 0.74812591) ('Testing Accuracy:', 0.57333332)\n",
      "Step 159: loss = 6928442.22 (12.533 sec) ('Training Accuracy:', 0.95914543) ('Testing Accuracy:', 0.61666667)\n",
      "Step 160: loss = 2317022.49 (11.751 sec) ('Training Accuracy:', 0.90854573) ('Testing Accuracy:', 0.63)\n",
      "Step 161: loss = 901205.35 (8.551 sec) ('Training Accuracy:', 0.94565219) ('Testing Accuracy:', 0.62333333)\n",
      "Step 162: loss = 1298400.81 (13.293 sec) ('Training Accuracy:', 0.89805096) ('Testing Accuracy:', 0.61666667)\n",
      "Step 163: loss = 1186828.25 (12.125 sec) ('Training Accuracy:', 0.94302851) ('Testing Accuracy:', 0.61666667)\n",
      "Step 164: loss = 1189311.26 (11.644 sec) ('Training Accuracy:', 0.88943028) ('Testing Accuracy:', 0.63)\n",
      "Step 165: loss = 1012839.18 (10.699 sec) ('Training Accuracy:', 0.90929538) ('Testing Accuracy:', 0.63999999)\n",
      "Step 166: loss = 1514862.73 (10.576 sec) ('Training Accuracy:', 0.91229385) ('Testing Accuracy:', 0.62333333)\n",
      "Step 167: loss = 1174471.40 (10.605 sec) ('Training Accuracy:', 0.87143928) ('Testing Accuracy:', 0.60000002)\n",
      "Step 168: loss = 927927.92 (8.869 sec) ('Training Accuracy:', 0.92278862) ('Testing Accuracy:', 0.62333333)\n",
      "Step 169: loss = 1565452.66 (13.748 sec) ('Training Accuracy:', 0.85869563) ('Testing Accuracy:', 0.60666668)\n",
      "Step 170: loss = 1139212.31 (8.581 sec) ('Training Accuracy:', 0.92203897) ('Testing Accuracy:', 0.62666667)\n",
      "Step 171: loss = 1746855.62 (9.367 sec) ('Training Accuracy:', 0.9190405) ('Testing Accuracy:', 0.62)\n",
      "Step 172: loss = 1816754.37 (9.259 sec) ('Training Accuracy:', 0.88493252) ('Testing Accuracy:', 0.61000001)\n",
      "Step 173: loss = 698150.52 (9.265 sec) ('Training Accuracy:', 0.92616194) ('Testing Accuracy:', 0.61666667)\n",
      "Step 174: loss = 1633830.42 (10.938 sec) ('Training Accuracy:', 0.84220392) ('Testing Accuracy:', 0.58666664)\n",
      "Step 175: loss = 883139.86 (11.669 sec) ('Training Accuracy:', 0.89730138) ('Testing Accuracy:', 0.62333333)\n",
      "Step 176: loss = 1715352.24 (12.332 sec) ('Training Accuracy:', 0.94715142) ('Testing Accuracy:', 0.60666668)\n",
      "Step 177: loss = 1473932.27 (12.623 sec) ('Training Accuracy:', 0.87781107) ('Testing Accuracy:', 0.63)\n",
      "Step 178: loss = 1010308.35 (11.846 sec) ('Training Accuracy:', 0.90592206) ('Testing Accuracy:', 0.63)\n",
      "Step 179: loss = 1634586.10 (11.101 sec) ('Training Accuracy:', 0.87968516) ('Testing Accuracy:', 0.63333333)\n",
      "Step 180: loss = 1459322.77 (8.750 sec) ('Training Accuracy:', 0.87818593) ('Testing Accuracy:', 0.63)\n",
      "Step 181: loss = 1321644.12 (11.818 sec) ('Training Accuracy:', 0.89992505) ('Testing Accuracy:', 0.63666666)\n",
      "Step 182: loss = 1515196.27 (12.898 sec) ('Training Accuracy:', 0.83883059) ('Testing Accuracy:', 0.5933333)\n",
      "Step 183: loss = 915201.64 (12.604 sec) ('Training Accuracy:', 0.84632683) ('Testing Accuracy:', 0.61000001)\n",
      "Step 184: loss = 1521248.67 (12.322 sec) ('Training Accuracy:', 0.90667164) ('Testing Accuracy:', 0.63)\n",
      "Step 185: loss = 1639029.61 (11.886 sec) ('Training Accuracy:', 0.80772114) ('Testing Accuracy:', 0.57999998)\n",
      "Step 186: loss = 944550.26 (8.599 sec) ('Training Accuracy:', 0.8935532) ('Testing Accuracy:', 0.63333333)\n",
      "Step 187: loss = 1369346.59 (13.352 sec) ('Training Accuracy:', 0.84820092) ('Testing Accuracy:', 0.59666669)\n",
      "Step 188: loss = 1068114.73 (8.791 sec) ('Training Accuracy:', 0.90967017) ('Testing Accuracy:', 0.63)\n",
      "Step 189: loss = 1659861.63 (11.485 sec) ('Training Accuracy:', 0.89992505) ('Testing Accuracy:', 0.61000001)\n",
      "Step 190: loss = 1582409.10 (10.791 sec) ('Training Accuracy:', 0.9070465) ('Testing Accuracy:', 0.63333333)\n",
      "Step 191: loss = 1649002.86 (10.685 sec) ('Training Accuracy:', 0.87931037) ('Testing Accuracy:', 0.61000001)\n",
      "Step 192: loss = 1591082.07 (10.723 sec) ('Training Accuracy:', 0.7942279) ('Testing Accuracy:', 0.57999998)\n",
      "Step 193: loss = 1011975.45 (8.743 sec) ('Training Accuracy:', 0.9377811) ('Testing Accuracy:', 0.61666667)\n",
      "Step 194: loss = 2106047.64 (13.252 sec) ('Training Accuracy:', 0.80734634) ('Testing Accuracy:', 0.58666664)\n",
      "Step 195: loss = 773179.10 (13.345 sec) ('Training Accuracy:', 0.92278862) ('Testing Accuracy:', 0.63333333)\n",
      "Step 196: loss = 1794033.51 (13.421 sec) ('Training Accuracy:', 0.77736133) ('Testing Accuracy:', 0.57666665)\n",
      "Step 197: loss = 838986.29 (13.209 sec) ('Training Accuracy:', 0.93028486) ('Testing Accuracy:', 0.62333333)\n",
      "Step 198: loss = 1570517.23 (11.115 sec) ('Training Accuracy:', 0.77661169) ('Testing Accuracy:', 0.57666665)\n",
      "Step 199: loss = 1654617.32 (13.469 sec) ('Training Accuracy:', 0.90892053) ('Testing Accuracy:', 0.61666667)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_acc_whole, test_acc_whole = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 378075023.00 (9.125 sec) ('Training Accuracy:', 0.57999253) ('Testing Accuracy:', 0.58333331)\n",
      "Step 1: loss = 212032418.00 (9.085 sec) ('Training Accuracy:', 0.5533908) ('Testing Accuracy:', 0.52333331)\n",
      "Step 2: loss = 176671522.00 (8.689 sec) ('Training Accuracy:', 0.54627204) ('Testing Accuracy:', 0.50666666)\n",
      "Step 3: loss = 162068939.00 (8.989 sec) ('Training Accuracy:', 0.53016108) ('Testing Accuracy:', 0.48666668)\n",
      "Step 4: loss = 159282855.00 (12.244 sec) ('Training Accuracy:', 0.54140127) ('Testing Accuracy:', 0.49000001)\n",
      "Step 5: loss = 168716221.00 (9.482 sec) ('Training Accuracy:', 0.64968151) ('Testing Accuracy:', 0.5933333)\n",
      "Step 6: loss = 142647160.00 (9.258 sec) ('Training Accuracy:', 0.67103785) ('Testing Accuracy:', 0.63999999)\n",
      "Step 7: loss = 112778716.75 (9.235 sec) ('Training Accuracy:', 0.66654176) ('Testing Accuracy:', 0.57666665)\n",
      "Step 8: loss = 99121671.00 (9.277 sec) ('Training Accuracy:', 0.65792429) ('Testing Accuracy:', 0.57666665)\n",
      "Step 9: loss = 88507973.00 (9.271 sec) ('Training Accuracy:', 0.66691643) ('Testing Accuracy:', 0.58666664)\n",
      "Step 10: loss = 81780810.25 (9.246 sec) ('Training Accuracy:', 0.65305358) ('Testing Accuracy:', 0.56333333)\n",
      "Step 11: loss = 75277717.25 (9.269 sec) ('Training Accuracy:', 0.6418134) ('Testing Accuracy:', 0.57666665)\n",
      "Step 12: loss = 69309493.50 (9.259 sec) ('Training Accuracy:', 0.65492696) ('Testing Accuracy:', 0.58666664)\n",
      "Step 13: loss = 64986339.88 (9.272 sec) ('Training Accuracy:', 0.70138627) ('Testing Accuracy:', 0.58666664)\n",
      "Step 14: loss = 63652648.00 (9.398 sec) ('Training Accuracy:', 0.70663172) ('Testing Accuracy:', 0.58333331)\n",
      "Step 15: loss = 65310698.75 (11.770 sec) ('Training Accuracy:', 0.6095916) ('Testing Accuracy:', 0.52666664)\n",
      "Step 16: loss = 58602175.25 (11.871 sec) ('Training Accuracy:', 0.64780819) ('Testing Accuracy:', 0.57666665)\n",
      "Step 17: loss = 49588613.00 (11.886 sec) ('Training Accuracy:', 0.68602473) ('Testing Accuracy:', 0.59666669)\n",
      "Step 18: loss = 48643172.50 (10.020 sec) ('Training Accuracy:', 0.72686398) ('Testing Accuracy:', 0.58333331)\n",
      "Step 19: loss = 48847229.50 (11.835 sec) ('Training Accuracy:', 0.69763958) ('Testing Accuracy:', 0.58333331)\n",
      "Step 20: loss = 49633919.00 (12.311 sec) ('Training Accuracy:', 0.61708504) ('Testing Accuracy:', 0.54333335)\n",
      "Step 21: loss = 47221449.38 (11.828 sec) ('Training Accuracy:', 0.65342826) ('Testing Accuracy:', 0.55666667)\n",
      "Step 22: loss = 40823729.38 (11.119 sec) ('Training Accuracy:', 0.727988) ('Testing Accuracy:', 0.60000002)\n",
      "Step 23: loss = 39729872.06 (15.716 sec) ('Training Accuracy:', 0.7632072) ('Testing Accuracy:', 0.60666668)\n",
      "Step 24: loss = 46205886.50 (12.486 sec) ('Training Accuracy:', 0.64705884) ('Testing Accuracy:', 0.56333333)\n",
      "Step 25: loss = 36421775.62 (11.401 sec) ('Training Accuracy:', 0.64406145) ('Testing Accuracy:', 0.55000001)\n",
      "Step 26: loss = 33777914.25 (11.217 sec) ('Training Accuracy:', 0.6863994) ('Testing Accuracy:', 0.58333331)\n",
      "Step 27: loss = 31505380.19 (14.486 sec) ('Training Accuracy:', 0.74747097) ('Testing Accuracy:', 0.5933333)\n",
      "Step 28: loss = 32600376.81 (9.251 sec) ('Training Accuracy:', 0.78006744) ('Testing Accuracy:', 0.58666664)\n",
      "Step 29: loss = 42770438.94 (9.197 sec) ('Training Accuracy:', 0.61034095) ('Testing Accuracy:', 0.54000002)\n",
      "Step 30: loss = 32284330.75 (9.180 sec) ('Training Accuracy:', 0.64630949) ('Testing Accuracy:', 0.55000001)\n",
      "Step 31: loss = 27778072.31 (9.530 sec) ('Training Accuracy:', 0.70775568) ('Testing Accuracy:', 0.58999997)\n",
      "Step 32: loss = 25293868.62 (10.207 sec) ('Training Accuracy:', 0.78980893) ('Testing Accuracy:', 0.60666668)\n",
      "Step 33: loss = 29061990.25 (9.271 sec) ('Training Accuracy:', 0.76245785) ('Testing Accuracy:', 0.57666665)\n",
      "Step 34: loss = 34253443.38 (9.516 sec) ('Training Accuracy:', 0.62270516) ('Testing Accuracy:', 0.50666666)\n",
      "Step 35: loss = 28274009.31 (9.455 sec) ('Training Accuracy:', 0.62270516) ('Testing Accuracy:', 0.52666664)\n",
      "Step 36: loss = 29042395.44 (10.735 sec) ('Training Accuracy:', 0.72723866) ('Testing Accuracy:', 0.57999998)\n",
      "Step 37: loss = 38152532.88 (10.407 sec) ('Training Accuracy:', 0.6849007) ('Testing Accuracy:', 0.56)\n",
      "Step 38: loss = 28427231.19 (9.398 sec) ('Training Accuracy:', 0.66167104) ('Testing Accuracy:', 0.54333335)\n",
      "Step 39: loss = 26309689.00 (9.571 sec) ('Training Accuracy:', 0.6702885) ('Testing Accuracy:', 0.55666667)\n",
      "Step 40: loss = 29372912.38 (10.419 sec) ('Training Accuracy:', 0.73136008) ('Testing Accuracy:', 0.57666665)\n",
      "Step 41: loss = 46783776.06 (10.099 sec) ('Training Accuracy:', 0.66953915) ('Testing Accuracy:', 0.54000002)\n",
      "Step 42: loss = 47717038.44 (9.627 sec) ('Training Accuracy:', 0.75009364) ('Testing Accuracy:', 0.57666665)\n",
      "Step 43: loss = 77173276.72 (9.450 sec) ('Training Accuracy:', 0.75458974) ('Testing Accuracy:', 0.68000001)\n",
      "Step 44: loss = 94689813.38 (9.204 sec) ('Training Accuracy:', 0.71112776) ('Testing Accuracy:', 0.67333335)\n",
      "Step 45: loss = 58290207.00 (9.591 sec) ('Training Accuracy:', 0.79093295) ('Testing Accuracy:', 0.67333335)\n",
      "Step 46: loss = 36940022.62 (9.448 sec) ('Training Accuracy:', 0.81041586) ('Testing Accuracy:', 0.6566667)\n",
      "Step 47: loss = 29427957.12 (9.881 sec) ('Training Accuracy:', 0.80966651) ('Testing Accuracy:', 0.66333336)\n",
      "Step 48: loss = 25434166.69 (10.244 sec) ('Training Accuracy:', 0.81116521) ('Testing Accuracy:', 0.66666669)\n",
      "Step 49: loss = 26188929.94 (9.986 sec) ('Training Accuracy:', 0.80591983) ('Testing Accuracy:', 0.66666669)\n",
      "Step 50: loss = 24333451.41 (12.170 sec) ('Training Accuracy:', 0.76882726) ('Testing Accuracy:', 0.67333335)\n",
      "Step 51: loss = 28568492.97 (12.383 sec) ('Training Accuracy:', 0.80554515) ('Testing Accuracy:', 0.66333336)\n",
      "Step 52: loss = 33304935.56 (12.331 sec) ('Training Accuracy:', 0.74634695) ('Testing Accuracy:', 0.67000002)\n",
      "Step 53: loss = 42922473.38 (9.395 sec) ('Training Accuracy:', 0.78418881) ('Testing Accuracy:', 0.66000003)\n",
      "Step 54: loss = 50733045.09 (9.213 sec) ('Training Accuracy:', 0.83327091) ('Testing Accuracy:', 0.60666668)\n",
      "Step 55: loss = 28062024.97 (10.326 sec) ('Training Accuracy:', 0.82053202) ('Testing Accuracy:', 0.5933333)\n",
      "Step 56: loss = 15598944.25 (9.750 sec) ('Training Accuracy:', 0.84113902) ('Testing Accuracy:', 0.58666664)\n",
      "Step 57: loss = 10348425.91 (14.025 sec) ('Training Accuracy:', 0.88235295) ('Testing Accuracy:', 0.61000001)\n",
      "Step 58: loss = 9133836.77 (12.237 sec) ('Training Accuracy:', 0.89584112) ('Testing Accuracy:', 0.63999999)\n",
      "Step 59: loss = 8934813.39 (12.634 sec) ('Training Accuracy:', 0.90071189) ('Testing Accuracy:', 0.63999999)\n",
      "Step 60: loss = 8929143.45 (13.307 sec) ('Training Accuracy:', 0.89996254) ('Testing Accuracy:', 0.64666665)\n",
      "Step 61: loss = 11290217.25 (14.592 sec) ('Training Accuracy:', 0.87186211) ('Testing Accuracy:', 0.64666665)\n",
      "Step 62: loss = 10148158.47 (12.748 sec) ('Training Accuracy:', 0.8355189) ('Testing Accuracy:', 0.66000003)\n",
      "Step 63: loss = 13067948.09 (12.562 sec) ('Training Accuracy:', 0.84750843) ('Testing Accuracy:', 0.65333331)\n",
      "Step 64: loss = 18079552.27 (9.726 sec) ('Training Accuracy:', 0.81153989) ('Testing Accuracy:', 0.66333336)\n",
      "Step 65: loss = 19576015.62 (9.640 sec) ('Training Accuracy:', 0.77294868) ('Testing Accuracy:', 0.66333336)\n",
      "Step 66: loss = 26050765.86 (9.348 sec) ('Training Accuracy:', 0.77969277) ('Testing Accuracy:', 0.65333331)\n",
      "Step 67: loss = 39679058.25 (9.294 sec) ('Training Accuracy:', 0.89321846) ('Testing Accuracy:', 0.63666666)\n",
      "Step 68: loss = 37006827.62 (10.708 sec) ('Training Accuracy:', 0.70363432) ('Testing Accuracy:', 0.55666667)\n",
      "Step 69: loss = 28206006.34 (10.643 sec) ('Training Accuracy:', 0.70513302) ('Testing Accuracy:', 0.54666668)\n",
      "Step 70: loss = 16569617.25 (12.707 sec) ('Training Accuracy:', 0.78119147) ('Testing Accuracy:', 0.5933333)\n",
      "Step 71: loss = 13385024.16 (9.752 sec) ('Training Accuracy:', 0.82652676) ('Testing Accuracy:', 0.60333335)\n",
      "Step 72: loss = 18940011.12 (10.928 sec) ('Training Accuracy:', 0.91082805) ('Testing Accuracy:', 0.63)\n",
      "Step 73: loss = 11432492.44 (11.069 sec) ('Training Accuracy:', 0.89883852) ('Testing Accuracy:', 0.65333331)\n",
      "Step 74: loss = 8642526.48 (10.294 sec) ('Training Accuracy:', 0.90146124) ('Testing Accuracy:', 0.64999998)\n",
      "Step 75: loss = 11742948.62 (12.015 sec) ('Training Accuracy:', 0.91757214) ('Testing Accuracy:', 0.64999998)\n",
      "Step 76: loss = 12375814.58 (11.576 sec) ('Training Accuracy:', 0.92131883) ('Testing Accuracy:', 0.64999998)\n",
      "Step 77: loss = 11392987.84 (12.489 sec) ('Training Accuracy:', 0.85612589) ('Testing Accuracy:', 0.59666669)\n",
      "Step 78: loss = 10565273.59 (11.293 sec) ('Training Accuracy:', 0.78606218) ('Testing Accuracy:', 0.58333331)\n",
      "Step 79: loss = 8912511.23 (11.171 sec) ('Training Accuracy:', 0.78756088) ('Testing Accuracy:', 0.58666664)\n",
      "Step 80: loss = 12772973.31 (13.037 sec) ('Training Accuracy:', 0.83402026) ('Testing Accuracy:', 0.60000002)\n",
      "Step 81: loss = 13356383.89 (11.474 sec) ('Training Accuracy:', 0.91907084) ('Testing Accuracy:', 0.62666667)\n",
      "Step 82: loss = 12515259.78 (10.791 sec) ('Training Accuracy:', 0.92918694) ('Testing Accuracy:', 0.64333332)\n",
      "Step 83: loss = 13478315.59 (8.904 sec) ('Training Accuracy:', 0.90520793) ('Testing Accuracy:', 0.66333336)\n",
      "Step 84: loss = 13291018.06 (15.351 sec) ('Training Accuracy:', 0.86024725) ('Testing Accuracy:', 0.66000003)\n",
      "Step 85: loss = 13555939.03 (12.557 sec) ('Training Accuracy:', 0.81153989) ('Testing Accuracy:', 0.67666668)\n",
      "Step 86: loss = 15943514.52 (10.584 sec) ('Training Accuracy:', 0.82315475) ('Testing Accuracy:', 0.66333336)\n",
      "Step 87: loss = 20529191.12 (12.573 sec) ('Training Accuracy:', 0.92731363) ('Testing Accuracy:', 0.66000003)\n",
      "Step 88: loss = 15854629.30 (11.986 sec) ('Training Accuracy:', 0.87523419) ('Testing Accuracy:', 0.60333335)\n",
      "Step 89: loss = 10473615.41 (11.185 sec) ('Training Accuracy:', 0.80591983) ('Testing Accuracy:', 0.57333332)\n",
      "Step 90: loss = 7508223.06 (9.360 sec) ('Training Accuracy:', 0.80741853) ('Testing Accuracy:', 0.57333332)\n",
      "Step 91: loss = 9337590.16 (11.672 sec) ('Training Accuracy:', 0.93967777) ('Testing Accuracy:', 0.63666666)\n",
      "Step 92: loss = 5404360.88 (15.353 sec) ('Training Accuracy:', 0.94117647) ('Testing Accuracy:', 0.65333331)\n",
      "Step 93: loss = 4347192.23 (15.581 sec) ('Training Accuracy:', 0.94567251) ('Testing Accuracy:', 0.64666665)\n",
      "Step 94: loss = 2892400.45 (11.090 sec) ('Training Accuracy:', 0.91270137) ('Testing Accuracy:', 0.63666666)\n",
      "Step 95: loss = 2662264.84 (13.696 sec) ('Training Accuracy:', 0.94155115) ('Testing Accuracy:', 0.63999999)\n",
      "Step 96: loss = 2279111.11 (13.579 sec) ('Training Accuracy:', 0.95166731) ('Testing Accuracy:', 0.63666666)\n",
      "Step 97: loss = 1828196.06 (13.004 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.66000003)\n",
      "Step 98: loss = 1936287.60 (11.009 sec) ('Training Accuracy:', 0.95466465) ('Testing Accuracy:', 0.64999998)\n",
      "Step 99: loss = 1404072.05 (14.041 sec) ('Training Accuracy:', 0.9437992) ('Testing Accuracy:', 0.63999999)\n",
      "Step 100: loss = 1826759.23 (11.938 sec) ('Training Accuracy:', 0.86586738) ('Testing Accuracy:', 0.60666668)\n",
      "Step 101: loss = 2623515.85 (12.217 sec) ('Training Accuracy:', 0.85687524) ('Testing Accuracy:', 0.59666669)\n",
      "Step 102: loss = 5310587.39 (11.636 sec) ('Training Accuracy:', 0.82802546) ('Testing Accuracy:', 0.58999997)\n",
      "Step 103: loss = 5140542.64 (11.203 sec) ('Training Accuracy:', 0.955414) ('Testing Accuracy:', 0.64999998)\n",
      "Step 104: loss = 4257954.34 (13.007 sec) ('Training Accuracy:', 0.93255901) ('Testing Accuracy:', 0.64666665)\n",
      "Step 105: loss = 5176554.70 (8.558 sec) ('Training Accuracy:', 0.92506558) ('Testing Accuracy:', 0.64666665)\n",
      "Step 106: loss = 6416615.37 (8.951 sec) ('Training Accuracy:', 0.85724992) ('Testing Accuracy:', 0.67333335)\n",
      "Step 107: loss = 7040457.54 (9.396 sec) ('Training Accuracy:', 0.87223679) ('Testing Accuracy:', 0.6566667)\n",
      "Step 108: loss = 10963599.23 (9.265 sec) ('Training Accuracy:', 0.81079054) ('Testing Accuracy:', 0.66000003)\n",
      "Step 109: loss = 13090877.56 (9.274 sec) ('Training Accuracy:', 0.88272762) ('Testing Accuracy:', 0.6566667)\n",
      "Step 110: loss = 17318501.66 (9.318 sec) ('Training Accuracy:', 0.8977145) ('Testing Accuracy:', 0.6566667)\n",
      "Step 111: loss = 20058047.98 (9.290 sec) ('Training Accuracy:', 0.93742973) ('Testing Accuracy:', 0.63666666)\n",
      "Step 112: loss = 28778759.91 (9.273 sec) ('Training Accuracy:', 0.80666918) ('Testing Accuracy:', 0.58999997)\n",
      "Step 113: loss = 29648552.28 (9.278 sec) ('Training Accuracy:', 0.63132262) ('Testing Accuracy:', 0.52999997)\n",
      "Step 114: loss = 28474337.03 (9.258 sec) ('Training Accuracy:', 0.5811165) ('Testing Accuracy:', 0.49000001)\n",
      "Step 115: loss = 33302148.00 (9.261 sec) ('Training Accuracy:', 0.60284752) ('Testing Accuracy:', 0.5)\n",
      "Step 116: loss = 31130747.70 (12.013 sec) ('Training Accuracy:', 0.71562386) ('Testing Accuracy:', 0.55000001)\n",
      "Step 117: loss = 31471304.84 (13.822 sec) ('Training Accuracy:', 0.89808917) ('Testing Accuracy:', 0.62)\n",
      "Step 118: loss = 32829190.62 (12.612 sec) ('Training Accuracy:', 0.87748218) ('Testing Accuracy:', 0.67333335)\n",
      "Step 119: loss = 21472036.42 (11.235 sec) ('Training Accuracy:', 0.86324465) ('Testing Accuracy:', 0.6566667)\n",
      "Step 120: loss = 9435949.64 (9.792 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.64666665)\n",
      "Step 121: loss = 3780964.98 (13.091 sec) ('Training Accuracy:', 0.92993629) ('Testing Accuracy:', 0.63999999)\n",
      "Step 122: loss = 2042414.19 (10.581 sec) ('Training Accuracy:', 0.95916075) ('Testing Accuracy:', 0.64333332)\n",
      "Step 123: loss = 1163070.14 (10.529 sec) ('Training Accuracy:', 0.94604719) ('Testing Accuracy:', 0.62333333)\n",
      "Step 124: loss = 924202.41 (9.863 sec) ('Training Accuracy:', 0.94042712) ('Testing Accuracy:', 0.63333333)\n",
      "Step 125: loss = 858442.12 (14.711 sec) ('Training Accuracy:', 0.95354062) ('Testing Accuracy:', 0.63333333)\n",
      "Step 126: loss = 1172240.91 (10.662 sec) ('Training Accuracy:', 0.96590483) ('Testing Accuracy:', 0.63)\n",
      "Step 127: loss = 745958.84 (12.928 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.63999999)\n",
      "Step 128: loss = 846377.99 (12.246 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.62333333)\n",
      "Step 129: loss = 699080.22 (11.865 sec) ('Training Accuracy:', 0.96665418) ('Testing Accuracy:', 0.63999999)\n",
      "Step 130: loss = 677216.77 (8.707 sec) ('Training Accuracy:', 0.94567251) ('Testing Accuracy:', 0.63)\n",
      "Step 131: loss = 645740.24 (12.544 sec) ('Training Accuracy:', 0.93443239) ('Testing Accuracy:', 0.62666667)\n",
      "Step 132: loss = 768924.92 (8.544 sec) ('Training Accuracy:', 0.96440613) ('Testing Accuracy:', 0.63666666)\n",
      "Step 133: loss = 803453.95 (10.048 sec) ('Training Accuracy:', 0.96815288) ('Testing Accuracy:', 0.64666665)\n",
      "Step 134: loss = 651669.15 (10.568 sec) ('Training Accuracy:', 0.90595728) ('Testing Accuracy:', 0.62)\n",
      "Step 135: loss = 788936.07 (13.354 sec) ('Training Accuracy:', 0.95204198) ('Testing Accuracy:', 0.63)\n",
      "Step 136: loss = 614202.75 (11.984 sec) ('Training Accuracy:', 0.9539153) ('Testing Accuracy:', 0.62333333)\n",
      "Step 137: loss = 530493.97 (12.494 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.62333333)\n",
      "Step 138: loss = 392806.05 (11.960 sec) ('Training Accuracy:', 0.92806292) ('Testing Accuracy:', 0.62666667)\n",
      "Step 139: loss = 566014.52 (8.931 sec) ('Training Accuracy:', 0.95991009) ('Testing Accuracy:', 0.62666667)\n",
      "Step 140: loss = 386271.54 (10.969 sec) ('Training Accuracy:', 0.92806292) ('Testing Accuracy:', 0.63)\n",
      "Step 141: loss = 486272.38 (12.951 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.64333332)\n",
      "Step 142: loss = 395432.23 (10.622 sec) ('Training Accuracy:', 0.92918694) ('Testing Accuracy:', 0.62666667)\n",
      "Step 143: loss = 447975.41 (11.362 sec) ('Training Accuracy:', 0.9393031) ('Testing Accuracy:', 0.63333333)\n",
      "Step 144: loss = 372770.17 (11.471 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.63333333)\n",
      "Step 145: loss = 352918.12 (12.213 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.62666667)\n",
      "Step 146: loss = 355124.07 (9.062 sec) ('Training Accuracy:', 0.97115022) ('Testing Accuracy:', 0.64666665)\n",
      "Step 147: loss = 456909.70 (12.128 sec) ('Training Accuracy:', 0.96890223) ('Testing Accuracy:', 0.63666666)\n",
      "Step 148: loss = 798388.78 (9.803 sec) ('Training Accuracy:', 0.95878607) ('Testing Accuracy:', 0.62)\n",
      "Step 149: loss = 638638.34 (14.307 sec) ('Training Accuracy:', 0.90670663) ('Testing Accuracy:', 0.62)\n",
      "Step 150: loss = 2109086.57 (12.886 sec) ('Training Accuracy:', 0.89733982) ('Testing Accuracy:', 0.62666667)\n",
      "Step 151: loss = 2778929.48 (12.644 sec) ('Training Accuracy:', 0.96290743) ('Testing Accuracy:', 0.62)\n",
      "Step 152: loss = 3089009.06 (10.637 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.67000002)\n",
      "Step 153: loss = 2182305.75 (14.059 sec) ('Training Accuracy:', 0.95241666) ('Testing Accuracy:', 0.66000003)\n",
      "Step 154: loss = 3476103.53 (12.011 sec) ('Training Accuracy:', 0.92843759) ('Testing Accuracy:', 0.66000003)\n",
      "Step 155: loss = 5590399.78 (11.575 sec) ('Training Accuracy:', 0.93443239) ('Testing Accuracy:', 0.66000003)\n",
      "Step 156: loss = 6600468.72 (10.783 sec) ('Training Accuracy:', 0.95766205) ('Testing Accuracy:', 0.64666665)\n",
      "Step 157: loss = 6320869.77 (11.152 sec) ('Training Accuracy:', 0.9584114) ('Testing Accuracy:', 0.64333332)\n",
      "Step 158: loss = 7380197.56 (9.874 sec) ('Training Accuracy:', 0.94117647) ('Testing Accuracy:', 0.62666667)\n",
      "Step 159: loss = 9508995.38 (11.523 sec) ('Training Accuracy:', 0.92056948) ('Testing Accuracy:', 0.62)\n",
      "Step 160: loss = 9784804.50 (13.902 sec) ('Training Accuracy:', 0.79880106) ('Testing Accuracy:', 0.58999997)\n",
      "Step 161: loss = 11859037.41 (9.020 sec) ('Training Accuracy:', 0.72648931) ('Testing Accuracy:', 0.55666667)\n",
      "Step 162: loss = 16476803.92 (12.288 sec) ('Training Accuracy:', 0.69351816) ('Testing Accuracy:', 0.55000001)\n",
      "Step 163: loss = 11797117.39 (12.663 sec) ('Training Accuracy:', 0.61820906) ('Testing Accuracy:', 0.50333333)\n",
      "Step 164: loss = 22075594.12 (13.123 sec) ('Training Accuracy:', 0.59048331) ('Testing Accuracy:', 0.47333333)\n",
      "Step 165: loss = 19916174.25 (13.609 sec) ('Training Accuracy:', 0.57811916) ('Testing Accuracy:', 0.46666667)\n",
      "Step 166: loss = 33714443.84 (15.771 sec) ('Training Accuracy:', 0.55901086) ('Testing Accuracy:', 0.45333335)\n",
      "Step 167: loss = 44183234.89 (12.818 sec) ('Training Accuracy:', 0.69726491) ('Testing Accuracy:', 0.53666669)\n",
      "Step 168: loss = 65983670.94 (13.226 sec) ('Training Accuracy:', 0.9276883) ('Testing Accuracy:', 0.64999998)\n",
      "Step 169: loss = 51171913.31 (11.217 sec) ('Training Accuracy:', 0.75796181) ('Testing Accuracy:', 0.68000001)\n",
      "Step 170: loss = 29479579.59 (13.450 sec) ('Training Accuracy:', 0.82053202) ('Testing Accuracy:', 0.67000002)\n",
      "Step 171: loss = 17327752.55 (11.807 sec) ('Training Accuracy:', 0.94679654) ('Testing Accuracy:', 0.64999998)\n",
      "Step 172: loss = 6127291.58 (12.579 sec) ('Training Accuracy:', 0.88834769) ('Testing Accuracy:', 0.62)\n",
      "Step 173: loss = 3311510.38 (12.609 sec) ('Training Accuracy:', 0.95878607) ('Testing Accuracy:', 0.64333332)\n",
      "Step 174: loss = 929487.80 (8.564 sec) ('Training Accuracy:', 0.94604719) ('Testing Accuracy:', 0.64333332)\n",
      "Step 175: loss = 734706.59 (12.644 sec) ('Training Accuracy:', 0.94454855) ('Testing Accuracy:', 0.63666666)\n",
      "Step 176: loss = 658669.98 (11.574 sec) ('Training Accuracy:', 0.94604719) ('Testing Accuracy:', 0.63999999)\n",
      "Step 177: loss = 574889.62 (12.827 sec) ('Training Accuracy:', 0.93630576) ('Testing Accuracy:', 0.63)\n",
      "Step 178: loss = 860356.54 (11.053 sec) ('Training Accuracy:', 0.96215808) ('Testing Accuracy:', 0.62666667)\n",
      "Step 179: loss = 529679.75 (13.731 sec) ('Training Accuracy:', 0.92731363) ('Testing Accuracy:', 0.63666666)\n",
      "Step 180: loss = 1441939.42 (12.501 sec) ('Training Accuracy:', 0.94567251) ('Testing Accuracy:', 0.62)\n",
      "Step 181: loss = 438367.40 (12.458 sec) ('Training Accuracy:', 0.94567251) ('Testing Accuracy:', 0.63999999)\n",
      "Step 182: loss = 573388.60 (12.524 sec) ('Training Accuracy:', 0.92656428) ('Testing Accuracy:', 0.63333333)\n",
      "Step 183: loss = 556241.68 (11.290 sec) ('Training Accuracy:', 0.93817908) ('Testing Accuracy:', 0.63666666)\n",
      "Step 184: loss = 1024660.35 (8.585 sec) ('Training Accuracy:', 0.91345072) ('Testing Accuracy:', 0.62)\n",
      "Step 185: loss = 502808.12 (14.593 sec) ('Training Accuracy:', 0.9201948) ('Testing Accuracy:', 0.63666666)\n",
      "Step 186: loss = 1123655.02 (11.577 sec) ('Training Accuracy:', 0.89921319) ('Testing Accuracy:', 0.62)\n",
      "Step 187: loss = 694844.85 (13.284 sec) ('Training Accuracy:', 0.90445858) ('Testing Accuracy:', 0.62)\n",
      "Step 188: loss = 1390375.47 (10.900 sec) ('Training Accuracy:', 0.94417387) ('Testing Accuracy:', 0.62666667)\n",
      "Step 189: loss = 848402.23 (11.196 sec) ('Training Accuracy:', 0.88497567) ('Testing Accuracy:', 0.61666667)\n",
      "Step 190: loss = 856792.70 (11.045 sec) ('Training Accuracy:', 0.91794682) ('Testing Accuracy:', 0.63333333)\n",
      "Step 191: loss = 1091310.82 (11.415 sec) ('Training Accuracy:', 0.90370923) ('Testing Accuracy:', 0.62)\n",
      "Step 192: loss = 904004.46 (12.026 sec) ('Training Accuracy:', 0.88609964) ('Testing Accuracy:', 0.61000001)\n",
      "Step 193: loss = 1361018.20 (12.564 sec) ('Training Accuracy:', 0.94829524) ('Testing Accuracy:', 0.63)\n",
      "Step 194: loss = 922140.53 (12.726 sec) ('Training Accuracy:', 0.88422632) ('Testing Accuracy:', 0.61333334)\n",
      "Step 195: loss = 1251015.41 (12.311 sec) ('Training Accuracy:', 0.87073809) ('Testing Accuracy:', 0.60666668)\n",
      "Step 196: loss = 1687105.11 (8.636 sec) ('Training Accuracy:', 0.96328211) ('Testing Accuracy:', 0.63666666)\n",
      "Step 197: loss = 880879.95 (12.628 sec) ('Training Accuracy:', 0.91270137) ('Testing Accuracy:', 0.62666667)\n",
      "Step 198: loss = 737316.34 (12.655 sec) ('Training Accuracy:', 0.89958787) ('Testing Accuracy:', 0.62333333)\n",
      "Step 199: loss = 1456403.48 (12.405 sec) ('Training Accuracy:', 0.93443239) ('Testing Accuracy:', 0.62666667)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_acc_face, test_acc_face = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98051703"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67000002"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngiAIgiAIgiAIgiAIQkbx/zEb8pUuxXYJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f60b54a6090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1000)\n",
    "\n",
    "plt.plot(x, train_acc)\n",
    "#plt.plot(x, test_acc)\n",
    "\n",
    "\n",
    "plt.legend(['y = training_samples', 'y = testing_samples'], loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f60b5492ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1000)\n",
    "\n",
    "plt.plot(x, test_acc)\n",
    "\n",
    "\n",
    "plt.legend(['y = testing_samples'], loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_list = pd.DataFrame(\n",
    "    {'Training_Acc': train_acc,\n",
    "     'Testing_Acc': test_acc,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes.AxesSubplot at 0x7f6118139810>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6123d7cb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_list.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes.AxesSubplot at 0x7fbaf3f39a10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEACAYAAACqOy3+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbb555e8a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_list_face = pd.DataFrame(\n",
    "    {'Training_Acc': train_acc_face,\n",
    "     'Testing_Acc': test_acc_face,\n",
    "    })\n",
    "accuracy_list_face.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fbb3c07f090>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEYCAYAAAC0tfaFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqYS5QAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbb207f5050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_list_whole = pd.DataFrame(\n",
    "    {'Training_Acc': train_acc_whole,\n",
    "     'Testing_Acc': test_acc_whole,\n",
    "    })\n",
    "\n",
    "ax = accuracy_list_whole.plot(title='Age Classification for whole Image')\n",
    "ax.set_xlabel(\"Samples\")\n",
    "ax.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
