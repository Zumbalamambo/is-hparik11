{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import os,sys\n",
    "import time\n",
    "from datetime import date\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import pprint\n",
    "from collections import deque\n",
    "from shutil import copyfile\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Gender Predictor File to predict Gender of LinkedIn Profiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load Gender_Prediction.py\n",
    "\n",
    "# Import Libraries\n",
    "\n",
    "import os\n",
    "import re\n",
    "import urllib2\n",
    "from zipfile import ZipFile\n",
    "import csv\n",
    "import cPickle as pickle\n",
    "\n",
    "def downloadNames():\n",
    "    u = urllib2.urlopen('https://www.ssa.gov/oact/babynames/names.zip')\n",
    "    localFile = open('names.zip', 'w')\n",
    "    localFile.write(u.read())\n",
    "    localFile.close()\n",
    "\n",
    "def getNameList():\n",
    "    if not os.path.exists('names.pickle'):\n",
    "        print 'names.pickle does not exist, generating'\n",
    "        \n",
    "        # https://www.ssa.gov/oact/babynames/names.zip\n",
    "        \n",
    "        if not os.path.exists('names.zip'):\n",
    "            print 'names.zip does not exist, downloading from github'\n",
    "            downloadNames()\n",
    "        else:\n",
    "            print 'names.zip exists, not downloading'\n",
    "        \n",
    "        print 'Extracting names from names.zip'  \n",
    "        \n",
    "        namesDict=extractNamesDict()\n",
    "        \n",
    "        maleNames=list()\n",
    "        femaleNames=list()\n",
    "        \n",
    "        print 'Sorting Names'\n",
    "        \n",
    "        for name in namesDict:\n",
    "            counts=namesDict[name]\n",
    "            tuple=(name,counts[0],counts[1])\n",
    "            if counts[0]>counts[1]:\n",
    "                maleNames.append(tuple)\n",
    "            elif counts[1]>counts[0]:\n",
    "                femaleNames.append(tuple)\n",
    "        \n",
    "        names=(maleNames,femaleNames)\n",
    "        \n",
    "        print 'Saving names.pickle'\n",
    "        fw=open('names.pickle','wb')\n",
    "        pickle.dump(names,fw,-1)\n",
    "        fw.close()\n",
    "        print 'Saved names.pickle'\n",
    "    else:\n",
    "        print 'names.pickle exists, loading data'\n",
    "        f=open('names.pickle','rb')\n",
    "        names=pickle.load(f)\n",
    "        print 'names.pickle loaded'\n",
    "        \n",
    "    print '%d male names loaded, %d female names loaded'%(len(names[0]),len(names[1]))\n",
    "    \n",
    "    return names[0],names[1]\n",
    "\n",
    "def extractNamesDict():\n",
    "    zf=ZipFile('names.zip', 'r')\n",
    "    filenames=zf.namelist()\n",
    "    \n",
    "    names=dict()\n",
    "    genderMap={'M':0,'F':1}\n",
    "    \n",
    "    for filename in filenames:\n",
    "        fp = zf.open(filename,'rU')\n",
    "        rows=csv.reader(fp, delimiter=',', dialect=csv.excel_tab)\n",
    "        try:\n",
    "            for row in rows:\n",
    "            #print name,row[1]\n",
    "                try:\n",
    "                    name=row[0].upper()\n",
    "                    gender=genderMap[row[1]]\n",
    "                    count=int(row[2])\n",
    "\n",
    "                    if not names.has_key(name):\n",
    "                        names[name]=[0,0]\n",
    "\n",
    "                    names[name][gender]=names[name][gender]+count\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        fp.close()\n",
    "        \n",
    "        print '\\tImported %s'%filename\n",
    "    return names\n",
    "\n",
    "\n",
    "def find_gender_from_first_name(name):\n",
    "    if name.upper() in new_male_list:\n",
    "        return \"Male\"\n",
    "    elif name.upper() in new_female_list:\n",
    "        return \"Female\"\n",
    "    else:\n",
    "        return \"Unknown\"    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "\tmale_names, female_names = getNameList()\n",
    "\tnew_male_list = []\n",
    "\tnew_female_list = []\n",
    "\n",
    "\tfor index,name in enumerate(male_names):\n",
    "\t\ttry:\n",
    "\t\t    if (name[1]/name[2])>=4:\n",
    "\t\t        new_male_list.append(name[0])\n",
    "\t\texcept:\n",
    "\t\t    new_male_list.append(name[0])\n",
    "\t\t    \n",
    "\t#print \"Total number of Male Names after is %d.\" %len(new_male_list)\t\n",
    "\t\n",
    "\t\n",
    "\tfor index,name in enumerate(female_names):\n",
    "\t\ttry:\n",
    "\t\t    if (name[2]/name[1])>=4:\n",
    "\t\t        new_female_list.append(name[0])\n",
    "\t\texcept:\n",
    "\t\t    new_female_list.append(name[0])\n",
    "\t\t    \n",
    "\t#print \"Total number of Female Names after is %d.\" %len(new_female_list)\n",
    "\t\n",
    "\t#find_gender_from_first_name('Harsh')\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Link of Profile Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getProfilePicLink(html):\n",
    "       \n",
    "    soup=BeautifulSoup(html,\"lxml\") \n",
    "    images = [x for x in soup.find_all('img')]\n",
    "    #print images\n",
    "    try:\n",
    "        if \"shrinknp_200_200\" in str(images[0]):\n",
    "            imageUrlString = str(images[0]).replace(\"shrinknp_200_200\", \"shrinknp_400_400\")\n",
    "        else:\n",
    "            imageUrlString = \"\"\n",
    "    except:\n",
    "        imageUrlString = \"\"\n",
    "    \n",
    "    #print imageUrlString\n",
    "    \n",
    "    return imageUrlString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Profile Picture to Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def storeProfilePicture(profileUrl,profile_link):\n",
    "    \n",
    "    lst = profileUrl.split()\n",
    "    userId = profile_link.split('/')[-1]\n",
    "    regex=re.compile(r'(src).*')\n",
    "    img_url = re.sub('src=','', \"\".join([m.group(0) for l in lst for m in [regex.search(l)] if m]))\n",
    "    img_url = img_url.strip('\"\"')\n",
    "    #print img_url\n",
    "    if img_url:\n",
    "        urllib.urlretrieve(img_url, \"Images/\" + userId + \".jpg\")\n",
    "        print userId + \".jpg is saved.\"\n",
    "        return img_url\n",
    "    else:\n",
    "        with open('ghost_person.png', 'rb') as f:\n",
    "            data = f.read()\n",
    "\n",
    "        with open(\"Images/\" + userId + \".png\", 'wb') as f:\n",
    "            f.write(data)\n",
    "\n",
    "        print userId + \".png is saved.\"\n",
    "        return \"https://static.licdn.com/scds/common/u/images/themes/katy/ghosts/person/ghost_person_100x100_v1.png\".strip(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap the UserID from Recommended Users' list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRecommendedUserIds(html):\n",
    "    \n",
    "    \n",
    "    soup=BeautifulSoup(html,\"lxml\")\n",
    "    #content = driver.page_source\n",
    "    #images = [x for x in soup.find_all('img')]\n",
    "    profLinks = [x for x in soup.find_all('li',{'class': 'profile-card'})]\n",
    "    \n",
    "    recUserIds = []\n",
    "    #print profLinks\n",
    "    for link in profLinks:\n",
    "        urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', str(link))\n",
    "        recId = urls[0].split('?')[0].split('/')[-1]\n",
    "        recUserIds.append(recId)\n",
    "    \n",
    "    return recUserIds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Full Name from title in source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getName(html):\n",
    "    \n",
    "    soup=BeautifulSoup(html,\"lxml\")\n",
    "    title = soup.find('title')\n",
    "    name = str(title).replace('<title>','')\n",
    "    full_name = name.replace(' | LinkedIn</title>','')\n",
    "    \n",
    "    return full_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get All Bachelor Degree List and Make a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBachelorList():\n",
    "\n",
    "    BachelorDict = {}\n",
    "    regex = re.compile('[^a-zA-Z/]')\n",
    "    with open('bachelor_degrees.txt') as fp:\n",
    "        for line in fp.readlines():\n",
    "            lineSepator = line.split('(')\n",
    "            abbr = regex.sub('', lineSepator[1])\n",
    "            abbr = abbr.split('/')\n",
    "            for abrv in abbr:\n",
    "                BachelorDict[abrv] = lineSepator[0].strip()\n",
    "        fp.close()\n",
    "\n",
    "    return BachelorDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Age from Bachelor Degree Starting Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_age(bachelor_year):\n",
    "    today = date.today()\n",
    "    return today.year - bachelor_year + 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Person's All Degrees and their Duration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Degree_Duration(html):\n",
    "    \n",
    "    soup=BeautifulSoup(html.encode(\"ascii\",\"ignore\"),\"lxml\")\n",
    "    schoolLinks = soup.find_all('li',{'class':'school'})\n",
    "    degreeList = []\n",
    "    time_range_list = []\n",
    "    \n",
    "    for soup1 in schoolLinks:\n",
    "\n",
    "        degreeLink = soup1.find('span',{'data-field-name':\"Education.DegreeName\"})\n",
    "        timeRange = soup1.find('span',{'class':\"date-range\"})\n",
    "\n",
    "        tempDegree = str(degreeLink).replace('<span class=\"translated translation\" data-field-name=\"Education.DegreeName\">','')\n",
    "        degree = tempDegree.replace('</span>','')\n",
    "\n",
    "        tempTime = str(timeRange).replace('<span class=\"date-range\">','')\n",
    "        time = tempTime.replace('<time>','')\n",
    "        temp_time = time.replace('</time>','')\n",
    "        time_range = temp_time.replace('</span>','')\n",
    "\n",
    "        degreeList.append(degree)\n",
    "        time_range_list.append(time_range)\n",
    "    \n",
    "    return degreeList,time_range_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Bachelor Year from Degree List and its Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_bachelor_year(degree_list,time_list):\n",
    "    \n",
    "    bachelor_degree_duration = set()\n",
    "    \n",
    "    BachelorDict = getBachelorList()\n",
    "    \n",
    "    for index,dg in enumerate(degree_list):\n",
    "\n",
    "        for key in BachelorDict.keys():\n",
    "            if key in dg:\n",
    "                bachelor_degree_duration.add(time_list[index])\n",
    "                break\n",
    "\n",
    "        for value in BachelorDict.values():\n",
    "            if value in dg:\n",
    "                bachelor_degree_duration.add(time_list[index])\n",
    "                break\n",
    "        \n",
    "        \n",
    "    #print time_list   \n",
    "    bachelor_degree_duration = list(bachelor_degree_duration)\n",
    "    \n",
    "    #print bachelor_degree_duration[0]\n",
    "    \n",
    "    try:\n",
    "        if bachelor_degree_duration[0] == 'None':\n",
    "            if time_list:\n",
    "                #print time_list\n",
    "                bachelor_year = int(time_list[0].split()[0]) - 5\n",
    "            else:\n",
    "                bachelor_year = None\n",
    "\n",
    "        else:\n",
    "            bachelor_year = int(bachelor_degree_duration[0].split()[0])    \n",
    "    \n",
    "    except:\n",
    "        bachelor_year = None\n",
    "    \n",
    "\n",
    "    return bachelor_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the age from LinkedIn Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def age_from_linkedin_profile(profileUrl):\n",
    "    \n",
    "    try:\n",
    "        degree_list, time_list = get_Degree_Duration(profileUrl)\n",
    "    \n",
    "        refined_degree_list = []\n",
    "        regex = re.compile('[^a-zA-Z\\s+]')\n",
    "\n",
    "        for degree in degree_list:\n",
    "            refined_degree_list.append(regex.sub('',degree))\n",
    "\n",
    "        bachelor_year = find_bachelor_year(refined_degree_list,time_list)\n",
    "        #print bachelor_year\n",
    "\n",
    "        if bachelor_year:\n",
    "            age = calculate_age(bachelor_year)\n",
    "        else:\n",
    "            age = None\n",
    "    \n",
    "    except:\n",
    "        age = None\n",
    "        \n",
    "    return age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make Dictionary for each Profile and and its Recommended Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakeProfileDictionary(usrid):\n",
    "    \n",
    "    #recommended_profile_ids = []\n",
    "    profileUrl = \"https://www.linkedin.com/in/\" + usrid\n",
    "        \n",
    "    driver = webdriver.PhantomJS(service_log_path=os.path.devnull)\n",
    "    driver.get(profileUrl)\n",
    "    html=driver.page_source\n",
    "    \n",
    "    if \"Parse the tracking code from cookies.\" in html:\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        with open(\"Profile_Source/\" + usrid + \".txt\", 'wb') as fp:\n",
    "                fp.write(html.encode('utf-8'))\n",
    "\n",
    "        fp.close()\n",
    "\n",
    "\n",
    "        profileDict = {}\n",
    "\n",
    "        profileDict['User_ID'] = usrid\n",
    "        profileDict['Full_Name'] = getName(html)\n",
    "        profileDict['Gender'] = find_gender_from_first_name(getName(html).split()[0])\n",
    "        recommended_profile_ids = getRecommendedUserIds(html)\n",
    "        profileDict['Recommended_Ids'] = recommended_profile_ids\n",
    "\n",
    "        profilePicUrl = getProfilePicLink(html)\n",
    "\n",
    "        picUrl = storeProfilePicture(profilePicUrl,profileUrl)\n",
    "\n",
    "        profileDict['Profile_Url'] = picUrl\n",
    "        profileDict['age'] = age_from_linkedin_profile(html)\n",
    "\n",
    "        return profileDict,recommended_profile_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying Files for the backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "copyfile('linkedin_UserIds.pickle','Temp_Files/linkedin_UserIds.pickle')\n",
    "copyfile('linkedin_Black_Listed_UserIds.pickle','Temp_Files/linkedin_Black_Listed_UserIds.pickle')\n",
    "copyfile('linkedin_profiles.pickle','Temp_Files/linkedin_profiles.pickle')\n",
    "copyfile('linkedin_profiles_temp.pickle','Temp_Files/linkedin_profiles_temp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_file = open(\"linkedin_UserIds.pickle\",\"rb\")\n",
    "userIds_list=pickle.load(pkl_file) # errors out here\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = open(\"linkedin_Black_Listed_UserIds.pickle\",\"rb\")\n",
    "black_listed_userids=pickle.load(pkl_file) # errors out here\n",
    "pkl_file.close()\n",
    "\n",
    "\n",
    "pkl_fl = open(\"linkedin_profiles.pickle\",\"rb\")\n",
    "my_original_list=pickle.load(pkl_fl) # errors out here\n",
    "pkl_fl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_original_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(black_listed_userids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "craig-pfledderer-a5b64832.png is saved.\n",
      "jamie-lewin-e-mail-20875a3.png is saved.\n",
      "brian-porter-6422a39.jpg is saved.\n",
      "james-grey-a0177526.jpg is saved.\n",
      "1\n",
      "christopher-conley-9485b822.png is saved.\n",
      "johncschroeder.jpg is saved.\n",
      "2\n",
      "noraali.jpg is saved.\n",
      "3\n",
      "ari-smolyar-a3a37183.jpg is saved.\n",
      "parkersean.jpg is saved.\n",
      "nikolai-roman-6335aa123.jpg is saved.\n",
      "anthony-silva-65b28b6.png is saved.\n",
      "marissa-mayer-19748a8.png is saved.\n",
      "lawal-chindo-88225186.png is saved.\n",
      "hrishikeshprajapati.jpg is saved.\n",
      "4\n",
      "april-ham-87095a54.jpg is saved.\n",
      "5\n",
      "qingyiliu.png is saved.\n",
      "stevekerdock.jpg is saved.\n",
      "6\n",
      "nivetha-balaji-09720369.jpg is saved.\n",
      "7\n",
      "riddhip05.png is saved.\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "beccafoy.png is saved.\n",
      "ndhayalini.jpg is saved.\n",
      "8\n",
      "sarahbethcrawford.jpg is saved.\n",
      "9\n",
      "rithunrajkrishna.jpg is saved.\n",
      "10\n",
      "mian-liang-cpa-6a6243b6.jpg is saved.\n",
      "carlos-guestrin-5352a869.jpg is saved.\n",
      "ric-watkins-bb9bba118.jpg is saved.\n",
      "11\n",
      "km-lim-649072113.jpg is saved.\n",
      "12\n",
      "joanne-salay-2126b244.png is saved.\n",
      "amanda-bailey-675b9b74.jpg is saved.\n",
      "13\n",
      "steven-gerard-6672373.png is saved.\n",
      "patrickbryan.png is saved.\n",
      "linda-ye-a514636b.png is saved.\n",
      "urvashi-poonia-1363597a.jpg is saved.\n",
      "14\n",
      "danielleharbour.jpg is saved.\n",
      "15\n",
      "dylan-rivera-031714a3.jpg is saved.\n",
      "16\n",
      "james-waldron-cpt-cet-a91818a4.png is saved.\n",
      "sumirmeghani.png is saved.\n",
      "kelly-blum-33938b6.png is saved.\n",
      "rafay-basheer-64230a17.png is saved.\n",
      "mitali-nandargikar-83270767.png is saved.\n",
      "stephanie-yandow-25b63583.jpg is saved.\n",
      "brittany-miller-80b3535b.jpg is saved.\n",
      "sandra-nieto-47594829.jpg is saved.\n",
      "17\n",
      "prathammantri.jpg is saved.\n",
      "18\n",
      "hrjeff.jpg is saved.\n",
      "udayseelamantula.jpg is saved.\n",
      "19\n",
      "srishtinegi.jpg is saved.\n",
      "20\n",
      "annette-troupe-701b867.png is saved.\n",
      "wayne-pinkerton-a44a3310.jpg is saved.\n",
      "21\n",
      "srinidhivijayaraghavan.png is saved.\n",
      "brad-burke-cfa-b898711.png is saved.\n",
      "rajashree-das-b9822383.png is saved.\n",
      "joshcampo.png is saved.\n",
      "jeremymonti.png is saved.\n",
      "pooja-priyanka-0914a43b.png is saved.\n",
      "may-yang-11223052.jpg is saved.\n",
      "22\n",
      "robert-krovetz-6020629.png is saved.\n",
      "tiffanie-mcdowell-8a77a2ab.jpg is saved.\n",
      "saureenshah.jpg is saved.\n",
      "23\n",
      "sonia-masquio-2496b025.jpg is saved.\n",
      "mark-layden-ba548063.jpg is saved.\n",
      "jamie-cook-7283169.png is saved.\n",
      "anujavelankar.png is saved.\n",
      "lifan-zhang-39b13a1b.jpg is saved.\n",
      "24\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "lalit-patil-7407968.png is saved.\n",
      "anant-kumar-singh-1b810260.jpg is saved.\n",
      "25\n",
      "aaron-white-6048b278.jpg is saved.\n",
      "26\n",
      "mounika-cherukuri-8981544a.png is saved.\n",
      "melissa-shaker-0274a396.jpg is saved.\n",
      "jeana-jones-5457009.jpg is saved.\n",
      "nicole-orogbu-5255a010a.jpg is saved.\n",
      "27\n",
      "billie-knutson-804205117.png is saved.\n",
      "christopher-conley-126b02.png is saved.\n",
      "janessa-gramson-b1085229.jpg is saved.\n",
      "28\n",
      "lpatil.jpg is saved.\n",
      "29\n",
      "deng-yongjian-14a87819.png is saved.\n",
      "darpanmankar.jpg is saved.\n",
      "30\n",
      "chris-marino-a253571b.png is saved.\n",
      "tfoutch.jpg is saved.\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "olya-ishchukova-74b34a42.jpg is saved.\n",
      "31\n",
      "stacie-ontiveros-b0433284.jpg is saved.\n",
      "joe-bruen-a173264.png is saved.\n",
      "eryn-olson-432679b1.png is saved.\n",
      "adam-craig-3a583612.jpg is saved.\n",
      "32\n",
      "johan-geerinck-b87b231a.jpg is saved.\n",
      "mattoxbeckman.png is saved.\n",
      "christopher-cabrera-9217aa85.jpg is saved.\n",
      "33\n",
      "tracy-foutch-b6360b29.jpg is saved.\n",
      "welchjack.png is saved.\n",
      "pete-yeadon-5965095.jpg is saved.\n",
      "34\n",
      "rishikaupadhyay.jpg is saved.\n",
      "35\n",
      "doris-lee-037797a4.jpg is saved.\n",
      "rohitgulia.jpg is saved.\n",
      "36\n",
      "deb-cornick-77104612.jpg is saved.\n",
      "steven-rice-1674283.png is saved.\n",
      "mikelipps.jpg is saved.\n",
      "hillary-clinton-9b1180114.jpg is saved.\n",
      "37\n",
      "truongbryan.jpg is saved.\n",
      "38\n",
      "lauraecummins.jpg is saved.\n",
      "brittanymiller01.jpg is saved.\n",
      "39\n",
      "simon-hughes-99b3823.png is saved.\n",
      "avn002.png is saved.\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "benjamin-fife-93422a77.jpg is saved.\n",
      "40\n",
      "gonzalo-peschiera-95518436.jpg is saved.\n",
      "41\n",
      "sameer-singh-19422b.jpg is saved.\n",
      "dipen-trivedi-a1855461.png is saved.\n",
      "audry-alabiso-a884ba16.png is saved.\n",
      "sandeep-gaur-93aa421.png is saved.\n",
      "chen-he-4502ba19.jpg is saved.\n",
      "42\n",
      "michael-lee-b1a67482.jpg is saved.\n",
      "sarah-eves-2937a719.jpg is saved.\n",
      "43\n",
      "parinithahirehal.jpg is saved.\n",
      "44\n",
      "stephen-coyle-5b0a79a8.jpg is saved.\n",
      "45\n",
      "shilpamandal.jpg is saved.\n",
      "46\n",
      "karen-mcquiston-a6b924b6.jpg is saved.\n",
      "shouvikdutta.jpg is saved.\n",
      "47\n",
      "ben-dsouza-a0993b18.jpg is saved.\n",
      "trebacz.png is saved.\n",
      "stacey-cao-58998647.jpg is saved.\n",
      "48\n",
      "mayank03.png is saved.\n",
      "sharon-hilding-7547014a.png is saved.\n",
      "brianleahy2.jpg is saved.\n",
      "49\n",
      "kim-sams-apr-82423b6.png is saved.\n",
      "paul-adams-38002a77.jpg is saved.\n",
      "50\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "cristiano-louvain-31067220.png is saved.\n",
      "clint-hillary-48051648.png is saved.\n",
      "diana-macfarlane-6128402b.jpg is saved.\n",
      "51\n",
      "maddogmikes.png is saved.\n",
      "thomas-driscoll-304a3522.jpg is saved.\n",
      "52\n",
      "diana-greenstone-8a284a5.png is saved.\n",
      "xiaocheng-zhang-4086b644.jpg is saved.\n",
      "keaton-whitehead-32331284.jpg is saved.\n",
      "53\n",
      "gabriel-chan-cfa-54169932.jpg is saved.\n",
      "54\n",
      "larry-page-13901365.jpg is saved.\n",
      "55\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "grace-kim-04288416.jpg is saved.\n",
      "56\n",
      "jacqueline-wilson-45bb4584.jpg is saved.\n",
      "jennifer-malachowski-09508251.jpg is saved.\n",
      "57\n",
      "christopher-cabrera-55aa8180.jpg is saved.\n",
      "sharrie-huang-67632232.jpg is saved.\n",
      "shawnkearney.jpg is saved.\n",
      "johnfwelch.jpg is saved.\n",
      "ravalinannuru.jpg is saved.\n",
      "58\n",
      "karyn-s-williams-130b0a21.png is saved.\n",
      "willrobinson2.jpg is saved.\n",
      "cleo-chang-0b4b6823.jpg is saved.\n",
      "sanjiv-kapoor-a3071a57.png is saved.\n",
      "tina-maddux-8001783.png is saved.\n",
      "peggy-liao-b7031739.jpg is saved.\n",
      "59\n",
      "brittany-moore-a38542a6.jpg is saved.\n",
      "salilparulekar.png is saved.\n",
      "vishal-mehta-8469a26.jpg is saved.\n",
      "60\n",
      "carolina-bermejo-lingres-807180123.jpg is saved.\n",
      "kelleyschorn.jpg is saved.\n",
      "61\n",
      "ryan-hamlin-ba90b08.png is saved.\n",
      "swathivijayakumar.jpg is saved.\n",
      "62\n",
      "alec-brecker-362b4423.png is saved.\n",
      "sahana-shekar-66193255.png is saved.\n",
      "cory-ellerbee-48a327bb.png is saved.\n",
      "akshayavr.jpg is saved.\n",
      "63\n",
      "aneena-s-nath-289277ab.jpg is saved.\n",
      "64\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "rahul013k.jpg is saved.\n",
      "65\n",
      "steven-dupre-aa77a3b3.jpg is saved.\n",
      "66\n",
      "jamie-denton-3b8b0686.jpg is saved.\n",
      "67\n",
      "donna-fogt-0b60789.jpg is saved.\n",
      "ben-wakeman-23483a.jpg is saved.\n",
      "craig-cipolla-43221411.png is saved.\n",
      "rosabella-ste-marthe-5b93724a.jpg is saved.\n",
      "billgatessrclarke.jpg is saved.\n",
      "akshay-natu-3634b729.jpg is saved.\n",
      "68\n",
      "caggarwal.png is saved.\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Your ip Got tracked... Wait for sometime..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    directory = \"Images\"\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    directory1 = \"Profile_Source\"\n",
    "    \n",
    "    if not os.path.exists(directory1):\n",
    "        os.makedirs(directory1)    \n",
    "    \n",
    "    #userIds = deque([\"harshparikh1001\",\"marissamayer\",\"williamhgates\",\"mbilgic\"])\n",
    "    #userIds = [\"harshparikh1001\"]\n",
    "    #recommended_profile_ids = []\n",
    "    \n",
    "    uids = [uid for uid in (d['User_ID'] for d in my_original_list)]\n",
    "    uniqueIds = list(set(uids))\n",
    "    \n",
    "    userIds = userIds_list\n",
    "    \n",
    "    profiles = []\n",
    "    \n",
    "    output = open(\"linkedin_profiles_temp.pickle\", 'wb')   # Save all profiles as pickle file\n",
    "    \n",
    "    count = 0\n",
    "    temp_count = 0\n",
    "    last_call = 0\n",
    "    #black_listed_userids = []\n",
    "    \n",
    "    while count != 100:\n",
    "        \n",
    "        usrid = random.choice(userIds)\n",
    "        userIds.remove(usrid)\n",
    "        #temp_count += 1\n",
    "        #print temp_count\n",
    "        \n",
    "        if (usrid not in uniqueIds) and (usrid not in black_listed_userids):\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                profileDict,recommed_id_list = MakeProfileDictionary(usrid)\n",
    "                \n",
    "                userIds.extend(recommed_id_list)\n",
    "                \n",
    "                if (profileDict['age']!=None) and (profileDict['Profile_Url'].endswith('.jpg')):\n",
    "                    count += 1\n",
    "                    print count\n",
    "                    profiles.append(profileDict)\n",
    "                else:\n",
    "                    black_listed_userids.append(usrid)\n",
    "                    \n",
    "                time.sleep(5)     # delays for 5 seconds\n",
    "\n",
    "                if count % 10 == 0:\n",
    "                    time.sleep(100) # delays for 100 seconds\n",
    "            \n",
    "            except:\n",
    "                print \"\\n\\n*******Your ip Got tracked... Wait for sometime..*******\\n\\n\"\n",
    "                last_call += 1\n",
    "                #if last_call>3:\n",
    "                #    print \"\\nPlease try again later.. :)\"\n",
    "                #    break\n",
    "                \n",
    "                time.sleep(900)\n",
    "                \n",
    "                pass\n",
    "        \n",
    "    pickle.dump(profiles, output,-1)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open pickle file and read stuff from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkl_fl = open(\"linkedin_UserIds.pickle\",\"wb\")\n",
    "pickle.dump(userIds,pkl_fl,-1) # errors out here\n",
    "pkl_fl.close()\n",
    "\n",
    "pk_fl = open(\"linkedin_Black_Listed_UserIds.pickle\",\"wb\")\n",
    "pickle.dump(black_listed_userids,pk_fl,-1) # errors out here\n",
    "pk_fl.close()\n",
    "\n",
    "pkl_file = open(\"linkedin_profiles_temp.pickle\",\"rb\")\n",
    "temp_list=pickle.load(pkl_file) # errors out here\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "profile_list = my_original_list + temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(profile_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for file in os.listdir(\"Images\"):\n",
    "    file_path = os.path.join(\"Images\", file)\n",
    "    try:\n",
    "        if file.endswith('.png'):\n",
    "            os.unlink(file_path)\n",
    "        #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pkl_file = open(\"linkedin_profiles.pickle\",\"wb\")\n",
    "pickle.dump(profile_list, pkl_file,-1) # errors out here\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test profile of \"Barbara Corcoran\"\n",
    "\n",
    "https://www.linkedin.com/in/barbaracorcoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.PhantomJS(service_log_path=os.path.devnull)\n",
    "driver.get(\"https://www.linkedin.com/in/barbaracorcoran\")\n",
    "html3=driver.page_source  \n",
    "print html3\n",
    "soup=BeautifulSoup(html3,\"lxml\") \n",
    "images = [x for x in soup.find_all('img')]\n",
    "#print images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if \"Parse the tracking code from cookies.\" in html3:\n",
    "    print \"Yes\"\n",
    "else:\n",
    "    print \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imageUrlString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_gender_from_first_name(getName(html3).split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print age_from_linkedin_profile(html3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## My Profile and Age\n",
    "\n",
    "https://www.linkedin.com/in/harshparikh1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = webdriver.PhantomJS(service_log_path=os.path.devnull)\n",
    "driver.get(\"https://www.linkedin.com/in/harshparikh1001\")\n",
    "html1=driver.page_source  \n",
    "#print html1\n",
    "soup=BeautifulSoup(html1,\"lxml\") \n",
    "images = [x for x in soup.find_all('img')]\n",
    "#print images\n",
    "try:\n",
    "    if \"shrinknp_200_200\" in str(images[0]):\n",
    "        imageUrlString = str(images[0]).replace(\"shrinknp_200_200\", \"shrinknp_400_400\")\n",
    "    else:\n",
    "        imageUrlString = \"\"\n",
    "except:\n",
    "    imageUrlString = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_gender_from_first_name(getName(html1).split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_from_linkedin_profile(html1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test profile of \"Satya Nadella\"\n",
    "\n",
    "https://www.linkedin.com/in/satya-nadella-3145136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = webdriver.PhantomJS(service_log_path=os.path.devnull)\n",
    "driver.get(\"https://www.linkedin.com/in/satya-nadella-3145136\")\n",
    "html2=driver.page_source  \n",
    "#print html2\n",
    "soup=BeautifulSoup(html2,\"lxml\") \n",
    "images = [x for x in soup.find_all('img')]\n",
    "#print images\n",
    "try:\n",
    "    if \"shrinknp_200_200\" in str(images[0]):\n",
    "        imageUrlString = str(images[0]).replace(\"shrinknp_200_200\", \"shrinknp_400_400\")\n",
    "    else:\n",
    "        imageUrlString = \"\"\n",
    "except:\n",
    "    imageUrlString = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_gender_from_first_name(getName(html2).split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_from_linkedin_profile(html2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#driver = webdriver.PhantomJS()\n",
    "#driver.get(\"https://www.linkedin.com/in/georgecorliss\")\n",
    "#html=driver.page_source    \n",
    "#soup=BeautifulSoup(html,\"lxml\")\n",
    "#schoolLinks = soup.find_all('li',{'class':'school'})\n",
    "\n",
    "#for soup1 in schoolLinks:\n",
    "    \n",
    "#    degreeLink = soup1.find('span',{'data-field-name':\"Education.DegreeName\"})\n",
    "#    timeRange = soup1.find('span',{'class':\"date-range\"})\n",
    "    \n",
    "#    tempDegree = str(degreeLink).replace('<span class=\"translated translation\" data-field-name=\"Education.DegreeName\">','')\n",
    "#    degree = tempDegree.replace('</span>','')\n",
    "    \n",
    "#    tempTime = str(timeRange).replace('<span class=\"date-range\">','')\n",
    "#    time = tempTime.replace('<time>','')\n",
    "#    temp_time = time.replace('</time>','')\n",
    "#    time_range = temp_time.replace('</span>','')\n",
    "    \n",
    "#    print degree\n",
    "#    print time_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link to get all profiles with the first Name\n",
    "\n",
    "https://www.linkedin.com/pub/dir/Bill"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
