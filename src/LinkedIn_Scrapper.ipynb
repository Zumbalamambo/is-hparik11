{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import os,sys\n",
    "import time\n",
    "from datetime import date\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import pprint\n",
    "from collections import deque\n",
    "from shutil import copyfile\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Gender Predictor File to predict Gender of LinkedIn Profiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load Gender_Prediction.py\n",
    "\n",
    "# Import Libraries\n",
    "\n",
    "import os\n",
    "import re\n",
    "import urllib2\n",
    "from zipfile import ZipFile\n",
    "import csv\n",
    "import cPickle as pickle\n",
    "\n",
    "def downloadNames():\n",
    "    u = urllib2.urlopen('https://www.ssa.gov/oact/babynames/names.zip')\n",
    "    localFile = open('names.zip', 'w')\n",
    "    localFile.write(u.read())\n",
    "    localFile.close()\n",
    "\n",
    "def getNameList():\n",
    "    if not os.path.exists('names.pickle'):\n",
    "        print 'names.pickle does not exist, generating'\n",
    "        \n",
    "        # https://www.ssa.gov/oact/babynames/names.zip\n",
    "        \n",
    "        if not os.path.exists('names.zip'):\n",
    "            print 'names.zip does not exist, downloading from github'\n",
    "            downloadNames()\n",
    "        else:\n",
    "            print 'names.zip exists, not downloading'\n",
    "        \n",
    "        print 'Extracting names from names.zip'  \n",
    "        \n",
    "        namesDict=extractNamesDict()\n",
    "        \n",
    "        maleNames=list()\n",
    "        femaleNames=list()\n",
    "        \n",
    "        print 'Sorting Names'\n",
    "        \n",
    "        for name in namesDict:\n",
    "            counts=namesDict[name]\n",
    "            tuple=(name,counts[0],counts[1])\n",
    "            if counts[0]>counts[1]:\n",
    "                maleNames.append(tuple)\n",
    "            elif counts[1]>counts[0]:\n",
    "                femaleNames.append(tuple)\n",
    "        \n",
    "        names=(maleNames,femaleNames)\n",
    "        \n",
    "        print 'Saving names.pickle'\n",
    "        fw=open('names.pickle','wb')\n",
    "        pickle.dump(names,fw,-1)\n",
    "        fw.close()\n",
    "        print 'Saved names.pickle'\n",
    "    else:\n",
    "        print 'names.pickle exists, loading data'\n",
    "        f=open('names.pickle','rb')\n",
    "        names=pickle.load(f)\n",
    "        print 'names.pickle loaded'\n",
    "        \n",
    "    print '%d male names loaded, %d female names loaded'%(len(names[0]),len(names[1]))\n",
    "    \n",
    "    return names[0],names[1]\n",
    "\n",
    "def extractNamesDict():\n",
    "    zf=ZipFile('names.zip', 'r')\n",
    "    filenames=zf.namelist()\n",
    "    \n",
    "    names=dict()\n",
    "    genderMap={'M':0,'F':1}\n",
    "    \n",
    "    for filename in filenames:\n",
    "        fp = zf.open(filename,'rU')\n",
    "        rows=csv.reader(fp, delimiter=',', dialect=csv.excel_tab)\n",
    "        try:\n",
    "            for row in rows:\n",
    "            #print name,row[1]\n",
    "                try:\n",
    "                    name=row[0].upper()\n",
    "                    gender=genderMap[row[1]]\n",
    "                    count=int(row[2])\n",
    "\n",
    "                    if not names.has_key(name):\n",
    "                        names[name]=[0,0]\n",
    "\n",
    "                    names[name][gender]=names[name][gender]+count\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        fp.close()\n",
    "        \n",
    "        print '\\tImported %s'%filename\n",
    "    return names\n",
    "\n",
    "\n",
    "def find_gender_from_first_name(name):\n",
    "    if name.upper() in new_male_list:\n",
    "        return \"Male\"\n",
    "    elif name.upper() in new_female_list:\n",
    "        return \"Female\"\n",
    "    else:\n",
    "        return \"Unknown\"    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "\tmale_names, female_names = getNameList()\n",
    "\tnew_male_list = []\n",
    "\tnew_female_list = []\n",
    "\n",
    "\tfor index,name in enumerate(male_names):\n",
    "\t\ttry:\n",
    "\t\t    if (name[1]/name[2])>=4:\n",
    "\t\t        new_male_list.append(name[0])\n",
    "\t\texcept:\n",
    "\t\t    new_male_list.append(name[0])\n",
    "\t\t    \n",
    "\t#print \"Total number of Male Names after is %d.\" %len(new_male_list)\t\n",
    "\t\n",
    "\t\n",
    "\tfor index,name in enumerate(female_names):\n",
    "\t\ttry:\n",
    "\t\t    if (name[2]/name[1])>=4:\n",
    "\t\t        new_female_list.append(name[0])\n",
    "\t\texcept:\n",
    "\t\t    new_female_list.append(name[0])\n",
    "\t\t    \n",
    "\t#print \"Total number of Female Names after is %d.\" %len(new_female_list)\n",
    "\t\n",
    "\t#find_gender_from_first_name('Harsh')\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Link of Profile Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getProfilePicLink(html):\n",
    "       \n",
    "    soup=BeautifulSoup(html,\"lxml\") \n",
    "    images = [x for x in soup.find_all('img')]\n",
    "    #print images\n",
    "    try:\n",
    "        if \"shrinknp_200_200\" in str(images[0]):\n",
    "            imageUrlString = str(images[0]).replace(\"shrinknp_200_200\", \"shrinknp_400_400\")\n",
    "        else:\n",
    "            imageUrlString = \"\"\n",
    "    except:\n",
    "        imageUrlString = \"\"\n",
    "    \n",
    "    #print imageUrlString\n",
    "    \n",
    "    return imageUrlString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Profile Picture to Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def storeProfilePicture(profileUrl,profile_link):\n",
    "    \n",
    "    lst = profileUrl.split()\n",
    "    userId = profile_link.split('/')[-1]\n",
    "    regex=re.compile(r'(src).*')\n",
    "    img_url = re.sub('src=','', \"\".join([m.group(0) for l in lst for m in [regex.search(l)] if m]))\n",
    "    img_url = img_url.strip('\"\"')\n",
    "    #print img_url\n",
    "    if img_url:\n",
    "        urllib.urlretrieve(img_url, \"Images/\" + userId + \".jpg\")\n",
    "        print userId + \".jpg is saved.\"\n",
    "        return img_url\n",
    "    else:\n",
    "        with open('ghost_person.png', 'rb') as f:\n",
    "            data = f.read()\n",
    "\n",
    "        with open(\"Images/\" + userId + \".png\", 'wb') as f:\n",
    "            f.write(data)\n",
    "\n",
    "        print userId + \".png is saved.\"\n",
    "        return \"https://static.licdn.com/scds/common/u/images/themes/katy/ghosts/person/ghost_person_100x100_v1.png\".strip(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap the UserID from Recommended Users' list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRecommendedUserIds(html):\n",
    "    \n",
    "    \n",
    "    soup=BeautifulSoup(html,\"lxml\")\n",
    "    #content = driver.page_source\n",
    "    #images = [x for x in soup.find_all('img')]\n",
    "    profLinks = [x for x in soup.find_all('li',{'class': 'profile-card'})]\n",
    "    \n",
    "    recUserIds = []\n",
    "    #print profLinks\n",
    "    for link in profLinks:\n",
    "        urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', str(link))\n",
    "        recId = urls[0].split('?')[0].split('/')[-1]\n",
    "        recUserIds.append(recId)\n",
    "    \n",
    "    return recUserIds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Full Name from title in source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getName(html):\n",
    "    \n",
    "    soup=BeautifulSoup(html,\"lxml\")\n",
    "    title = soup.find('title')\n",
    "    name = str(title).replace('<title>','')\n",
    "    full_name = name.replace(' | LinkedIn</title>','')\n",
    "    \n",
    "    return full_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get All Bachelor Degree List and Make a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBachelorList():\n",
    "\n",
    "    BachelorDict = {}\n",
    "    regex = re.compile('[^a-zA-Z/]')\n",
    "    with open('bachelor_degrees.txt') as fp:\n",
    "        for line in fp.readlines():\n",
    "            lineSepator = line.split('(')\n",
    "            abbr = regex.sub('', lineSepator[1])\n",
    "            abbr = abbr.split('/')\n",
    "            for abrv in abbr:\n",
    "                BachelorDict[abrv] = lineSepator[0].strip()\n",
    "        fp.close()\n",
    "\n",
    "    return BachelorDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Age from Bachelor Degree Starting Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_age(bachelor_year):\n",
    "    today = date.today()\n",
    "    return today.year - bachelor_year + 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Person's All Degrees and their Duration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Degree_Duration(html):\n",
    "    \n",
    "    soup=BeautifulSoup(html.encode(\"ascii\",\"ignore\"),\"lxml\")\n",
    "    schoolLinks = soup.find_all('li',{'class':'school'})\n",
    "    degreeList = []\n",
    "    time_range_list = []\n",
    "    #print schoolLinks\n",
    "    \n",
    "    for soup1 in schoolLinks:\n",
    "\n",
    "        degreeLink = soup1.find('span',{'data-field-name':\"Education.DegreeName\"})\n",
    "        #print degreeLink\n",
    "        timeRange = soup1.find('span',{'class':\"date-range\"})\n",
    "        #print timeRange\n",
    "        tempDegree = str(degreeLink).replace('<span class=\"translated translation\" data-field-name=\"Education.DegreeName\">','')\n",
    "        degree = tempDegree.replace('</span>','')\n",
    "\n",
    "        tempTime = str(timeRange).replace('<span class=\"date-range\">','')\n",
    "        time = tempTime.replace('<time>','')\n",
    "        temp_time = time.replace('</time>','')\n",
    "        time_range = temp_time.replace('</span>','')\n",
    "        #print time_range\n",
    "        degreeList.append(degree)\n",
    "        time_range_list.append(time_range)\n",
    "    \n",
    "    return degreeList,time_range_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Bachelor Year from Degree List and its Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_bachelor_year(degree_list,time_list):\n",
    "    \n",
    "    bachelor_degree_duration = set()\n",
    "    \n",
    "    BachelorDict = getBachelorList()\n",
    "    \n",
    "    for index,dg in enumerate(degree_list):\n",
    "\n",
    "        for key in BachelorDict.keys():\n",
    "            if key in dg:\n",
    "                bachelor_degree_duration.add(time_list[index])\n",
    "                break\n",
    "\n",
    "        for value in BachelorDict.values():\n",
    "            if value in dg:\n",
    "                bachelor_degree_duration.add(time_list[index])\n",
    "                break\n",
    "        \n",
    "        \n",
    "    #print time_list   \n",
    "    bachelor_degree_duration = list(bachelor_degree_duration)\n",
    "    \n",
    "    #print bachelor_degree_duration\n",
    "    #print time_list[0]\n",
    "    try:\n",
    "        if not bachelor_degree_duration:\n",
    "            if time_list[0]:\n",
    "                \n",
    "                bachelor_year = int(time_list[0].split()[0]) - 5\n",
    "            else:\n",
    "                bachelor_year = None\n",
    "\n",
    "        else:\n",
    "            bachelor_year = int(bachelor_degree_duration[0].split()[0])    \n",
    "    \n",
    "    except:\n",
    "        bachelor_year = None\n",
    "    \n",
    "\n",
    "    return bachelor_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the age from LinkedIn Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def age_from_linkedin_profile(profileUrl):\n",
    "    \n",
    "    try:\n",
    "        degree_list, time_list = get_Degree_Duration(profileUrl)\n",
    "    \n",
    "        refined_degree_list = []\n",
    "        regex = re.compile('[^a-zA-Z\\s+]')\n",
    "\n",
    "        for degree in degree_list:\n",
    "            refined_degree_list.append(regex.sub('',degree))\n",
    "\n",
    "        bachelor_year = find_bachelor_year(refined_degree_list,time_list)\n",
    "        #print bachelor_year\n",
    "\n",
    "        if bachelor_year:\n",
    "            age = calculate_age(bachelor_year)\n",
    "        else:\n",
    "            age = None\n",
    "    \n",
    "    except:\n",
    "        age = None\n",
    "        \n",
    "    return age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make Dictionary for each Profile and and its Recommended Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakeProfileDictionary(usrid):\n",
    "    \n",
    "    #recommended_profile_ids = []\n",
    "    profileUrl = \"https://www.linkedin.com/in/\" + usrid\n",
    "        \n",
    "    driver = webdriver.PhantomJS(service_log_path=os.path.devnull)\n",
    "    driver.get(profileUrl)\n",
    "    html=driver.page_source\n",
    "    \n",
    "    if \"Parse the tracking code from cookies.\" in html:\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        with open(\"Profile_Source/\" + usrid + \".txt\", 'wb') as fp:\n",
    "                fp.write(html.encode('utf-8'))\n",
    "\n",
    "        fp.close()\n",
    "\n",
    "\n",
    "        profileDict = {}\n",
    "\n",
    "        profileDict['User_ID'] = usrid\n",
    "        profileDict['Full_Name'] = getName(html)\n",
    "        profileDict['Gender'] = find_gender_from_first_name(getName(html).split()[0])\n",
    "        recommended_profile_ids = getRecommendedUserIds(html)\n",
    "        profileDict['Recommended_Ids'] = recommended_profile_ids\n",
    "\n",
    "        profilePicUrl = getProfilePicLink(html)\n",
    "\n",
    "        picUrl = storeProfilePicture(profilePicUrl,profileUrl)\n",
    "\n",
    "        profileDict['Profile_Url'] = picUrl\n",
    "        profileDict['age'] = age_from_linkedin_profile(html)\n",
    "\n",
    "        return profileDict,recommended_profile_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying Files for the backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "copyfile('linkedin_UserIds.pickle','Temp_Files/linkedin_UserIds.pickle')\n",
    "copyfile('linkedin_Black_Listed_UserIds.pickle','Temp_Files/linkedin_Black_Listed_UserIds.pickle')\n",
    "copyfile('linkedin_profiles.pickle','Temp_Files/linkedin_profiles.pickle')\n",
    "copyfile('linkedin_profiles_temp.pickle','Temp_Files/linkedin_profiles_temp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_file = open(\"linkedin_UserIds.pickle\",\"rb\")\n",
    "userIds_list=pickle.load(pkl_file) # errors out here\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = open(\"linkedin_Black_Listed_UserIds.pickle\",\"rb\")\n",
    "black_listed_userids=pickle.load(pkl_file) # errors out here\n",
    "pkl_file.close()\n",
    "\n",
    "\n",
    "pkl_fl = open(\"linkedin_profiles.pickle\",\"rb\")\n",
    "my_original_list=pickle.load(pkl_fl) # errors out here\n",
    "pkl_fl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_original_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(black_listed_userids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "\n",
      "*******Your IP got tracked... Wait for sometime..*******\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    directory = \"Images\"\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    directory1 = \"Profile_Source\"\n",
    "    \n",
    "    if not os.path.exists(directory1):\n",
    "        os.makedirs(directory1)    \n",
    "    \n",
    "    #userIds = deque([\"harshparikh1001\",\"marissamayer\",\"williamhgates\",\"mbilgic\"])\n",
    "    #userIds = [\"harshparikh1001\"]\n",
    "    #recommended_profile_ids = []\n",
    "    \n",
    "    uids = [uid for uid in (d['User_ID'] for d in my_original_list)]\n",
    "    uniqueIds = list(set(uids))\n",
    "    \n",
    "    userIds = userIds_list\n",
    "    \n",
    "    profiles = []\n",
    "    \n",
    "    output = open(\"linkedin_profiles_temp.pickle\", 'wb')   # Save all profiles as pickle file\n",
    "    \n",
    "    count = 0\n",
    "    temp_count = 0\n",
    "    last_call = 0\n",
    "    #black_listed_userids = []\n",
    "    \n",
    "    while count != 100:\n",
    "        \n",
    "        usrid = userIds.popleft()\n",
    "        #userIds.remove(usrid)\n",
    "        temp_count+=1\n",
    "        print temp_count\n",
    "        \n",
    "        if (usrid not in uniqueIds) and (usrid not in black_listed_userids):\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                profileDict,recommed_id_list = MakeProfileDictionary(usrid)\n",
    "                \n",
    "                userIds.extend(recommed_id_list)\n",
    "                \n",
    "                if (profileDict['age']!=None) and (profileDict['Profile_Url'].endswith('.jpg')):\n",
    "                    count += 1\n",
    "                    print count\n",
    "                    profiles.append(profileDict)\n",
    "                else:\n",
    "                    black_listed_userids.append(usrid)\n",
    "                    \n",
    "                time.sleep(5)     # delays for 5 seconds\n",
    "\n",
    "                if temp_count % 10 == 0:\n",
    "                    time.sleep(100) # delays for 100 seconds\n",
    "            \n",
    "            except:\n",
    "                if last_call>=3:\n",
    "                    print \"\\nPlease try again later.. :)\"\n",
    "                    break\n",
    "                    \n",
    "                print \"\\n\\n*******Your IP got tracked... Wait for sometime..*******\\n\\n\"\n",
    "                last_call += 1\n",
    "                time.sleep(900)\n",
    "                \n",
    "                pass\n",
    "        \n",
    "    pickle.dump(profiles, output,-1)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open pickle file and read stuff from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkl_fl = open(\"linkedin_UserIds.pickle\",\"wb\")\n",
    "pickle.dump(userIds,pkl_fl,-1) # errors out here\n",
    "pkl_fl.close()\n",
    "\n",
    "pk_fl = open(\"linkedin_Black_Listed_UserIds.pickle\",\"wb\")\n",
    "pickle.dump(black_listed_userids,pk_fl,-1) # errors out here\n",
    "pk_fl.close()\n",
    "\n",
    "pkl_file = open(\"linkedin_profiles_temp.pickle\",\"rb\")\n",
    "temp_list=pickle.load(pkl_file) # errors out here\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "profile_list = my_original_list + temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(profile_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for file in os.listdir(\"Images\"):\n",
    "    file_path = os.path.join(\"Images\", file)\n",
    "    try:\n",
    "        if file.endswith('.png'):\n",
    "            os.unlink(file_path)\n",
    "        #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pkl_file = open(\"linkedin_profiles.pickle\",\"wb\")\n",
    "pickle.dump(profile_list, pkl_file,-1) # errors out here\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test profile of \"Barbara Corcoran\"\n",
    "\n",
    "https://www.linkedin.com/in/barbaracorcoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.PhantomJS(service_log_path=os.path.devnull)\n",
    "driver.get(\"https://www.linkedin.com/in/williamhgates\")\n",
    "html3=driver.page_source  \n",
    "print html3\n",
    "soup=BeautifulSoup(html3,\"lxml\") \n",
    "images = [x for x in soup.find_all('img')]\n",
    "#print images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imageUrlString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_gender_from_first_name(getName(html3).split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print age_from_linkedin_profile(html3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "williamhgates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## My Profile and Age\n",
    "\n",
    "https://www.linkedin.com/in/harshparikh1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = webdriver.PhantomJS(service_log_path=os.path.devnull)\n",
    "driver.get(\"https://www.linkedin.com/in/harshparikh1001\")\n",
    "html1=driver.page_source  \n",
    "#print html1\n",
    "soup=BeautifulSoup(html1,\"lxml\") \n",
    "images = [x for x in soup.find_all('img')]\n",
    "#print images\n",
    "try:\n",
    "    if \"shrinknp_200_200\" in str(images[0]):\n",
    "        imageUrlString = str(images[0]).replace(\"shrinknp_200_200\", \"shrinknp_400_400\")\n",
    "    else:\n",
    "        imageUrlString = \"\"\n",
    "except:\n",
    "    imageUrlString = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_gender_from_first_name(getName(html1).split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_from_linkedin_profile(html1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test profile of \"Satya Nadella\"\n",
    "\n",
    "https://www.linkedin.com/in/satya-nadella-3145136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = webdriver.PhantomJS(service_log_path=os.path.devnull)\n",
    "driver.get(\"https://www.linkedin.com/in/marissamayer\")\n",
    "html2=driver.page_source  \n",
    "#print html2\n",
    "soup=BeautifulSoup(html2,\"lxml\") \n",
    "images = [x for x in soup.find_all('img')]\n",
    "#print images\n",
    "try:\n",
    "    if \"shrinknp_200_200\" in str(images[0]):\n",
    "        imageUrlString = str(images[0]).replace(\"shrinknp_200_200\", \"shrinknp_400_400\")\n",
    "    else:\n",
    "        imageUrlString = \"\"\n",
    "except:\n",
    "    imageUrlString = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_gender_from_first_name(getName(html2).split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print age_from_linkedin_profile(html2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#driver = webdriver.PhantomJS()\n",
    "#driver.get(\"https://www.linkedin.com/in/georgecorliss\")\n",
    "#html=driver.page_source    \n",
    "#soup=BeautifulSoup(html,\"lxml\")\n",
    "#schoolLinks = soup.find_all('li',{'class':'school'})\n",
    "\n",
    "#for soup1 in schoolLinks:\n",
    "    \n",
    "#    degreeLink = soup1.find('span',{'data-field-name':\"Education.DegreeName\"})\n",
    "#    timeRange = soup1.find('span',{'class':\"date-range\"})\n",
    "    \n",
    "#    tempDegree = str(degreeLink).replace('<span class=\"translated translation\" data-field-name=\"Education.DegreeName\">','')\n",
    "#    degree = tempDegree.replace('</span>','')\n",
    "    \n",
    "#    tempTime = str(timeRange).replace('<span class=\"date-range\">','')\n",
    "#    time = tempTime.replace('<time>','')\n",
    "#    temp_time = time.replace('</time>','')\n",
    "#    time_range = temp_time.replace('</span>','')\n",
    "    \n",
    "#    print degree\n",
    "#    print time_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link to get all profiles with the first Name\n",
    "\n",
    "https://www.linkedin.com/pub/dir/Bill"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
